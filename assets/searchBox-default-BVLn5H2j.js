const t='{"documentCount":140,"nextId":140,"documentIds":{"0":"/article/a9jn9iq6/#标题-2","1":"/article/8djf7t1b/#一、本文介绍","2":"/article/a9jn9iq6/#标题-3","3":"/article/3pv1asqx/#一、本文介绍","4":"/article/8djf7t1b/#二、biformer的作用机制","5":"/article/ebule7cy/#一、本文介绍","6":"/article/a9jn9iq6/#标题-4","7":"/article/3pv1asqx/#二、rcs-osa模块原理","8":"/article/ln4qficp/#一、本文介绍","9":"/article/8djf7t1b/#三、biformer的优劣势","10":"/article/ebule7cy/#二、triplet-attention机制原理","11":"/article/a9jn9iq6/#标题-5","12":"/article/3pv1asqx/#_2-1-rcs-osa的基本原理","13":"/article/ln4qficp/#二、msda框架原理","14":"/article/l7o183q2/#一、本文介绍","15":"/article/8djf7t1b/#四、biformer的结构","16":"/article/ebule7cy/#_2-1-triplet-attention的基本原理","17":"/article/a9jn9iq6/#标题-6","18":"/article/3pv1asqx/#_2-2-rcs","19":"/article/ln4qficp/#三、msda核心代码","20":"/article/l7o183q2/#二、lskattention的机制原理","21":"/article/llrg5yeo/#一、本文介绍","22":"/article/8djf7t1b/#五、添加biformer注意力机制","23":"/article/ebule7cy/#_2-2-triplet-attention和其它简单注意力机制的对比","24":"/article/3pv1asqx/#_2-3-rcs模块","25":"/article/ln4qficp/#四、手把手教你添加msda模块","26":"/article/l7o183q2/#三、lskattention的代码","27":"/article/llrg5yeo/#二、hattention框架原理","28":"/article/7qi9b2wu/#一、本文介绍","29":"/article/8djf7t1b/#步骤一","30":"/article/ebule7cy/#_2-3-triplet-attention的实现流程","31":"/article/3pv1asqx/#_2-4-osa","32":"/article/ln4qficp/#_4-1-msda添加步骤","33":"/article/l7o183q2/#四、手把手教你将lskattention添加到你的网络结构中","34":"/article/llrg5yeo/#_2-1-混合注意力变换器-hat","35":"/article/7qi9b2wu/#二、focused-linear-attention的机制原理","36":"/article/3cw8b7e9/#一、本文介绍","37":"/article/8djf7t1b/#步骤二","38":"/article/ebule7cy/#三、triplet-attention的完整代码","39":"/article/3pv1asqx/#_2-5-特征级联","40":"/article/ln4qficp/#_4-1-1-步骤一","41":"/article/l7o183q2/#_4-1-lskattention的添加教程","42":"/article/llrg5yeo/#三、hattention的核心代码","43":"/article/7qi9b2wu/#_2-1-softmax和线性注意力机制的对比","44":"/article/3cw8b7e9/#二、-deformable-lka机制原理","45":"/article/2p0fwpcg/#一、本文介绍","46":"/article/8djf7t1b/#步骤三","47":"/article/ebule7cy/#_3-1-triplet-attention的核心代码","48":"/article/uirz1e4c/#一、本文介绍","49":"/article/3pv1asqx/#三、rcs-osa核心代码","50":"/article/ln4qficp/#_4-1-2-步骤二","51":"/article/l7o183q2/#_4-2-lskattention的yaml文件和训练截图","52":"/article/llrg5yeo/#四、手把手教你添加hattention机制","53":"/article/7qi9b2wu/#_2-2-focused-linear-attention的提出","54":"/article/3cw8b7e9/#_2-1-deformable-lka的基本原理","55":"/article/2p0fwpcg/#二、dat的网络结构思想","56":"/article/8djf7t1b/#六、配置biformer注意力机制","57":"/article/ebule7cy/#_3-2-修改了triplet-attention机制的c2f和bottleneck","58":"/article/uirz1e4c/#_2-1-acmix的基本原理","59":"/article/3pv1asqx/#四、手把手教你添加rcs-osa模块","60":"/article/ln4qficp/#_4-1-3-步骤三","61":"/article/l7o183q2/#_4-2-1-lskattention的yaml文件","62":"/article/llrg5yeo/#修改一","63":"/article/7qi9b2wu/#_2-3-效果对比","64":"/article/3cw8b7e9/#_2-2-大卷积核","65":"/article/2p0fwpcg/#_2-1-dat的主要思想和改进","66":"/article/8djf7t1b/#七、训练模型","67":"/article/ebule7cy/#四、手把手教你添加triplet-attention","68":"/article/uirz1e4c/#_2-1-1-自注意力和卷积的整合","69":"/article/3pv1asqx/#_4-1-rcs-osa添加步骤","70":"/article/ln4qficp/#_4-2-msda的yaml文件和训练截图","71":"/article/l7o183q2/#_4-2-2-lskattention的训练过程截图","72":"/article/llrg5yeo/#修改二","73":"/article/7qi9b2wu/#三、实验效果对比","74":"/article/3cw8b7e9/#_2-3-可变形卷积","75":"/article/2p0fwpcg/#_2-2-dat的网络结构图","76":"/article/8djf7t1b/#八、结果分析","77":"/article/ebule7cy/#_4-1-triplet-attention的添加教程","78":"/article/uirz1e4c/#_2-1-2-运算分解与重构","79":"/article/3pv1asqx/#_4-1-1-步骤一","80":"/article/ln4qficp/#_4-2-1-msda的yaml版本一-推荐","81":"/article/l7o183q2/#五、lskattention可添加的位置","82":"/article/llrg5yeo/#五、hattention的yaml文件","83":"/article/7qi9b2wu/#四、focusedlinearattention代码","84":"/article/3cw8b7e9/#_2-4-2d和3d适应性","85":"/article/2p0fwpcg/#_2-3-dat和其他机制的对比","86":"/article/ebule7cy/#_4-2-triplet-attention的yaml文件和训练截图","87":"/article/uirz1e4c/#四、手把手教你添加acmix","88":"/article/3pv1asqx/#_4-1-2-步骤二","89":"/article/ln4qficp/#_4-2-2-msda的yaml版本二","90":"/article/l7o183q2/#_5-1-推荐lskattention可添加的位置","91":"/article/llrg5yeo/#_5-1-hattention的yaml文件一","92":"/article/7qi9b2wu/#五、添加focused-linear-attention到模型中","93":"/article/3cw8b7e9/#三、d-lka代码和c2f-d-lka","94":"/article/2p0fwpcg/#三、dat即插即用的代码块","95":"/article/ebule7cy/#_4-2-1-triplet-attention的yaml文件一-推荐","96":"/article/uirz1e4c/#_4-1-acmix添加步骤","97":"/article/3pv1asqx/#_4-1-3-步骤三","98":"/article/ln4qficp/#_4-3-推荐msda可添加的位置","99":"/article/l7o183q2/#_5-2图示lskattention可添加的位置","100":"/article/llrg5yeo/#_5-2-hattention的训练过程截图","101":"/article/7qi9b2wu/#_5-1-focused-linear-attention的添加教程","102":"/article/3cw8b7e9/#四、手把手教你添加d-lka","103":"/article/2p0fwpcg/#四、添加dat到你的网络中","104":"/article/ebule7cy/#_4-2-2-triplet-attention的yaml文件二","105":"/article/uirz1e4c/#_4-1-1-步骤一","106":"/article/3pv1asqx/#_4-2-rcs-osa的yaml文件和训练截图","107":"/article/ln4qficp/#_4-4-msda的训练过程截图","108":"/article/7qi9b2wu/#_5-2-focused-linear-attention的yaml文件和训练截图","109":"/article/3cw8b7e9/#_4-1-1-修改一","110":"/article/2p0fwpcg/#_4-1-dat的yaml文件和训练过程","111":"/article/ebule7cy/#_4-2-2-d-lka的训练过程截图","112":"/article/uirz1e4c/#_4-1-2-步骤二","113":"/article/3pv1asqx/#_4-2-1-rcs-osa的yaml版本一-推荐","114":"/article/7qi9b2wu/#_5-2-1-focused-linear-attention的yaml文件","115":"/article/3cw8b7e9/#_4-1-2-修改二","116":"/article/2p0fwpcg/#五、dat可添加的位置","117":"/article/ebule7cy/#五、triplet-attention可添加的位置","118":"/article/uirz1e4c/#_4-1-3-步骤三","119":"/article/3pv1asqx/#_4-2-2-rcs-osa的yaml版本二","120":"/article/7qi9b2wu/#_5-2-2-focused-linear-attention的训练过程截图","121":"/article/3cw8b7e9/#_4-1-3-修改三","122":"/article/2p0fwpcg/#_5-1推荐dat可添加的位置","123":"/article/ebule7cy/#_5-1-推荐triplet-attention可添加的位置","124":"/article/uirz1e4c/#五、acmix的yaml文件和运行记录","125":"/article/3pv1asqx/#_4-2-2-rcs-osa的训练过程截图","126":"/article/3cw8b7e9/#_4-1-4-修改四","127":"/article/2p0fwpcg/#_5-2图示dat可添加的位置","128":"/article/ebule7cy/#_5-2-图示d-lka可添加的位置","129":"/article/uirz1e4c/#_5-1-acmix的yaml版本一-推荐","130":"/article/3cw8b7e9/#_4-2-d-lka的yaml文件和训练截图-仔细看这个否则会报错","131":"/article/uirz1e4c/#_5-2-acmix的yaml版本二","132":"/article/3cw8b7e9/#_4-2-1-d-lka的yaml文件一-推荐","133":"/article/uirz1e4c/#_5-3-推荐acmix可添加的位置","134":"/article/3cw8b7e9/#_4-2-2-d-lka的yaml文件二","135":"/article/uirz1e4c/#_5-4-acmix的训练过程截图","136":"/article/3cw8b7e9/#_4-2-2-d-lka的训练过程截图","137":"/article/3cw8b7e9/#五、d-lka可添加的位置","138":"/article/3cw8b7e9/#_5-1-推荐d-lka可添加的位置","139":"/article/3cw8b7e9/#_5-2-图示d-lka可添加的位置"},"fieldIds":{"title":0,"titles":1,"text":2},"fieldLength":{"0":[2,1,1],"1":[2,1,19],"2":[2,3,1],"3":[2,1,24],"4":[2,1,98],"5":[2,1,19],"6":[2,4,1],"7":[3,1,5],"8":[2,1,23],"9":[2,1,52],"10":[3,1,5],"11":[2,5,1],"12":[4,4,34],"13":[2,1,97],"14":[2,1,16],"15":[2,1,33],"16":[4,4,55],"17":[2,6,173],"18":[2,4,16],"19":[2,1,311],"20":[2,1,106],"21":[2,1,12],"22":[2,1,8],"23":[3,4,65],"24":[3,4,33],"25":[2,1,1],"26":[2,1,64],"27":[2,1,81],"28":[2,1,22],"29":[1,3,346],"30":[4,4,40],"31":[3,4,46],"32":[3,3,1],"33":[2,1,1],"34":[5,3,33],"35":[4,1,5],"36":[2,1,18],"37":[1,3,12],"38":[3,1,1],"39":[3,4,47],"40":[3,6,9],"41":[3,3,24],"42":[2,1,582],"43":[3,5,53],"44":[3,1,6],"45":[2,1,26],"46":[1,3,20],"47":[4,4,84],"48":[2,1,19],"49":[3,1,221],"50":[4,6,12],"51":[3,3,1],"52":[2,1,8],"53":[4,5,37],"54":[4,4,29],"55":[2,1,7],"56":[2,1,150],"57":[4,4,104],"58":[3,3,25],"59":[3,1,1],"60":[4,6,13],"61":[4,6,10],"62":[1,3,12],"63":[3,5,66],"64":[2,4,51],"65":[3,3,31],"66":[2,1,13],"67":[3,1,1],"68":[3,6,51],"69":[4,4,1],"70":[3,3,8],"71":[3,6,2],"72":[1,3,18],"73":[2,1,7],"74":[3,4,12],"75":[2,3,22],"76":[2,1,8],"77":[4,4,22],"78":[3,6,183],"79":[3,8,9],"80":[6,6,138],"81":[2,1,1],"82":[2,1,9],"83":[2,1,217],"84":[3,4,41],"85":[3,3,7],"86":[4,4,1],"87":[2,1,1],"88":[4,8,12],"89":[3,6,134],"90":[3,3,22],"91":[3,3,136],"92":[4,1,1],"93":[4,1,198],"94":[2,1,217],"95":[7,8,135],"96":[3,3,1],"97":[4,8,10],"98":[3,3,20],"99":[2,3,1],"100":[3,3,9],"101":[5,5,19],"102":[3,1,1],"103":[2,1,1],"104":[4,8,137],"105":[3,6,8],"106":[4,4,1],"107":[2,3,4],"108":[5,5,1],"109":[3,7,8],"110":[3,3,3],"111":[4,8,4],"112":[4,6,12],"113":[7,8,136],"114":[6,9,6],"115":[4,7,5],"116":[2,1,1],"117":[3,1,1],"118":[4,6,20],"119":[4,8,132],"120":[5,9,6],"121":[4,7,7],"122":[2,3,22],"123":[4,4,19],"124":[2,1,8],"125":[4,8,6],"126":[3,7,5],"127":[2,3,1],"128":[4,4,2],"129":[5,3,140],"130":[6,4,1],"131":[3,3,134],"132":[7,9,135],"133":[3,3,15],"134":[4,9,138],"135":[3,3,3],"136":[4,9,4],"137":[3,1,1],"138":[4,4,24],"139":[4,4,2]},"averageFieldLength":[3.0428571428571423,3.4714285714285715,45.48571428571429],"storedFields":{"0":{"title":"标题 2","titles":[null]},"1":{"title":"一、本文介绍","titles":[null]},"2":{"title":"标题 3","titles":[null,"标题 2"]},"3":{"title":"一、本文介绍","titles":[null]},"4":{"title":"二、Biformer的作用机制","titles":[null]},"5":{"title":"一、本文介绍","titles":[null]},"6":{"title":"标题 4","titles":[null,"标题 2","标题 3"]},"7":{"title":"二、RCS-OSA模块原理","titles":[null]},"8":{"title":"一、本文介绍","titles":[null]},"9":{"title":"三、Biformer的优劣势","titles":[null]},"10":{"title":"二、Triplet Attention机制原理","titles":[null]},"11":{"title":"标题 5","titles":[null,"标题 2","标题 3","标题 4"]},"12":{"title":"2.1 RCS-OSA的基本原理","titles":[null,"二、RCS-OSA模块原理"]},"13":{"title":"二、MSDA框架原理","titles":[null]},"14":{"title":"一、本文介绍","titles":[null]},"15":{"title":"四、Biformer的结构","titles":[null]},"16":{"title":"2.1 Triplet Attention的基本原理","titles":[null,"二、Triplet Attention机制原理"]},"17":{"title":"标题 6","titles":[null,"标题 2","标题 3","标题 4","标题 5"]},"18":{"title":"2.2 RCS","titles":[null,"二、RCS-OSA模块原理"]},"19":{"title":"三、MSDA核心代码","titles":[null]},"20":{"title":"二、LSKAttention的机制原理","titles":[null]},"21":{"title":"一、本文介绍","titles":[null]},"22":{"title":"五、添加Biformer注意力机制","titles":[null]},"23":{"title":"2.2 Triplet Attention和其它简单注意力机制的对比","titles":[null,"二、Triplet Attention机制原理"]},"24":{"title":"2.3 RCS模块","titles":[null,"二、RCS-OSA模块原理"]},"25":{"title":"四、手把手教你添加MSDA模块","titles":[null]},"26":{"title":"三、LSKAttention的代码","titles":[null]},"27":{"title":"二、HAttention框架原理","titles":[null]},"28":{"title":"一、本文介绍","titles":[null]},"29":{"title":"步骤一","titles":[null,"五、添加Biformer注意力机制"]},"30":{"title":"2.3 Triplet Attention的实现流程","titles":[null,"二、Triplet Attention机制原理"]},"31":{"title":"2.4 OSA","titles":[null,"二、RCS-OSA模块原理"]},"32":{"title":"4.1 MSDA添加步骤","titles":[null,"四、手把手教你添加MSDA模块"]},"33":{"title":"四、手把手教你将LSKAttention添加到你的网络结构中","titles":[null]},"34":{"title":"2.1 混合注意力变换器（HAT）","titles":[null,"二、HAttention框架原理"]},"35":{"title":"二、Focused Linear Attention的机制原理","titles":[null]},"36":{"title":"一、本文介绍","titles":[null]},"37":{"title":"步骤二","titles":[null,"五、添加Biformer注意力机制"]},"38":{"title":"三、Triplet Attention的完整代码","titles":[null]},"39":{"title":"2.5 特征级联","titles":[null,"二、RCS-OSA模块原理"]},"40":{"title":"4.1.1 步骤一","titles":[null,"四、手把手教你添加MSDA模块","4.1 MSDA添加步骤"]},"41":{"title":"4.1 LSKAttention的添加教程","titles":[null,"四、手把手教你将LSKAttention添加到你的网络结构中"]},"42":{"title":"三、HAttention的核心代码","titles":[null]},"43":{"title":"2.1 Softmax和线性注意力机制的对比","titles":[null,"二、Focused Linear Attention的机制原理"]},"44":{"title":"二、 Deformable-LKA机制原理","titles":[null]},"45":{"title":"一、本文介绍","titles":[null]},"46":{"title":"步骤三","titles":[null,"五、添加Biformer注意力机制"]},"47":{"title":"3.1 Triplet Attention的核心代码","titles":[null,"三、Triplet Attention的完整代码"]},"48":{"title":"一、本文介绍","titles":[null]},"49":{"title":"三、RCS-OSA核心代码","titles":[null]},"50":{"title":"4.1.2 步骤二","titles":[null,"四、手把手教你添加MSDA模块","4.1 MSDA添加步骤"]},"51":{"title":"4.2 LSKAttention的yaml文件和训练截图","titles":[null,"四、手把手教你将LSKAttention添加到你的网络结构中"]},"52":{"title":"四、手把手教你添加HAttention机制","titles":[null]},"53":{"title":"2.2 Focused Linear Attention的提出","titles":[null,"二、Focused Linear Attention的机制原理"]},"54":{"title":"2.1 Deformable-LKA的基本原理","titles":[null,"二、 Deformable-LKA机制原理"]},"55":{"title":"二、DAT的网络结构思想","titles":[null]},"56":{"title":"六、配置Biformer注意力机制","titles":[null]},"57":{"title":"3.2 修改了Triplet Attention机制的C2f和Bottleneck","titles":[null,"三、Triplet Attention的完整代码"]},"58":{"title":"2.1 ACMix的基本原理","titles":[null,"一、本文介绍"]},"59":{"title":"四、手把手教你添加RCS-OSA模块","titles":[null]},"60":{"title":"4.1.3 步骤三","titles":[null,"四、手把手教你添加MSDA模块","4.1 MSDA添加步骤"]},"61":{"title":"4.2.1 LSKAttention的yaml文件","titles":[null,"四、手把手教你将LSKAttention添加到你的网络结构中","4.2 LSKAttention的yaml文件和训练截图"]},"62":{"title":"修改一","titles":[null,"四、手把手教你添加HAttention机制"]},"63":{"title":"2.3 效果对比","titles":[null,"二、Focused Linear Attention的机制原理"]},"64":{"title":"2.2 大卷积核","titles":[null,"二、 Deformable-LKA机制原理"]},"65":{"title":"2.1 DAT的主要思想和改进","titles":[null,"二、DAT的网络结构思想"]},"66":{"title":"七、训练模型","titles":[null]},"67":{"title":"四、手把手教你添加Triplet Attention","titles":[null]},"68":{"title":"2.1.1 自注意力和卷积的整合","titles":[null,"一、本文介绍","2.1 ACMix的基本原理"]},"69":{"title":"4.1 RCS-OSA添加步骤","titles":[null,"四、手把手教你添加RCS-OSA模块"]},"70":{"title":"4.2 MSDA的yaml文件和训练截图","titles":[null,"四、手把手教你添加MSDA模块"]},"71":{"title":"4.2.2 LSKAttention的训练过程截图","titles":[null,"四、手把手教你将LSKAttention添加到你的网络结构中","4.2 LSKAttention的yaml文件和训练截图"]},"72":{"title":"修改二","titles":[null,"四、手把手教你添加HAttention机制"]},"73":{"title":"三、实验效果对比","titles":[null]},"74":{"title":"2.3 可变形卷积","titles":[null,"二、 Deformable-LKA机制原理"]},"75":{"title":"2.2 DAT的网络结构图","titles":[null,"二、DAT的网络结构思想"]},"76":{"title":"八、结果分析","titles":[null]},"77":{"title":"4.1 Triplet Attention的添加教程","titles":[null,"四、手把手教你添加Triplet Attention"]},"78":{"title":"2.1.2 运算分解与重构","titles":[null,"一、本文介绍","2.1 ACMix的基本原理"]},"79":{"title":"4.1.1 步骤一","titles":[null,"四、手把手教你添加RCS-OSA模块","4.1 RCS-OSA添加步骤"]},"80":{"title":"4.2.1 MSDA的yaml版本一(推荐)","titles":[null,"四、手把手教你添加MSDA模块","4.2 MSDA的yaml文件和训练截图"]},"81":{"title":"五、LSKAttention可添加的位置","titles":[null]},"82":{"title":"五、HAttention的yaml文件","titles":[null]},"83":{"title":"四、FocusedLinearAttention代码","titles":[null]},"84":{"title":"2.4 2D和3D适应性","titles":[null,"二、 Deformable-LKA机制原理"]},"85":{"title":"2.3 DAT和其他机制的对比","titles":[null,"二、DAT的网络结构思想"]},"86":{"title":"4.2 Triplet Attention的yaml文件和训练截图","titles":[null,"四、手把手教你添加Triplet Attention"]},"87":{"title":"四、手把手教你添加ACmix","titles":[null]},"88":{"title":"4.1.2 步骤二","titles":[null,"四、手把手教你添加RCS-OSA模块","4.1 RCS-OSA添加步骤"]},"89":{"title":"4.2.2 MSDA的yaml版本二","titles":[null,"四、手把手教你添加MSDA模块","4.2 MSDA的yaml文件和训练截图"]},"90":{"title":"5.1 推荐LSKAttention可添加的位置","titles":[null,"五、LSKAttention可添加的位置"]},"91":{"title":"5.1 HAttention的yaml文件一","titles":[null,"五、HAttention的yaml文件"]},"92":{"title":"五、添加Focused Linear Attention到模型中","titles":[null]},"93":{"title":"三、D-LKA代码和C2f-D-LKA","titles":[null]},"94":{"title":"三、DAT即插即用的代码块","titles":[null]},"95":{"title":"4.2.1 Triplet Attention的yaml文件一(推荐)","titles":[null,"四、手把手教你添加Triplet Attention","4.2 Triplet Attention的yaml文件和训练截图"]},"96":{"title":"4.1 ACmix添加步骤","titles":[null,"四、手把手教你添加ACmix"]},"97":{"title":"4.1.3 步骤三","titles":[null,"四、手把手教你添加RCS-OSA模块","4.1 RCS-OSA添加步骤"]},"98":{"title":"4.3 推荐MSDA可添加的位置","titles":[null,"四、手把手教你添加MSDA模块"]},"99":{"title":"5.2图示LSKAttention可添加的位置","titles":[null,"五、LSKAttention可添加的位置"]},"100":{"title":"5.2 HAttention的训练过程截图","titles":[null,"五、HAttention的yaml文件"]},"101":{"title":"5.1 Focused Linear Attention的添加教程","titles":[null,"五、添加Focused Linear Attention到模型中"]},"102":{"title":"四、手把手教你添加D-LKA","titles":[null]},"103":{"title":"四、添加DAT到你的网络中","titles":[null]},"104":{"title":"4.2.2 Triplet Attention的yaml文件二","titles":[null,"四、手把手教你添加Triplet Attention","4.2 Triplet Attention的yaml文件和训练截图"]},"105":{"title":"4.1.1 步骤一","titles":[null,"四、手把手教你添加ACmix","4.1 ACmix添加步骤"]},"106":{"title":"4.2 RCS-OSA的yaml文件和训练截图","titles":[null,"四、手把手教你添加RCS-OSA模块"]},"107":{"title":"4.4 MSDA的训练过程截图","titles":[null,"四、手把手教你添加MSDA模块"]},"108":{"title":"5.2 Focused Linear Attention的yaml文件和训练截图","titles":[null,"五、添加Focused Linear Attention到模型中"]},"109":{"title":"4.1.1 修改一","titles":[null,"四、手把手教你添加D-LKA","2.4 2D和3D适应性"]},"110":{"title":"4.1 DAT的yaml文件和训练过程","titles":[null,"四、添加DAT到你的网络中"]},"111":{"title":"4.2.2 D-LKA的训练过程截图","titles":[null,"四、手把手教你添加Triplet Attention","4.2 Triplet Attention的yaml文件和训练截图"]},"112":{"title":"4.1.2 步骤二","titles":[null,"四、手把手教你添加ACmix","4.1 ACmix添加步骤"]},"113":{"title":"4.2.1 RCS-OSA的yaml版本一(推荐)","titles":[null,"四、手把手教你添加RCS-OSA模块","4.2 RCS-OSA的yaml文件和训练截图"]},"114":{"title":"5.2.1 Focused Linear Attention的yaml文件","titles":[null,"五、添加Focused Linear Attention到模型中","5.2 Focused Linear Attention的yaml文件和训练截图"]},"115":{"title":"4.1.2 修改二","titles":[null,"四、手把手教你添加D-LKA","2.4 2D和3D适应性"]},"116":{"title":"五、DAT可添加的位置","titles":[null]},"117":{"title":"五、Triplet Attention可添加的位置","titles":[null]},"118":{"title":"4.1.3 步骤三","titles":[null,"四、手把手教你添加ACmix","4.1 ACmix添加步骤"]},"119":{"title":"4.2.2 RCS-OSA的yaml版本二","titles":[null,"四、手把手教你添加RCS-OSA模块","4.2 RCS-OSA的yaml文件和训练截图"]},"120":{"title":"5.2.2 Focused Linear Attention的训练过程截图","titles":[null,"五、添加Focused Linear Attention到模型中","5.2 Focused Linear Attention的yaml文件和训练截图"]},"121":{"title":"4.1.3 修改三","titles":[null,"四、手把手教你添加D-LKA","2.4 2D和3D适应性"]},"122":{"title":"5.1推荐DAT可添加的位置","titles":[null,"五、DAT可添加的位置"]},"123":{"title":"5.1 推荐Triplet Attention可添加的位置","titles":[null,"五、Triplet Attention可添加的位置"]},"124":{"title":"五、ACmix的yaml文件和运行记录","titles":[null]},"125":{"title":"4.2.2 RCS-OSA的训练过程截图","titles":[null,"四、手把手教你添加RCS-OSA模块","4.2 RCS-OSA的yaml文件和训练截图"]},"126":{"title":"4.1.4 修改四","titles":[null,"四、手把手教你添加D-LKA","2.4 2D和3D适应性"]},"127":{"title":"5.2图示DAT可添加的位置","titles":[null,"五、DAT可添加的位置"]},"128":{"title":"5.2 图示D-LKA可添加的位置","titles":[null,"五、Triplet Attention可添加的位置"]},"129":{"title":"5.1 ACMix的yaml版本一(推荐)","titles":[null,"五、ACmix的yaml文件和运行记录"]},"130":{"title":"4.2 D-LKA的yaml文件和训练截图(仔细看这个否则会报错)","titles":[null,"四、手把手教你添加D-LKA"]},"131":{"title":"5.2 ACMix的yaml版本二","titles":[null,"五、ACmix的yaml文件和运行记录"]},"132":{"title":"4.2.1 D-LKA的yaml文件一(推荐)","titles":[null,"四、手把手教你添加D-LKA","4.2 D-LKA的yaml文件和训练截图(仔细看这个否则会报错)"]},"133":{"title":"5.3 推荐ACMix可添加的位置","titles":[null,"五、ACmix的yaml文件和运行记录"]},"134":{"title":"4.2.2 D-LKA的yaml文件二","titles":[null,"四、手把手教你添加D-LKA","4.2 D-LKA的yaml文件和训练截图(仔细看这个否则会报错)"]},"135":{"title":"5.4 ACMix的训练过程截图","titles":[null,"五、ACmix的yaml文件和运行记录"]},"136":{"title":"4.2.2 D-LKA的训练过程截图","titles":[null,"四、手把手教你添加D-LKA","4.2 D-LKA的yaml文件和训练截图(仔细看这个否则会报错)"]},"137":{"title":"五、D-LKA可添加的位置","titles":[null]},"138":{"title":"5.1 推荐D-LKA可添加的位置","titles":[null,"五、D-LKA可添加的位置"]},"139":{"title":"5.2 图示D-LKA可添加的位置","titles":[null,"五、D-LKA可添加的位置"]}},"dirtCount":0,"index":[["检测头中的卷积",{"2":{"138":1}}],["检查权重的数据类型",{"2":{"94":1}}],["那个效果更好针对不同的数据集可能也不一样所以大家需要进行尝试",{"2":{"134":1}}],["仔细看这个否则会报错",{"0":{"130":1},"1":{"132":1,"134":1,"136":1}}],["帮助大家省了一些事",{"2":{"114":1}}],["无需进行传入会根据模型输入自动计算",{"2":{"114":1}}],["无序列表3",{"2":{"17":1}}],["无序列表2",{"2":{"17":1}}],["无序列表1",{"2":{"17":1}}],["第三步我门中到如下文件",{"2":{"121":1}}],["第三个分支则聚焦于图像的深度",{"2":{"5":1}}],["第二步我们在该目录下创建一个新的py文件名字为",{"2":{"115":1}}],["第一还是建立文件",{"2":{"109":1}}],["估计也不影响运行和精度就没去处理",{"2":{"100":1}}],["给大家",{"2":{"98":1}}],["能添加的位置很多",{"2":{"98":1}}],["能够综合不同维度上的信息",{"2":{"30":1}}],["能够在你自己的网络结构中进行添加",{"2":{"28":1}}],["能够在不同的计算机视觉任务中发挥优异的性能",{"2":{"15":1}}],["能够有效降低内存占用和计算成本",{"2":{"20":1}}],["能够看到图片的立体效果一样",{"2":{"5":1}}],["更加集中注意力于最关键的特征",{"2":{"90":1,"122":1,"138":1}}],["更好地捕获数据的内在特征",{"2":{"30":1}}],["输出层前",{"2":{"90":1,"122":1}}],["输入为四维",{"2":{"83":1}}],["输入特征先转换成查询",{"2":{"78":1}}],["输入的低分辨率",{"2":{"34":1}}],["输入被分为两部分",{"2":{"31":1}}],["输入通过通道分割",{"2":{"24":1}}],["残差连接中",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["当然不一定要按照我推荐的地方添加",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["当然如果你不想用快捷键也可以自己寻找大概在",{"2":{"46":1}}],["置大家可以进行参考",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["突出了它们处理查询的不同方法",{"2":{"85":1}}],["充分利用体积图像数据中的空间上下文信息",{"2":{"84":1}}],["技术应用于不同维度数据的能力",{"2":{"84":1}}],["序列长度",{"2":{"83":1}}],["调整输入以匹配原始模块的预期格式",{"2":{"83":1}}],["宽度",{"2":{"83":1}}],["宽度三个维度之间的交互关系来计算注意力权重",{"2":{"16":1}}],["批次大小",{"2":{"83":1}}],["创作不易而且免费给大家看",{"2":{"83":1}}],["创建一个py文件粘贴进去hattention",{"2":{"62":1}}],["创建一个py文件粘贴进去",{"2":{"42":1}}],["后期我会发文件里面集成上百种的改进机制",{"2":{"98":1}}],["后面大家有什么想看的机制都可以指定",{"2":{"91":1}}],["后面经过各种处理信息早已经丢失了",{"2":{"82":1}}],["后合并",{"2":{"31":1}}],["或者neck的输出部分是最好的",{"2":{"82":1}}],["涨点效果最好",{"2":{"82":1}}],["像这种注意力机制不要添加在主干上",{"2":{"82":1}}],["推荐d",{"0":{"138":1}}],["推荐acmix可添加的位置",{"0":{"133":1}}],["推荐triplet",{"0":{"123":1}}],["推荐msda可添加的位置",{"0":{"98":1}}],["推荐lskattention可添加的位置",{"0":{"90":1}}],["推荐",{"0":{"80":1,"95":1,"113":1,"129":1,"132":1}}],["强调了两种机制的融合并提供了每个操作块的计算复杂度",{"2":{"78":1}}],["强调在计算注意力权重时",{"2":{"16":1}}],["右图显示了acmix模块的流程",{"2":{"78":1}}],["右侧是2d",{"2":{"84":1}}],["右侧的表格详细列出了不同模型的分辨率",{"2":{"63":1}}],["右侧",{"2":{"15":1,"84":1}}],["作为最终输出",{"2":{"78":1}}],["作者提出了一个新颖的聚焦线性注意力",{"2":{"53":1}}],["作者提出了一种新的预训练方法",{"2":{"27":1}}],["作者的模型",{"2":{"34":1}}],["作者还提出了一个重叠交叉注意模块来增强跨窗口信息的交互",{"2":{"27":1}}],["既包含了卷积的空间特征提取能力",{"2":{"78":1}}],["八",{"0":{"76":1}}],["根据变形点从采样特征中投影出变形的键和值",{"2":{"75":1}}],["根据查询自适应地关注最相关的键",{"2":{"15":1}}],["被用来增强模型对医学图像中的不规则形状和大小的捕捉能力",{"2":{"74":1}}],["被用来展示其它各个块对它的注意力程度",{"2":{"13":1}}],["实验效果图如下所示",{"2":{"73":1}}],["实验效果对比",{"0":{"73":1}}],["实现两者优势的结合",{"2":{"58":1}}],["实现了对象检测的任务",{"2":{"39":1}}],["实现了检测精度提升",{"2":{"14":1}}],["实现快速推理",{"2":{"18":1}}],["实现内容感知的稀疏性",{"2":{"15":1}}],["让使用方法统一起来所以大家用着很简单",{"2":{"72":1}}],["为什么这么简单是因为我修改了官方的代码",{"2":{"72":1}}],["为了清晰展示",{"2":{"75":1}}],["为了解决这些问题",{"2":{"53":1}}],["为了后面提出论文提到的注意力机制在线性注意力机制上的优化",{"2":{"43":1}}],["为了以高效的方式全局定位有价值的键",{"2":{"4":1}}],["为了减少注意力的复杂度",{"2":{"4":1}}],["按照我的添加在parse",{"2":{"126":1}}],["按照我的方法进行添加",{"2":{"72":1}}],["按快捷键ctrl+f可以进行文件搜索",{"2":{"46":1}}],["针对不同的数据集效果也不一样",{"2":{"70":1,"124":1}}],["整体上",{"2":{"68":1}}],["包含多个3d",{"2":{"84":1}}],["包含一个k2",{"2":{"68":1}}],["包括",{"2":{"64":1}}],["包括1x1和3x3的卷积",{"2":{"24":1}}],["卷积",{"2":{"68":1,"78":1}}],["卷积和自注意力生成的特征通过求和操作进行融合",{"2":{"68":1}}],["卷积和自注意力共享相同的1x1卷积运算",{"2":{"68":1}}],["加强了模型的特征提取能力",{"2":{"68":1}}],["加粗文字",{"2":{"17":1}}],["加粗",{"2":{"17":1}}],["运算共享",{"2":{"68":1}}],["运算分解与重构的概念是指将传统的卷积运算和自注意力运算拆分",{"2":{"78":1}}],["运算分解与重构",{"0":{"78":1},"2":{"58":1}}],["键",{"2":{"68":1,"78":2}}],["键和值矩阵",{"2":{"43":1}}],["键和值",{"2":{"8":1,"13":1}}],["控制台进行了模型结构的输出如下",{"2":{"66":1}}],["训练模型",{"0":{"66":1}}],["七",{"0":{"66":1}}],["改进了视觉transformer的效率和性能",{"2":{"65":1}}],["动态采样点",{"2":{"65":1}}],["增加非线性",{"2":{"64":1}}],["增强了输出转换特征的多头注意力",{"2":{"75":1}}],["增强了模型的表现力",{"2":{"13":1}}],["增强了模型的学习能力和效率",{"2":{"13":1}}],["增强网络的表征能力",{"2":{"68":1}}],["增强网络对复杂数据结构的理解和处理能力",{"2":{"30":1}}],["增强网络对特征的重要部分的关注度",{"2":{"23":1}}],["激活函数gelu",{"2":{"64":1}}],["偏移场",{"2":{"64":1}}],["允许网络根据输入特征自适应地调整其感受野",{"2":{"64":1}}],["允许模型的采样网格根据图像特征灵活变形",{"2":{"54":1}}],["带有偏移量的变形卷积",{"2":{"64":1}}],["机制的感受野",{"2":{"64":1}}],["括号中的百分点",{"2":{"63":1}}],["表中突出了flatten版本的transformer模型在top",{"2":{"63":1}}],["表示卷积核大小和卷积操作的聚合",{"2":{"68":1}}],["表示使用二维卷积神经网络的检测层",{"2":{"39":1}}],["表示参与的像素范围",{"2":{"34":1}}],["计算量",{"2":{"63":1}}],["计算效率提升",{"2":{"20":1}}],["类似地",{"2":{"63":1}}],["对应实验结果的图二",{"2":{"104":1}}],["对象检测等",{"2":{"65":1}}],["对比了swin",{"2":{"63":1}}],["对比了不同版本的pvt",{"2":{"63":1}}],["对于构建轻量级和大规模的对象检测器尤为重要",{"2":{"31":1}}],["对于每个头部i",{"2":{"13":1}}],["对于这种情况",{"2":{"4":1}}],["效果对比",{"0":{"63":1}}],["修改四",{"0":{"126":1}}],["修改三",{"0":{"121":1}}],["修改了flattention的c2f和bottleneck",{"2":{"83":1}}],["修改了triplet",{"0":{"57":1}}],["修改二",{"0":{"72":1,"115":1}}],["修改一",{"0":{"62":1,"109":1}}],["代表多头注意力机制中每个头部的线性变换",{"2":{"68":1}}],["代表卷积核的大小",{"2":{"61":1}}],["代码演示",{"2":{"17":1}}],["代码块聚焦",{"2":{"17":1}}],["代码块高亮",{"2":{"17":1}}],["代码分组",{"2":{"17":1}}],["代码",{"2":{"17":1}}],["代码地址",{"2":{"4":1,"7":1,"10":1,"13":1,"20":1,"35":1,"44":1,"55":1}}],["参数我以及设定好了",{"2":{"114":1}}],["参数量为407120",{"2":{"113":1}}],["参数数量",{"2":{"63":1}}],["参数位置可以填写的有7",{"2":{"61":1}}],["参数调整的挑战",{"2":{"9":1}}],["到此就修改完成了",{"2":{"72":1,"126":1}}],["到此我们的所有准备工作都已完成",{"2":{"66":1}}],["到此我们就注册成功了",{"2":{"60":1,"97":1,"118":1}}],["到这里我们就已经成功的导入了注意力机制",{"2":{"56":1}}],["你现在不知道其是干什么用的可以看添加教程",{"2":{"101":1}}],["你放在主干上",{"2":{"82":1}}],["你也可以自己进行组合",{"2":{"70":1,"124":1}}],["你也可以根据你自己的习惯起",{"2":{"40":1}}],["你没有删除即可",{"2":{"60":1}}],["自注意力",{"2":{"68":1,"78":1}}],["自注意力和acmix各自的结构和计算复杂度",{"2":{"68":1}}],["自注意力和卷积的整合通过以下方式实现",{"2":{"68":1}}],["自注意力和卷积的整合",{"0":{"68":1},"2":{"58":1}}],["自注意力机制的查询",{"2":{"68":1}}],["自注意力子层使用双层路由注意力机制",{"2":{"15":1}}],["🚀",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["找到这个文件之后初始如下所示",{"2":{"56":1}}],["离修改模型只差最后一步",{"2":{"56":1}}],["恭喜你",{"2":{"56":1}}],["配置biformer注意力机制",{"0":{"56":1}}],["六",{"0":{"56":1}}],["文字大家可能看我描述不太懂",{"2":{"123":1,"138":1}}],["文章中指出",{"2":{"68":1}}],["文末有修改了官方代码bug的代码块复制粘贴即可",{"2":{"55":1}}],["文件夹",{"2":{"109":1}}],["文件",{"2":{"50":1,"88":1,"112":1}}],["适应不同的数据模式",{"2":{"54":1}}],["适用于常见的医学成像方法",{"2":{"84":1}}],["适用于多种任务",{"2":{"20":1}}],["适用于各种计算机视觉任务",{"2":{"15":1}}],["大概在六百多行吧",{"2":{"60":1,"97":1,"118":1}}],["大卷积核与可变形卷积结合使用",{"2":{"64":1}}],["大卷积核",{"0":{"64":1},"2":{"54":1,"64":1}}],["大家可能看我描述不太懂",{"2":{"90":1,"122":1}}],["大家可以复制下面的yaml文件运行",{"2":{"126":1}}],["大家可以复制进行训练",{"2":{"70":1,"124":1}}],["大家可以看下面的运行结果和添加的位置所以不存在我发的代码不全或者运行不了的问题大家有问题也可以在评论区评论我看到都会为大家解答",{"2":{"100":1,"125":1}}],["大家可以看下面的网络结构图中我进行了标注",{"2":{"90":1,"122":1,"123":1,"138":1}}],["大家可以看我下面的文章",{"2":{"41":1,"77":1}}],["大家可以自行训练",{"2":{"76":1}}],["大家应该就清楚了",{"2":{"56":1}}],["大家在下面的代码中可以看到",{"2":{"49":1}}],["大家复制代码的时候需要注意这是一种无参数的注意力机制",{"2":{"47":1}}],["影响了模型的表现力",{"2":{"53":1}}],["传统的transformer使用标准的自注意力机制",{"2":{"65":1}}],["传统方法在特征表达上缺乏多样性",{"2":{"53":1}}],["传统卷积操作和自注意力模块的大部分计算都可以通过1x1的卷积来实现",{"2":{"48":1,"58":1}}],["导致计算量很大",{"2":{"65":1}}],["导致计算复杂度为",{"2":{"43":1}}],["导致计算复杂度较高",{"2":{"9":1}}],["导致模型难以有效地关注重要特征",{"2":{"53":1}}],["应用于原始注意力矩阵的秩恢复模块来增加特征多样性",{"2":{"53":1}}],["还包括了作者提出的flatten版本的transformer模型",{"2":{"63":1}}],["还通过深度卷积",{"2":{"53":1}}],["还能够在目标检测",{"2":{"20":1}}],["尽管线性注意力降低了复杂度",{"2":{"53":1}}],["方便大家使用",{"2":{"52":1}}],["又能通过卷积捕获局部特征",{"2":{"48":1,"58":1}}],["现在我们以及将biformer文件导入了模型中了",{"2":{"46":1}}],["听着是不是和可变形动态卷积dcn挺相似",{"2":{"45":1}}],["尤其是在处理大规模数据集时的优势",{"2":{"43":1}}],["享受更大的接收域和更高的吞吐量的好处",{"2":{"43":1}}],["此版本的gflops大概涨到了24",{"2":{"113":1}}],["此处提出了线性注意力机制的优势",{"2":{"43":1}}],["此外",{"2":{"18":1,"27":1,"53":1,"54":1}}],["例如",{"2":{"43":1}}],["线性注意力的限制和改进",{"2":{"53":1}}],["线性注意力模块因此能够在节省计算成本的同时",{"2":{"43":1}}],["线性注意力模块实际上降低了总体计算成本",{"2":{"43":1}}],["线性注意力可以解耦softmax操作",{"2":{"43":1}}],["线性注意力",{"2":{"43":1}}],["`activating",{"2":{"42":1}}],["|",{"2":{"42":10}}],["里面详细的介绍了拿到一个任意机制",{"2":{"41":1,"77":1}}],["添加dat到你的网络中",{"0":{"103":1},"1":{"110":1}}],["添加focused",{"0":{"92":1},"1":{"101":1,"108":1,"114":1,"120":1}}],["添加的位置是检测头和neck中间的位置",{"2":{"104":1}}],["添加的位置不同效果也不同",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["添加的版本二具体那种适合你需要大家自己多做实验来尝试",{"2":{"89":1,"119":1,"131":1}}],["添加在检测头里",{"2":{"82":1}}],["添加之后的效果如下",{"2":{"56":1}}],["添加如下图所示内容",{"2":{"46":1}}],["添加过程又需要截特别图片会导致文章大家读者也不通顺如果你已经会添加注意力机制了",{"2":{"41":1,"77":1}}],["添加教程这里不再重复介绍",{"2":{"41":1,"77":1}}],["添加biformer注意力机制",{"0":{"22":1},"1":{"29":1,"37":1,"46":1}}],["名字可以根据你自己的习惯起",{"2":{"105":1}}],["名字为rcsosa即可",{"2":{"79":1}}],["名字为dilation即可",{"2":{"40":1}}],["名为biformer",{"2":{"4":1}}],["构成",{"2":{"39":1}}],["橙色模块",{"2":{"39":1}}],["蓝色模块",{"2":{"39":1}}],["路径上维持有限数量的特征级联来实现的",{"2":{"39":1}}],["步骤三",{"0":{"46":1,"60":1,"97":1,"118":1}}],["步骤二",{"0":{"37":1,"50":1,"88":1,"112":1}}],["步骤一",{"0":{"29":1,"40":1,"79":1,"105":1}}],["数值越高表示使用的像素越多",{"2":{"34":1}}],["数学表达式",{"2":{"17":1}}],["扩散指数",{"2":{"34":1}}],["扩张深度卷积",{"2":{"20":1}}],["手把手教你添加d",{"0":{"102":1},"1":{"109":1,"115":1,"121":1,"126":1,"130":1,"132":1,"134":1,"136":1}}],["手把手教你添加acmix",{"0":{"87":1},"1":{"96":1,"105":1,"112":1,"118":1}}],["手把手教你添加triplet",{"0":{"67":1},"1":{"77":1,"86":1,"95":1,"104":1,"111":1}}],["手把手教你添加rcs",{"0":{"59":1},"1":{"69":1,"79":1,"88":1,"97":1,"106":1,"113":1,"119":1,"125":1}}],["手把手教你添加hattention机制",{"0":{"52":1},"1":{"62":1,"72":1}}],["手把手教你添加msda模块",{"0":{"25":1},"1":{"32":1,"40":1,"50":1,"60":1,"70":1,"80":1,"89":1,"98":1,"107":1}}],["手把手教你将lskattention添加到你的网络结构中",{"0":{"33":1},"1":{"41":1,"51":1,"61":1,"71":1}}],["处理后的特征和直接通过的特征在通道混洗",{"2":{"31":1}}],["形成rcs",{"2":{"31":1}}],["最终的效果图如下",{"2":{"75":1}}],["最终得到三重注意力输出",{"2":{"30":1}}],["最后强调一下triplet",{"2":{"77":1}}],["最后",{"2":{"23":1,"27":1}}],["最后经过另一个卷积层和sigmoid函数生成注意力权重",{"2":{"23":1}}],["最后通过sigmoid函数生成每个通道的权重",{"2":{"23":1}}],["最后本文会手把手教你添加rcs",{"2":{"3":1}}],["之后我们找到",{"2":{"50":1,"88":1,"112":1}}],["之后也通过sigmoid函数生成注意力权重",{"2":{"30":1}}],["之后同样进行残差变换",{"2":{"16":1}}],["执行z池化和卷积操作",{"2":{"30":1}}],["负责捕获通道维度c与空间维度h和w之间的依赖性",{"2":{"30":1}}],["负责计算通道维度c和空间维度w的注意力权重",{"2":{"30":1}}],["操作",{"2":{"30":1}}],["just",{"2":{"42":1}}],["j=self",{"2":{"29":5}}],["j",{"2":{"29":10,"83":9}}],["jit",{"2":{"19":1,"42":2}}],["该代码本身存在一个bug",{"2":{"78":1}}],["该结果只能展示出该机制有效",{"2":{"73":1}}],["该模块由多个卷积层组成",{"2":{"64":1}}],["该模块通过简单的映射函数调整查询和键的特征方向",{"2":{"53":1}}],["该目录的构造如下",{"2":{"29":1}}],["该架构包括多个biformer块的堆叠",{"2":{"15":1}}],["简言之",{"2":{"28":1}}],["简称bra",{"2":{"4":1}}],["聚焦能力",{"2":{"53":1}}],["聚焦能力和特征多样性",{"2":{"28":1}}],["聚焦线性注意力",{"2":{"28":1,"53":1}}],["旨在减少计算开销并整合轻量级的聚合操作",{"2":{"68":1}}],["旨在提高网络在处理密集连接时的效率",{"2":{"31":1}}],["旨在提高效率和表现力",{"2":{"28":1,"53":1}}],["旨在训练阶段通过多分支结构学习丰富的特征信息",{"2":{"18":1}}],["是一种引入了可变形注意力机制的视觉transformer",{"2":{"65":1}}],["是一种用于捕捉图像中的广泛上下文信息的机制",{"2":{"64":1}}],["是一种用于视觉transformer模型的注意力机制",{"2":{"28":1,"53":1}}],["是一种技术",{"2":{"39":1}}],["是一个关键的模块",{"2":{"31":1}}],["是rcs",{"2":{"12":1,"18":1,"31":1}}],["恢复成高分辨率的图像",{"2":{"27":1}}],["经过多个rhag处理的特征通过图像重建部分",{"2":{"27":1}}],["经过softmax函数进行归一化",{"2":{"23":1}}],["深层特征提取",{"2":{"27":1}}],["进行导入和注册我们的模块",{"2":{"121":1}}],["进行swda操作",{"2":{"13":1}}],["进一步增加了模型对复杂图像模式的适应性",{"2":{"64":1}}],["进一步证明了hat模型的有效性",{"2":{"27":1}}],["特别是当处理大规模数据时",{"2":{"43":1}}],["特别是在医学图像中常见的不规则和可变形的器官",{"2":{"74":1}}],["特别是在rcs",{"2":{"39":1}}],["特别是在复杂背景或不同光照条件下",{"2":{"36":1}}],["特别是在urban100数据集上",{"2":{"27":1}}],["特征金字塔",{"2":{"90":1,"122":1}}],["特征融合",{"2":{"68":1}}],["特征分解",{"2":{"68":1}}],["特征多样性",{"2":{"53":1}}],["特征图的通道首先被分割成不同的头部",{"2":{"13":1}}],["特征聚合",{"2":{"13":1}}],["特征级联的目的是为了减轻网络计算负担并降低内存占用",{"2":{"39":1}}],["特征级联",{"0":{"39":1},"2":{"12":1,"39":1}}],["比swinir和edt有显著提升",{"2":{"27":1}}],["比如说",{"2":{"5":1}}],["度量上",{"2":{"27":1}}],["峰值信噪比",{"2":{"27":1}}],["专门针对hat",{"2":{"27":1}}],["同任务预训练策略",{"2":{"27":1}}],["同时降低了模型的复杂度",{"2":{"78":1}}],["同时将自注意力机制中的查询",{"2":{"78":1}}],["同时通过减少参数数量来降低计算复杂度",{"2":{"64":1}}],["同时大家也可以放在其他的位置即插即用",{"2":{"57":1}}],["同时避免了传统自我关注机制的高计算成本",{"2":{"54":1}}],["同时避免了过度复杂化网络结构所带来的低效率和高资源消耗",{"2":{"39":1}}],["同时在网络结构中引入一个dat计算量由8",{"2":{"45":1}}],["同时这种方法在计算上是高效的",{"2":{"30":1}}],["同时没有降低性能",{"2":{"20":1}}],["同时减少内存消耗",{"2":{"18":1}}],["同时保持良好的性能",{"2":{"65":1}}],["同时保持计算效率",{"2":{"31":1}}],["同时保持训练阶段学到的特征表达能力",{"2":{"24":1}}],["同时保持通道间的信息交换",{"2":{"18":1}}],["同时保持了性能",{"2":{"13":1}}],["同时本文对rcs",{"2":{"3":1}}],["同时关注空间维度上的重要特征",{"2":{"3":1}}],["9gflops涨到了9",{"2":{"45":1}}],["96",{"2":{"42":3}}],["9涨到了9",{"2":{"28":1}}],["9",{"2":{"26":2,"42":1,"56":3,"80":3,"89":3,"91":1,"95":3,"104":3,"113":3,"119":3,"129":3,"131":3,"132":3,"134":3}}],["79",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["768",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["75",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["700多行",{"2":{"27":1}}],["7",{"2":{"26":3,"42":1,"47":1,"56":2,"80":2,"89":2,"91":2,"93":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["7x7",{"2":{"13":1}}],["矩形的宽度代表张量的通道数",{"2":{"24":1}}],["渐变色的矩形代表张量的特定特征",{"2":{"24":1}}],["黑色边框的矩形代表特定的模块操作",{"2":{"24":1}}],["原来的多分支结构被简化为一个单一的3x3",{"2":{"24":1}}],["原始lka设计",{"2":{"20":1}}],["原始注意力",{"2":{"4":1}}],["与卷积操作通过1x1卷积进行特征分解",{"2":{"68":1}}],["与另一部分输入进行通道shuffle和连接",{"2":{"24":1}}],["与1×1卷积结合",{"2":{"20":1}}],["另一部分通过堆叠的rcs模块进行处理",{"2":{"31":1}}],["另一部分保持不变",{"2":{"24":1}}],["另一个分支专注于高度",{"2":{"5":1}}],["用于深度特征学习和分辨率恢复",{"2":{"84":1}}],["用于指导变形卷积层如何调整其采样位置",{"2":{"64":1}}],["用于捕获空间维度之间的依赖性",{"2":{"30":1}}],["用于单图像超分辨率重建",{"2":{"27":1}}],["用于学习丰富的特征表示",{"2":{"24":1}}],["用于改进卷积神经网络",{"2":{"20":1}}],["结果图如下",{"2":{"76":1}}],["结果分析",{"0":{"76":1}}],["结果表明",{"2":{"34":1}}],["结果",{"2":{"34":1}}],["结构被重参数化成一个单一的3x3卷积",{"2":{"24":1}}],["结构在训练阶段使用多个分支",{"2":{"24":1}}],["结合了",{"2":{"78":1}}],["结合了卷积和自注意力聚合",{"2":{"68":1}}],["结合了自注意力机制和卷积运算的优势",{"2":{"58":1}}],["结合了通道混洗",{"2":{"12":1}}],["结合可变形卷积技术",{"2":{"54":1}}],["结合",{"2":{"23":1}}],["⊕代表广播元素级加法",{"2":{"23":1}}],["zpool",{"2":{"47":2}}],["z",{"2":{"30":1,"83":3}}],["zeros",{"2":{"29":1,"42":5,"49":2,"78":1,"83":3,"94":2}}],["zebra",{"2":{"17":1}}],["z池化",{"2":{"23":1}}],["再接一个7x7卷积",{"2":{"23":1}}],["再通过一个卷积层和relu激活函数",{"2":{"23":1}}],["再变换回原来的输入形状",{"2":{"16":1}}],["中间",{"2":{"104":1}}],["中间使用relu激活函数",{"2":{"23":1}}],["中部分支",{"2":{"30":1}}],["中的应用问题进行的改进",{"2":{"20":1}}],["生成一组中间特征",{"2":{"48":1,"58":1}}],["生成通道描述符",{"2":{"23":1}}],["生成注意力权重",{"2":{"16":1}}],["希望大家能够举一反三",{"2":{"22":1}}],["五",{"0":{"22":1,"81":1,"82":1,"92":1,"116":1,"117":1,"124":1,"137":1},"1":{"29":1,"37":1,"46":1,"90":1,"91":1,"99":1,"100":1,"101":1,"108":1,"114":1,"120":1,"122":1,"123":1,"127":1,"128":1,"129":1,"131":1,"133":1,"135":1,"138":1,"139":1}}],["但在实际实现中实际上有更多的点",{"2":{"75":1}}],["但在现代gpu上效率较低",{"2":{"4":1}}],["但现有的线性注意力方法仍存在性能下降的问题",{"2":{"53":1}}],["但是本文的方法需要按照有参的注意力机制添加但是只是不需要进行传入参数在yaml文件中",{"2":{"94":1}}],["但是如果你想要放在残差中配置c2f的时候就需要按照c2f的配置来添加",{"2":{"77":1}}],["但是如果都替换的话我觉得那就是rcs",{"2":{"49":1}}],["但是并不能产生决定性结果",{"2":{"73":1}}],["但是组合用很多具体那种最有效果都不一定",{"2":{"70":1,"124":1}}],["但是使用更少的参数和计算量",{"2":{"64":1}}],["但是在rcs",{"2":{"49":1}}],["但是可以作为一个参考",{"2":{"45":1}}],["但是其也可以用在我们的yolo系列当中从而提高检测精度",{"2":{"28":1,"53":1}}],["但是效果挺好的也是10月份最新的成果非常适合添加到大家自己的论文中",{"2":{"21":1}}],["通常小于标记数",{"2":{"43":1}}],["通道数",{"2":{"83":1}}],["通道池化后的7x7卷积",{"2":{"23":1}}],["通道注意力关注于识别哪些通道更重要",{"2":{"21":1,"34":1}}],["通过扩展",{"2":{"84":1}}],["通过上面的方式产生多种参考点分布在图像上",{"2":{"75":1}}],["通过学习图像特征本身来确定这些偏移量",{"2":{"74":1}}],["通过模块化设计",{"2":{"68":1}}],["通过使用深度可分离的卷积",{"2":{"64":1}}],["通过分解自注意力和卷积中的运算",{"2":{"58":1}}],["通过采用大卷积核来模拟类似自我关注的感受野",{"2":{"54":1}}],["通过适当的近似手段",{"2":{"43":1}}],["通过在网络的一次性聚合",{"2":{"39":1}}],["通过在网络的最后一部分只进行一次特征聚合",{"2":{"31":1}}],["通过在不同头部设置不同的扩张率",{"2":{"13":1}}],["通过本文你能够了解到",{"2":{"28":1}}],["通过广泛的实验",{"2":{"27":1}}],["通过广播加法结合原始特征图",{"2":{"23":1}}],["通过结构重参数化",{"2":{"18":1}}],["通过结构化重参数化简化为单一分支",{"2":{"12":1}}],["通过这样的设计",{"2":{"16":1}}],["通过这种方法",{"2":{"13":1}}],["通过这种双层路由注意力机制",{"2":{"1":1}}],["通过旋转输入张量和应用残差变换来建立不同维度间的依赖",{"2":{"16":1}}],["通过一个创新的三支结构捕获通道",{"2":{"16":1}}],["通过对每个分支中的输入张量进行排列变换",{"2":{"16":1}}],["通过整体架构和biformer块的设计",{"2":{"15":1}}],["通过实验yolov8在整合lskattention机制后",{"2":{"14":1}}],["通过不同头部的自注意力机制",{"2":{"13":1}}],["通过为不同的头部设置不同的扩张率",{"2":{"13":1}}],["通过重参数化卷积来增强网络的特征提取能力",{"2":{"12":1}}],["通过双层路由注意力机制",{"2":{"9":1}}],["通过收集键",{"2":{"4":1}}],["通过构建一个区域级别的亲和度图",{"2":{"4":1}}],["混合注意力变换器",{"0":{"34":1},"2":{"21":1,"27":1,"34":1}}],["显示出更高的计算效率和更小的内存占用",{"2":{"20":1}}],["显示出其广泛的适用性",{"2":{"20":1}}],["有一个方法的名字叫",{"2":{"46":1}}],["有多个残差混合注意力组",{"2":{"27":1}}],["有效地处理复杂的视觉信息",{"2":{"36":1}}],["有效地整合了全局的像素信息",{"2":{"21":1,"34":1}}],["有效降低了计算复杂度和内存需求",{"2":{"20":1}}],["有序列表3",{"2":{"17":1}}],["有序列表2",{"2":{"17":1}}],["有序列表1",{"2":{"17":1}}],["个人总结",{"2":{"20":1,"63":1}}],["每层输入和输出特征图的大小都有标注",{"2":{"75":1}}],["每层由两个1d卷积层组成",{"2":{"20":1}}],["每种模块都设计用于处理特征图",{"2":{"23":1}}],["每个核处理不同的特征子集",{"2":{"78":1}}],["每个组内包含多个混合注意力块",{"2":{"27":1}}],["每个分支在生成注意力权重后",{"2":{"30":1}}],["每个分支都有一个permute操作来调整维度",{"2":{"23":1}}],["每个分支进行不同的处理",{"2":{"23":1}}],["每个分支负责捕获空间维度h或w与通道维度c之间的交互特征",{"2":{"16":1}}],["每个头部的自注意力操作针对的是其对应的扩张率和感受野",{"2":{"13":1}}],["每个头部处理不同的特征子集",{"2":{"13":1}}],["每个区域只需要关注前k个路由的区域",{"2":{"4":1}}],["每个图像块都与一个位置路由器相关联",{"2":{"1":1}}],["将分解后的卷积和自注意力运算重构成一个统一的混合模块",{"2":{"78":1}}],["将标准的卷积核分解成多个1×1卷积核",{"2":{"78":1}}],["将自注意力和卷积技术融合",{"2":{"58":1}}],["将下面的代码复制粘贴到",{"2":{"42":1}}],["将下面的代码在",{"2":{"26":1}}],["将lka的前两层分解为四层",{"2":{"20":1}}],["将特征图的通道分成n个不同的头部",{"2":{"8":1,"13":1}}],["串联的水平和垂直1d大核深度卷积与1×1卷积结合",{"2":{"20":1}}],["串联卷积操作",{"2":{"20":1}}],["朴素的2d大核深度卷积",{"2":{"20":1}}],["具体哪个效果好可能要大家自己进行一定的尝试才可以",{"2":{"61":1}}],["具体添加到哪里由你自己决定",{"2":{"56":1}}],["具体包括",{"2":{"20":1}}],["具体来说",{"2":{"1":1,"13":1}}],["论文展示了所提出模块和预训练策略的有效性",{"2":{"27":1}}],["论文还引入了一种同任务预训练策略",{"2":{"27":1}}],["论文提出的lska",{"2":{"20":1}}],["论文地址",{"2":{"4":1,"7":1,"10":1,"13":1,"20":1,"35":1,"44":1,"55":1}}],["十亿浮点运算次数",{"2":{"20":1}}],["精度权衡",{"2":{"20":1}}],["语义分割等多种计算机视觉任务中有效应用",{"2":{"20":1}}],["语义分割等方面的积极影响",{"2":{"14":1}}],["时",{"2":{"20":1}}],["纹理和形状",{"2":{"20":1}}],["如x射线或mri中的单层切片",{"2":{"84":1}}],["如右侧所示",{"2":{"75":1}}],["如图像分类",{"2":{"65":1}}],["如果大家看yaml文件看的不清楚自己把结构添加到了哪里可以看下图",{"2":{"56":1}}],["如果你能够成功复现希望大家给博文评论支持以下",{"2":{"52":1}}],["如果你对rcs",{"2":{"49":1}}],["如果你还不会",{"2":{"41":1,"77":1}}],["如下图所示",{"2":{"50":1,"62":1,"88":1,"112":1}}],["如何添加dattention到你的结构中实现涨点",{"2":{"45":1}}],["如何添加到你的网络结构中去",{"2":{"41":1,"77":1}}],["如swinir和edt进行了比较",{"2":{"27":1}}],["如边缘",{"2":{"20":1}}],["如局部窗口",{"2":{"4":1}}],["虽然这个实验不能产生确定性的结论",{"2":{"45":1}}],["虽然采用了分解和串联的策略",{"2":{"20":1}}],["虽然稀疏矩阵乘法是适用的",{"2":{"4":1}}],["保持了高效的图像处理能力",{"2":{"20":1}}],["保持效果",{"2":{"20":1}}],["保留每个节点的前k个连接",{"2":{"4":1}}],["纵向",{"2":{"20":1}}],["横向扩展以下自己的知识库",{"2":{"23":1}}],["横向",{"2":{"20":1}}],["核分解",{"2":{"20":1}}],["核心思想是在粗粒度的区域级别上过滤掉最不相关的键",{"2":{"4":1}}],["80",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["8左右",{"2":{"36":1}}],["8",{"2":{"19":2,"42":1,"49":2,"56":7,"80":4,"89":4,"91":5,"94":3,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],[">=",{"2":{"94":2}}],[">",{"2":{"19":3,"29":21,"42":4,"78":2,"83":15,"93":1,"94":9}}],[">非常",{"2":{"17":1}}],["kk",{"2":{"94":4}}],["ksize",{"2":{"94":3}}],["ksize=9",{"2":{"94":1}}],["k2的1x1卷积",{"2":{"68":1}}],["k=1",{"2":{"93":1}}],["k=",{"2":{"57":2,"83":2,"93":2}}],["k=self",{"2":{"29":1}}],["ktv来改变计算顺序",{"2":{"43":1}}],["k^2",{"2":{"68":1}}],["k^",{"2":{"43":1}}],["kq",{"2":{"29":1}}],["kq+c",{"2":{"29":2}}],["kv=kv",{"2":{"29":1}}],["kv",{"2":{"29":99,"42":7,"83":9,"94":8}}],["kvgather",{"2":{"29":2}}],["keepdim=true",{"2":{"83":4}}],["keep",{"2":{"29":1,"42":3}}],["kenel",{"2":{"29":2}}],["keywords",{"2":{"42":1}}],["key",{"2":{"29":8,"42":5,"68":1,"78":2,"94":8}}],["kernels",{"2":{"57":1,"83":1,"93":1}}],["kernelid",{"2":{"49":2}}],["kernel1x1",{"2":{"49":5}}],["kernel3x3",{"2":{"49":2}}],["kernel=none",{"2":{"29":1}}],["kernel",{"2":{"19":33,"20":3,"26":24,"29":4,"42":1,"47":5,"49":17,"54":1,"64":2,"78":38,"83":6,"84":2,"93":10,"94":6}}],["k代表最大感受野",{"2":{"20":1}}],["kwargs",{"2":{"19":6,"42":1}}],["k",{"2":{"19":12,"26":9,"29":22,"42":7,"43":1,"57":2,"78":11,"83":26,"93":11,"94":8}}],["up",{"2":{"42":1,"49":2}}],["upscale=2",{"2":{"42":1}}],["upscale",{"2":{"42":5}}],["upsampler=",{"2":{"42":1}}],["upsampler",{"2":{"42":5}}],["upsample",{"2":{"42":8,"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["util",{"2":{"42":1}}],["utils",{"2":{"42":1}}],["usage",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["using",{"2":{"29":1,"57":1,"83":1,"93":1}}],["used",{"2":{"42":1}}],["use",{"2":{"29":2,"42":14,"49":2,"94":6}}],["unembedding",{"2":{"42":1}}],["unembed",{"2":{"42":4}}],["unsqueeze",{"2":{"42":6,"47":2,"78":10,"94":4}}],["unit",{"2":{"29":1,"93":2}}],["unfold",{"2":{"19":4,"42":3,"78":9}}],["urban100和manga109",{"2":{"27":1}}],["u",{"2":{"26":2,"93":2}}],["ultralytics",{"2":{"19":1,"26":1,"29":1,"37":2,"40":1,"42":1,"46":1,"50":1,"56":3,"62":2,"66":1,"79":1,"80":2,"88":1,"89":2,"91":2,"95":2,"104":2,"105":1,"112":1,"113":2,"119":2,"121":1,"129":2,"131":2,"132":2,"134":2}}],["05左右",{"2":{"80":1,"129":1}}],["0n",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["00",{"2":{"56":3,"80":3,"89":3,"91":3,"95":3,"104":3,"113":3,"119":3,"129":3,"131":3,"132":3,"134":3}}],["01",{"2":{"42":2,"47":1,"94":2}}],["02",{"2":{"19":1,"42":4}}],["0",{"2":{"19":31,"26":24,"29":13,"42":105,"47":5,"49":4,"56":8,"57":2,"78":22,"80":8,"83":14,"89":8,"91":8,"93":3,"94":39,"95":8,"104":8,"113":8,"119":8,"129":8,"131":8,"132":8,"134":8}}],["06左右",{"2":{"8":1}}],["q",{"2":{"19":7,"29":18,"42":15,"43":1,"78":11,"83":23,"94":31}}],["qk+c",{"2":{"29":3}}],["qkvo",{"2":{"29":3}}],["qkvlinear",{"2":{"29":3}}],["qkv",{"2":{"19":25,"29":10,"42":28,"83":1}}],["qk",{"2":{"19":18,"29":42,"42":23,"83":3}}],["quality",{"2":{"42":1}}],["query",{"2":{"29":7,"42":5,"68":1,"78":2}}],["queryselector",{"2":{"17":1}}],["quot",{"2":{"3":4,"8":2,"26":2,"46":2,"56":2}}],["x1",{"2":{"42":2,"49":5}}],["x0",{"2":{"42":2}}],["xxxpool",{"2":{"29":1}}],["x4",{"2":{"27":1}}],["x3",{"2":{"27":1,"42":2,"49":2}}],["x3c",{"2":{"17":6,"42":3}}],["x26",{"2":{"42":1}}],["x2",{"2":{"27":1,"42":2,"49":5}}],["x",{"2":{"19":108,"23":2,"26":3,"29":17,"42":191,"47":41,"49":33,"56":1,"57":8,"78":12,"80":1,"83":45,"89":1,"91":1,"93":42,"94":30,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["我没有进行尝试大家有兴趣的可以试试",{"2":{"138":1}}],["我仅在大目标检测层的输出添加了一个acmix模块",{"2":{"129":1}}],["我仅在大目标检测层的输出添加了一个msda模块",{"2":{"80":1}}],["我的对比实验是用这个版本跑出来的",{"2":{"113":1}}],["我知道的",{"2":{"100":1,"125":1}}],["我将其进行了解决",{"2":{"78":1}}],["我添加的位置只是随便添加的",{"2":{"76":1}}],["我不可每一种都做实验",{"2":{"70":1,"124":1}}],["我把三个biformer添加到了网络结构中的那个部位",{"2":{"56":1}}],["我这里建议添加到",{"2":{"56":1}}],["我这里起名字的dattention",{"2":{"42":1}}],["我这里起名为dilation然后粘贴进去",{"2":{"19":1}}],["我在官方的代码基础上做了一定的修改",{"2":{"52":1}}],["我后面也会提高rcs",{"2":{"49":1}}],["我只用了100张图片的数据集进行了100个epoch的训练",{"2":{"45":1}}],["我下面会出将其用在rt",{"2":{"43":1}}],["我进行了简单的实验",{"2":{"36":1}}],["我们经过等待之后",{"2":{"76":1}}],["我们直接就可以使用该代码了",{"2":{"72":1}}],["我们只保留字典里你需要的dat就行",{"2":{"72":1}}],["我们找到如下ultralytics",{"2":{"109":1}}],["我们找到如下的地方",{"2":{"60":1,"97":1,"118":1}}],["我们找到七百多行的代码",{"2":{"72":1}}],["我们找到parse",{"2":{"60":1,"97":1,"118":1}}],["我们找到该方法对其进行修改",{"2":{"46":1}}],["我们找到该文件",{"2":{"37":1}}],["我们可以开始进行训练了",{"2":{"66":1}}],["我们可以在某一层中添加biformer注意力机制",{"2":{"56":1}}],["我们可以将主要原理概括如下",{"2":{"12":1}}],["我们需要找到如下文件进行修改",{"2":{"56":1}}],["我们需要改动的地方有三处",{"2":{"22":1}}],["我们以后在想导入其它的注意力机制就可以重复步骤一和步骤二",{"2":{"46":1}}],["我们在控制台输入",{"2":{"66":1}}],["我们在步骤二的文件中",{"2":{"46":1}}],["我们在其中复制如下代码即可",{"2":{"29":1}}],["我们在其中创建一个名字为biformer的py文件如图所示",{"2":{"29":1}}],["我们的方法",{"2":{"23":1,"68":1}}],["我们通过下图来看一下biformer的网络结构",{"2":{"15":1}}],["我们介绍将这一机制整合到yolov8的方法",{"2":{"14":1}}],["我们介绍lskattention机制的基本原理",{"2":{"14":1}}],["我们将其复制导",{"2":{"19":1}}],["我们将讲解如何将lskattention大核注意力机制应用于yolov8",{"2":{"14":1}}],["我们将这种方法称为双层路由注意力",{"2":{"4":1}}],["我们提出了一种简单的解决方案",{"2":{"4":1}}],["我们先来看一下不同注意力机制的效果",{"2":{"4":1}}],["目录下创建一个py文件复制粘贴进去然后按照章节四进行添加即可",{"2":{"26":1}}],["目录下",{"2":{"19":1}}],["目标检测和语义分割在内的各种计算机视觉任务上的实验结果表明",{"2":{"4":1}}],["基于通道shuffle的重参数化卷积",{"2":{"18":1,"24":1,"31":1}}],["行内脚注文本",{"2":{"17":1}}],["行内的脚注",{"2":{"17":1}}],["↩︎",{"2":{"17":4}}],["也是我实验跑出来的版本",{"2":{"91":1}}],["也融入了自注意力的全局信息聚合功能",{"2":{"78":1}}],["也可以用在残差中也就是下面这个代码具体那种看你自己选择",{"2":{"57":1}}],["也可以由多个段落组成",{"2":{"17":1}}],["也就是我们的特征融合层",{"2":{"56":1}}],["也能够感知到更广泛区域的上下文信息",{"2":{"13":1}}],["重新生成位置编码",{"2":{"83":1}}],["重构为混合模块",{"2":{"78":1}}],["重构为1×1卷积形式",{"2":{"58":1}}],["重叠交叉注意模块",{"2":{"27":1}}],["重复的页脚定义",{"2":{"17":1}}],["重要内容",{"2":{"17":1}}],["重要",{"2":{"17":2}}],["定义",{"2":{"17":1}}],["脚注文字",{"2":{"17":1}}],["脚注",{"2":{"17":4}}],["选项卡",{"2":{"17":1}}],["非常强大",{"2":{"17":1}}],["p=none",{"2":{"93":2}}],["p=drop",{"2":{"42":1}}],["pe=false",{"2":{"94":1}}],["pe=true",{"2":{"94":2}}],["pe",{"2":{"78":7,"94":14}}],["perform",{"2":{"93":1}}],["perm2",{"2":{"47":2}}],["perm1",{"2":{"47":2}}],["permutation",{"2":{"30":1}}],["permute",{"2":{"19":13,"42":14,"47":4,"78":1,"83":6}}],["per",{"2":{"19":35,"29":12,"42":2,"49":2}}],["p",{"2":{"78":2,"93":4,"94":2}}],["pvt",{"2":{"63":2}}],["p4",{"2":{"56":5,"80":5,"89":5,"91":5,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":5}}],["p1",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["p5",{"2":{"56":5,"80":5,"89":5,"91":5,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":8}}],["p3",{"2":{"56":8,"80":5,"89":5,"91":5,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":5}}],["planes",{"2":{"47":6,"78":16}}],["pw",{"2":{"42":2}}],["ph",{"2":{"42":2}}],["pyramid",{"2":{"63":1}}],["pytorch",{"2":{"42":3}}],["py配置的代码如下",{"2":{"41":1,"101":1}}],["py",{"2":{"37":1,"42":3,"46":1,"50":1,"62":2,"88":1,"112":1,"115":1,"121":1}}],["pix",{"2":{"29":22}}],["pixels",{"2":{"42":1}}],["pixelshuffledirect",{"2":{"42":1}}],["pixelshuffle",{"2":{"42":5}}],["pixel",{"2":{"29":1}}],["p2",{"2":{"29":15,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["p^2",{"2":{"29":36}}],["prob=none",{"2":{"42":1}}],["prob",{"2":{"42":9}}],["projection",{"2":{"29":1,"42":2}}],["proj",{"2":{"19":21,"42":10,"83":6,"93":4,"94":20}}],["print",{"2":{"19":1,"29":1,"49":2,"78":1,"93":1}}],["pretrained=false",{"2":{"19":1}}],["pretrained=true",{"2":{"19":3}}],["points",{"2":{"94":2}}],["pointconv",{"2":{"19":1}}],["policy",{"2":{"29":2}}],["pool2d",{"2":{"49":1,"94":1}}],["pooling",{"2":{"29":2}}],["pool",{"2":{"23":1,"30":1}}],["positional",{"2":{"78":1,"83":3}}],["position",{"2":{"42":36,"78":2}}],["pos",{"2":{"19":7,"42":6,"94":5}}],["pass",{"2":{"57":2,"83":2,"93":2}}],["paper",{"2":{"45":1}}],["padded",{"2":{"29":2}}],["padding",{"2":{"29":2,"49":5,"78":2,"93":2}}],["padding=9",{"2":{"93":1}}],["padding=2",{"2":{"93":1}}],["padding=kernel",{"2":{"83":1}}],["padding=padding",{"2":{"47":1,"49":4,"93":2}}],["padding=side",{"2":{"29":1}}],["padding=",{"2":{"26":24,"42":1,"47":1}}],["padding=0",{"2":{"19":4,"42":2,"47":1,"78":1,"94":4}}],["padding=1",{"2":{"19":11,"49":1,"78":1,"93":1,"94":1}}],["pad",{"2":{"29":15,"49":3,"78":4,"93":2,"94":2}}],["pad=true",{"2":{"29":1}}],["parse",{"2":{"46":1}}],["partition",{"2":{"42":7}}],["partial",{"2":{"17":1,"19":2}}],["partial^r",{"2":{"17":1}}],["parameters",{"2":{"56":6,"78":2,"80":6,"89":6,"91":6,"94":1,"95":6,"104":6,"113":7,"119":6,"129":6,"131":6,"132":6,"134":6}}],["parameter",{"2":{"42":5,"78":3,"83":3,"94":6}}],["param=true",{"2":{"29":1}}],["params",{"2":{"29":1,"42":8,"63":1}}],["param",{"2":{"29":20}}],["patchunembed",{"2":{"42":3}}],["patchify",{"2":{"29":1}}],["patchmerging",{"2":{"19":3,"42":1}}],["patches=64",{"2":{"83":1}}],["patches",{"2":{"19":4,"42":23,"83":1}}],["patchembed",{"2":{"19":2,"42":3}}],["patch",{"2":{"19":21,"42":52,"84":1}}],["paths",{"2":{"42":2}}],["path=dpr",{"2":{"19":2,"42":1}}],["path=drop",{"2":{"19":2,"42":2}}],["path=0",{"2":{"19":5,"42":3}}],["path",{"2":{"19":17,"42":19}}],["p>",{"2":{"17":2}}],["g",{"2":{"57":1,"83":1,"93":1,"94":12}}],["g=self",{"2":{"94":2}}],["g=g",{"2":{"57":1,"83":1}}],["g=1",{"2":{"57":2,"83":2,"93":3}}],["given",{"2":{"57":1,"83":1,"93":2}}],["github",{"2":{"4":1,"42":2}}],["gflop",{"2":{"91":1,"129":1,"131":1}}],["gflops",{"2":{"56":5,"80":5,"89":5,"91":4,"95":5,"104":5,"113":5,"119":5,"129":4,"131":4,"132":5,"134":5}}],["gfm",{"2":{"17":1}}],["generating",{"2":{"78":1}}],["get",{"2":{"49":1,"94":5}}],["gelu",{"2":{"19":11,"42":5,"93":1,"94":1}}],["grid=displacement",{"2":{"94":1}}],["grid=pos",{"2":{"94":1}}],["grid",{"2":{"94":9}}],["grad",{"2":{"94":3}}],["grad=true",{"2":{"78":1}}],["gradients",{"2":{"56":5,"80":5,"89":5,"91":5,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":5}}],["gradient",{"2":{"29":1}}],["groups",{"2":{"42":1,"49":6,"57":1,"83":1,"93":3,"94":15}}],["groups=g",{"2":{"93":1}}],["groups=groups",{"2":{"47":1,"49":4,"93":1}}],["groups=head",{"2":{"83":1}}],["groups=self",{"2":{"78":1,"94":2}}],["groups=1",{"2":{"47":1,"49":2,"94":1}}],["groups=out",{"2":{"19":1}}],["groups=dim",{"2":{"19":2,"26":24,"29":1,"93":2}}],["group",{"2":{"42":3,"49":2,"57":1,"83":1,"93":1,"94":12}}],["gating",{"2":{"93":2}}],["gather",{"2":{"29":6}}],["gamma",{"2":{"49":4}}],["gap",{"2":{"23":1}}],["gt",{"2":{"29":1,"50":1,"52":1,"54":1,"73":1,"88":1,"101":1,"112":1}}],["gc",{"2":{"23":1}}],["gmp",{"2":{"23":1}}],["global",{"2":{"23":2,"29":2}}],["globalstage",{"2":{"19":2}}],["globalblock",{"2":{"19":2}}],["globalattention",{"2":{"19":2}}],["错误内容",{"2":{"17":1}}],["错误",{"2":{"17":1}}],["警告内容",{"2":{"17":1}}],["警告",{"2":{"17":2}}],["信息内容",{"2":{"17":1}}],["信息",{"2":{"17":1}}],["注意",{"2":{"17":2}}],["注意力图显示了具有高注意力得分的块在查询块周围稀疏分布",{"2":{"13":1}}],["注意力的一个关键特性是全局感受野",{"2":{"4":1}}],["注",{"2":{"17":1}}],["注释内容",{"2":{"17":1}}],["注释",{"2":{"17":1}}],["示例",{"2":{"17":1}}],["v1",{"2":{"78":2}}],["v2",{"2":{"63":2,"94":1}}],["v8",{"2":{"56":1}}],["vovnet",{"2":{"49":1}}],["vit",{"2":{"63":1}}],["view",{"2":{"29":3,"42":24,"49":3,"78":10}}],["vision",{"2":{"19":1,"45":1,"63":2,"65":1}}],["visual",{"2":{"8":1,"20":1}}],["var",{"2":{"49":5}}],["van",{"2":{"20":1}}],["valueerror",{"2":{"29":2,"42":1}}],["values",{"2":{"29":1}}],["value",{"2":{"17":1,"29":1,"42":5,"49":3,"68":1,"78":2}}],["v",{"2":{"19":6,"26":7,"29":18,"42":6,"43":2,"78":5,"83":14,"94":8}}],["very",{"2":{"17":2,"42":1}}],["vscode",{"2":{"17":1}}],["图示d",{"0":{"128":1,"139":1}}],["图示包含了",{"2":{"78":1}}],["图片展示的很直观",{"2":{"85":1}}],["图片中的符号⊗代表矩阵乘法",{"2":{"23":1}}],["图像中每个像素的重要性",{"2":{"34":1}}],["图像中标记框内区域时",{"2":{"34":1}}],["图中仅显示了4个参考点",{"2":{"75":1}}],["图中",{"2":{"68":1}}],["图中分为四个部分",{"2":{"63":1}}],["图中显示的多层rcs",{"2":{"39":1}}],["图中详细展示了三个分支如何处理输入张量",{"2":{"30":1}}],["图中的idetect是从yolov7中借鉴过来的",{"2":{"39":1}}],["图中的每个子图表示三重注意力中的一个分支",{"2":{"16":1}}],["图中的例子展示了三种不同的扩张率",{"2":{"13":1}}],["图表显示",{"2":{"27":1}}],["图d",{"2":{"20":1}}],["图c",{"2":{"20":1}}],["图b",{"2":{"20":1}}],["图a",{"2":{"20":1}}],["图标",{"2":{"17":1}}],["外部链接",{"2":{"17":1}}],["链接",{"2":{"17":3}}],["引入可变形注意力机制和动态采样点",{"2":{"45":1}}],["引入可以将其用在c2f和检测头中进行改进估计效果会更高",{"2":{"36":1}}],["引入了一种新颖的双层路由机制来改进传统的注意力机制",{"2":{"4":1}}],["引用内容",{"2":{"17":2}}],["$1",{"2":{"17":1}}],["$12",{"2":{"17":1}}],["$1600",{"2":{"17":1}}],["type",{"2":{"42":1,"78":3}}],["typing",{"2":{"29":1}}],["tanh",{"2":{"94":1}}],["tasks",{"2":{"37":1,"46":1,"50":1,"56":1,"62":1,"80":1,"88":1,"89":1,"91":1,"95":1,"104":1,"112":1,"113":1,"119":1,"121":1,"129":1,"131":1,"132":1,"134":1}}],["table",{"2":{"42":9,"94":15}}],["tables",{"2":{"17":1}}],["tab1tab2const",{"2":{"17":1}}],["two",{"2":{"29":1,"57":1,"83":1,"93":1}}],["twitter",{"2":{"17":1}}],["tba",{"2":{"29":1}}],["testing",{"2":{"42":1}}],["temperature",{"2":{"29":1}}],["tensors",{"2":{"29":1,"42":1}}],["tensor",{"2":{"29":15,"42":5,"49":10,"78":8,"93":1,"94":1}}],["text",{"2":{"17":4}}],["tuple",{"2":{"29":2,"42":12}}],["t",{"2":{"19":2,"29":2,"42":1,"43":1,"49":2}}],["through",{"2":{"57":1,"83":1,"93":1}}],["than",{"2":{"42":1}}],["that",{"2":{"29":1}}],["there",{"2":{"29":2}}],["the",{"2":{"19":3,"29":4,"42":13,"57":1,"83":1,"93":1}}],["this",{"2":{"17":4}}],["tiny作为对比的模型",{"2":{"20":1}}],["tiny",{"2":{"19":2}}],["time",{"2":{"42":1}}],["times",{"2":{"19":1,"29":1}}],["timm",{"2":{"19":3,"42":2,"94":1}}],["tip",{"2":{"17":1}}],["tokens",{"2":{"63":1}}],["token",{"2":{"42":2,"63":1}}],["todo",{"2":{"29":3}}],["topk=self",{"2":{"29":1}}],["topk=4",{"2":{"29":2}}],["topk",{"2":{"29":41}}],["topkrouting",{"2":{"29":2}}],["torchvision",{"2":{"93":2}}],["torchscript",{"2":{"42":1}}],["torch",{"2":{"19":6,"26":2,"29":7,"42":31,"47":4,"49":11,"57":2,"78":16,"83":15,"93":5,"94":25}}],["to",{"2":{"17":4,"19":4,"29":11,"42":28,"49":4,"57":1,"63":1,"78":2,"83":3,"93":3,"94":1}}],["tolong",{"2":{"17":1}}],["training",{"2":{"42":3}}],["transposed",{"2":{"93":1}}],["transpose",{"2":{"19":5,"29":2,"42":6,"49":1}}],["transformer中d=32",{"2":{"43":1}}],["transformer`",{"2":{"42":1}}],["transformer",{"2":{"8":1,"19":4,"27":1,"42":2,"45":1,"63":7,"65":1}}],["tripleat",{"2":{"57":3}}],["tripletat",{"2":{"57":3,"95":3}}],["tripletattention",{"2":{"47":2,"57":1,"104":3}}],["triplet",{"0":{"10":1,"16":1,"23":1,"30":1,"38":1,"47":1,"77":1,"86":1,"95":1,"104":1,"117":1},"1":{"16":1,"23":1,"30":1,"47":1,"57":1,"95":1,"104":1,"111":1,"123":1,"128":1},"2":{"16":2,"23":1,"30":1,"77":1,"95":1,"123":2}}],["trivial和van中的lska在核大小增加时显著降低了gflops",{"2":{"20":1}}],["trivial",{"2":{"20":3}}],["true",{"2":{"19":5,"42":13,"56":4,"80":4,"83":1,"89":4,"91":4,"93":1,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["trunc",{"2":{"19":2,"42":5,"94":3}}],["任务列表4",{"2":{"17":1}}],["任务列表3",{"2":{"17":1}}],["任务列表2",{"2":{"17":1}}],["任务列表1",{"2":{"17":1}}],["ωyω​",{"2":{"17":2}}],["∂ωr∂r​",{"2":{"17":1}}],["∂r∂ωr",{"2":{"17":1}}],["^",{"2":{"17":2}}],["^r",{"2":{"17":2}}],["+8",{"2":{"94":1}}],["+1",{"2":{"94":1}}],["+=",{"2":{"42":5}}],["+",{"2":{"17":13,"19":9,"23":1,"29":4,"42":22,"47":3,"49":7,"57":2,"78":3,"83":9,"93":5,"94":7}}],["lr",{"2":{"34":1}}],["lce",{"2":{"29":1}}],["l是hat的一个更大的变体",{"2":{"27":1}}],["lska模块在保持与标准大核注意力",{"2":{"20":1}}],["lska",{"2":{"20":2,"26":1,"41":1}}],["lskattention可以是一种即插即用的注意力机制",{"2":{"90":1}}],["lskattention可添加的位置",{"0":{"81":1},"1":{"90":1,"99":1}}],["lskattention通过创新的核分解和串联卷积策略",{"2":{"20":1}}],["lskattention通过以下几个关键步骤和原理来解决这些问题",{"2":{"20":1}}],["lskattention不仅在图像分类任务中表现出色",{"2":{"20":1}}],["lskattention能够有效地捕捉到重要信息",{"2":{"20":1}}],["lskattention仍然能够保持类似于原始lka的性能",{"2":{"20":1}}],["lskattention在执行时的计算效率得到显著提升",{"2":{"20":1}}],["lskattention首先使用一个1d核对输入进行水平方向上的卷积",{"2":{"20":1}}],["lskattention的训练过程截图",{"0":{"71":1}}],["lskattention的yaml文件",{"0":{"61":1}}],["lskattention的yaml文件和训练截图",{"0":{"51":1},"1":{"61":1,"71":1}}],["lskattention的添加教程",{"0":{"41":1}}],["lskattention的代码",{"0":{"26":1}}],["lskattention的核心创新是将传统的2d卷积核分解为两个1d卷积核",{"2":{"20":1}}],["lskattention的机制原理",{"0":{"20":1}}],["lka是一种即插即用的模块",{"2":{"138":1}}],["lka可以使模型在做出最终预测之前",{"2":{"138":1}}],["lka可以帮助模型更有效地融合不同层次的特征",{"2":{"138":1}}],["lka可以被用于提升对小目标和不规则形状目标的检测能力",{"2":{"36":1}}],["lka可添加的位置",{"0":{"128":1,"137":1,"138":1,"139":1},"1":{"138":1,"139":1}}],["lka代码和c2f",{"0":{"93":1}}],["lka注意力机制进行特征学习",{"2":{"84":1}}],["lka块",{"2":{"84":2}}],["lka模型",{"2":{"84":4}}],["lka模块在处理大尺寸卷积核时面临着高计算和内存需求的挑战",{"2":{"20":1}}],["lka则扩展了这种技术",{"2":{"84":1}}],["lka专为处理二维图像数据设计",{"2":{"84":1}}],["lka中",{"2":{"64":1}}],["lka通过可变形卷积来灵活调整采样网格",{"2":{"54":1}}],["lka的位置",{"2":{"132":1}}],["lka的yaml文件二",{"0":{"134":1}}],["lka的yaml文件一",{"0":{"132":1}}],["lka的yaml文件和训练截图",{"0":{"130":1},"1":{"132":1,"134":1,"136":1}}],["lka的训练截图",{"2":{"111":1,"136":1}}],["lka的训练过程截图",{"0":{"111":1,"136":1}}],["lka的2d和3d版本",{"2":{"54":1}}],["lka的基本原理",{"0":{"54":1}}],["lka的朴素设计",{"2":{"20":1}}],["lka机制原理",{"0":{"44":1},"1":{"54":1,"64":1,"74":1,"84":1}}],["lka结合了大卷积核的广阔感受野和可变形卷积的灵活性",{"2":{"36":1}}],["lka",{"0":{"93":1,"102":1},"1":{"109":1,"115":1,"121":1,"126":1,"130":1,"132":1,"134":1,"136":1},"2":{"20":4,"36":2,"54":2,"64":1,"84":2,"93":6,"134":3,"138":1}}],["last",{"2":{"42":3}}],["lam展示了在重建高分辨率",{"2":{"34":1}}],["lam",{"2":{"34":1}}],["lambda",{"2":{"29":1}}],["larger",{"2":{"42":1}}],["large",{"2":{"20":3,"42":1,"54":1,"56":1,"64":2,"80":1,"84":2,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":4}}],["layer=partial",{"2":{"19":1}}],["layer=none",{"2":{"42":4}}],["layer=norm",{"2":{"19":4,"42":7}}],["layer=nn",{"2":{"19":10,"42":9}}],["layer=act",{"2":{"19":4,"42":1}}],["layernormproxy",{"2":{"94":2}}],["layernorm",{"2":{"19":7,"42":12,"83":1,"94":1}}],["layer",{"2":{"19":39,"29":1,"42":50,"57":2,"83":2,"93":3}}],["layers",{"2":{"19":4,"42":8,"56":5,"80":5,"89":5,"91":5,"94":1,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":5}}],["l",{"2":{"19":1,"29":2,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["license",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["like",{"2":{"29":1}}],["list",{"2":{"19":2,"42":1,"57":2,"83":3,"93":3}}],["linspace",{"2":{"19":1,"42":1,"78":4,"94":2}}],["linear",{"0":{"35":1,"53":1,"92":1,"101":1,"108":1,"114":1,"120":1},"1":{"43":1,"53":1,"63":1,"101":1,"108":1,"114":2,"120":2},"2":{"19":8,"28":3,"29":7,"42":11,"53":5,"83":4,"94":2,"120":1}}],["link",{"2":{"17":6}}],["loc",{"2":{"78":8}}],["local",{"2":{"29":3,"42":2}}],["loss",{"2":{"41":1,"77":1}}],["look",{"2":{"19":1}}],["longtensor",{"2":{"29":1}}],["long",{"2":{"17":4}}],["log2",{"2":{"94":2}}],["logit",{"2":{"29":4}}],["logy",{"2":{"17":2}}],["log",{"2":{"17":5,"42":1,"94":5}}],["log⁡y",{"2":{"17":2}}],["leakyrelu",{"2":{"42":1}}],["leak",{"2":{"29":1}}],["learnable",{"2":{"29":1,"42":5}}],["lepe",{"2":{"29":7,"94":2}}],["len",{"2":{"19":2,"42":5}}],["left",{"2":{"17":3}}],["level",{"2":{"1":1,"4":2,"29":1}}],["==",{"2":{"19":9,"26":6,"29":15,"42":16,"49":3,"57":1,"78":1,"83":3,"93":2,"94":2}}],["=>",{"2":{"17":1}}],["=",{"2":{"17":35,"19":147,"26":32,"29":85,"41":1,"42":285,"47":26,"49":75,"57":11,"72":1,"78":55,"83":78,"93":43,"94":94,"101":1,"118":1}}],["yaml文件二",{"2":{"123":1,"138":1}}],["yaml文件一",{"2":{"123":1,"138":1}}],["yaml文件一和二",{"2":{"98":1,"133":1}}],["yaml文件截图如下",{"2":{"110":1}}],["yaml",{"2":{"56":3,"66":1,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["yet",{"2":{"29":2}}],["y4",{"2":{"19":2}}],["y3",{"2":{"19":2}}],["y2",{"2":{"19":2}}],["y1",{"2":{"19":3}}],["y",{"2":{"17":2,"19":4,"42":2,"49":2,"57":8,"83":9,"93":9,"94":9}}],["y^",{"2":{"17":2}}],["yωω",{"2":{"17":2}}],["yolo",{"2":{"56":1,"57":1,"66":1,"80":1,"83":1,"89":1,"91":1,"93":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8的neck部分负责特征融合",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["yolov8x",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8l",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8m",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8s",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8n",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["yolov8",{"2":{"56":5,"80":4,"89":4,"91":4,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["yolo感兴趣的话",{"2":{"49":1}}],["yolo了没啥区别了",{"2":{"49":1}}],["yolo我们只是用其中的rcs",{"2":{"49":1}}],["yolo主要由rcs",{"2":{"39":1}}],["yolo的yaml文件供大家参考",{"2":{"49":1}}],["yolo的整体架构",{"2":{"39":1}}],["yolo的核心组成部分",{"2":{"18":1}}],["yolo架构中的一个关键组成部分旨在通过一次性聚合来提高模型处理特征的能力",{"2":{"31":1}}],["yolo中还有一个repvgg模块",{"2":{"49":1}}],["yolo中",{"2":{"31":1,"39":1}}],["yolo中提出的一种结构",{"2":{"12":1}}],["yolo提出的rcs",{"2":{"3":1}}],["~",{"2":{"17":1}}],["ns",{"2":{"94":2}}],["np",{"2":{"49":2,"94":2}}],["n=1",{"2":{"49":1,"57":1,"83":1,"93":1}}],["n=196",{"2":{"43":1}}],["n=49",{"2":{"43":1}}],["nd^2",{"2":{"43":1}}],["nd2",{"2":{"43":2}}],["ndim",{"2":{"42":1}}],["n^2",{"2":{"43":1}}],["n×d",{"2":{"43":1}}],["nq",{"2":{"42":4}}],["nc=2",{"2":{"42":1}}],["nc",{"2":{"42":2,"56":2,"80":2,"89":2,"91":2,"94":13,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["nw",{"2":{"42":16}}],["nh",{"2":{"42":9}}],["nhwc",{"2":{"29":2}}],["n代表hadamard乘积",{"2":{"20":1}}],["name",{"2":{"19":1,"49":1,"78":1,"93":1}}],["numpy",{"2":{"49":2,"94":1}}],["num",{"2":{"19":56,"29":9,"42":74,"49":4,"83":12}}],["number",{"2":{"17":3,"29":4,"42":19,"56":1,"57":1,"80":1,"83":1,"89":1,"91":1,"93":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["no",{"2":{"19":1,"29":2,"42":3,"47":5,"94":11}}],["nonlinearity",{"2":{"49":5}}],["non",{"2":{"29":1,"42":3}}],["none",{"2":{"19":1,"29":12,"42":30,"47":4,"49":6,"56":2,"78":2,"80":2,"89":2,"91":2,"93":1,"94":5,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["nonoverlaping",{"2":{"19":2}}],["nothing",{"2":{"29":1}}],["notimplementederror",{"2":{"29":3}}],["not",{"2":{"19":2,"29":13,"42":10,"47":4,"49":1,"78":2,"94":3}}],["note",{"2":{"17":1,"29":4}}],["norm=true",{"2":{"42":1}}],["norm2",{"2":{"19":4,"42":4}}],["norm1",{"2":{"19":4,"42":4}}],["norm",{"2":{"19":23,"29":1,"42":45,"83":10,"94":2}}],["normalization",{"2":{"42":8,"93":1}}],["normal",{"2":{"19":2,"29":1,"42":5,"94":3}}],["nn",{"2":{"19":79,"26":29,"29":23,"37":2,"40":1,"42":82,"46":1,"47":7,"49":22,"50":1,"56":2,"57":3,"62":2,"78":15,"79":1,"80":2,"83":20,"88":1,"89":2,"91":2,"93":19,"94":27,"95":2,"104":2,"105":1,"109":1,"112":1,"113":2,"119":2,"121":1,"129":2,"131":2,"132":2,"134":2}}],["n",{"2":{"17":2,"19":4,"29":91,"39":1,"42":13,"43":1,"49":4,"56":2,"57":2,"80":2,"83":8,"89":2,"91":2,"93":2,"94":66,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["net",{"2":{"93":2}}],["network在网络的代码中需要控制可添加可不添加",{"2":{"75":1}}],["networks",{"2":{"20":1}}],["network",{"2":{"15":1,"63":1}}],["neck部分",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["neck层",{"2":{"56":1}}],["neurons=out",{"2":{"49":1}}],["neurons",{"2":{"49":3}}],["neural",{"2":{"15":1}}],["nearest",{"2":{"49":1,"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["nearest+conv",{"2":{"42":1}}],["neat",{"2":{"17":1}}],["need",{"2":{"29":4}}],["−1",{"2":{"17":2}}],["−",{"2":{"17":2}}],["标准的2d卷积",{"2":{"64":1}}],["标记为",{"2":{"63":1}}],["标记",{"2":{"17":1}}],["标题1标题2内容区块",{"2":{"17":2}}],["标题",{"0":{"0":1,"2":1,"6":1,"11":1,"17":1},"1":{"2":1,"6":2,"11":3,"17":4}}],["内容区块",{"2":{"17":2}}],["内容右对齐",{"2":{"17":1}}],["内容居中",{"2":{"17":1}}],["内容",{"2":{"17":1}}],["删除文字",{"2":{"17":1}}],["斜体文字",{"2":{"17":1}}],["斜体",{"2":{"17":1}}],["68229632",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["68229648",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["67",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["650行左右",{"2":{"46":1}}],["64",{"2":{"42":2,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["6",{"0":{"17":1},"2":{"19":8,"42":8,"56":3,"80":3,"83":3,"89":3,"91":3,"95":3,"104":3,"113":3,"119":3,"129":3,"131":3,"132":3,"134":3}}],["6左右",{"2":{"3":1}}],["和上面的思想也不一样",{"2":{"134":1}}],["和d",{"2":{"84":1}}],["和",{"2":{"78":1}}],["和值",{"2":{"78":2}}],["和深度可分离的带扩张的卷积",{"2":{"64":1}}],["和top",{"2":{"63":1}}],["和repvgg",{"2":{"39":1}}],["和窗口式多头自注意力",{"2":{"27":1}}],["和一个重叠交叉注意块",{"2":{"27":1}}],["和不同数据集",{"2":{"27":1}}],["和推理阶段",{"2":{"24":1}}],["和1×1卷积",{"2":{"20":1}}],["和垂直",{"2":{"20":1}}],["和通道",{"2":{"16":2}}],["和前馈神经网络子层",{"2":{"15":1}}],["wk",{"2":{"94":7}}],["wg",{"2":{"94":2}}],["w=num",{"2":{"83":1}}],["w=w",{"2":{"29":1,"83":1}}],["wse",{"2":{"42":12}}],["ws",{"2":{"42":12}}],["work",{"2":{"42":1}}],["word",{"2":{"17":1}}],["wo",{"2":{"29":3}}],["ww",{"2":{"29":1,"42":17}}],["whose",{"2":{"42":1}}],["whether",{"2":{"42":3}}],["when",{"2":{"29":1,"42":2}}],["where",{"2":{"29":1}}],["wh",{"2":{"29":1,"42":17}}],["wrong",{"2":{"42":2}}],["write",{"2":{"29":1}}],["wrapper",{"2":{"17":1}}],["w2",{"2":{"29":9}}],["w^2",{"2":{"29":11}}],["will",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["width",{"2":{"42":2,"49":3,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["wise",{"2":{"29":2,"64":2}}],["win=4",{"2":{"29":1}}],["win=7",{"2":{"29":1}}],["win",{"2":{"29":39,"42":10}}],["windowattention",{"2":{"42":2}}],["windows",{"2":{"29":2,"42":48}}],["window",{"2":{"29":5,"42":132}}],["without",{"2":{"29":1}}],["with",{"2":{"29":3,"42":4,"45":1,"49":1,"56":2,"57":4,"65":1,"80":2,"83":4,"89":2,"91":2,"93":6,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["were",{"2":{"29":1}}],["we",{"2":{"29":1,"42":1}}],["wether",{"2":{"29":5}}],["weight=r",{"2":{"29":1}}],["weight=mul",{"2":{"29":1}}],["weight=",{"2":{"29":1}}],["weight",{"2":{"19":3,"29":24,"42":5,"49":4,"78":1,"94":4}}],["weights",{"2":{"19":2,"29":2,"42":2}}],["w是宽度",{"2":{"23":1}}],["way=merging",{"2":{"19":2}}],["way=patch",{"2":{"19":1}}],["way=",{"2":{"19":2}}],["way=none",{"2":{"19":3}}],["way",{"2":{"19":15}}],["warning",{"2":{"17":1,"29":1}}],["w",{"2":{"16":1,"19":23,"23":1,"27":1,"29":40,"42":54,"78":28,"83":11,"94":40}}],["没有进行旋转",{"2":{"16":1}}],["分解卷积和自注意力",{"2":{"78":1}}],["分别重用和聚合这些中间特征",{"2":{"48":1,"58":1}}],["分别代表查询",{"2":{"43":1}}],["分别关注图像的不同维度",{"2":{"5":1}}],["分为训练阶段",{"2":{"24":1}}],["分为三个分支",{"2":{"23":1}}],["分支",{"2":{"16":3}}],["展示了特征图的转换过程",{"2":{"78":1}}],["展示了偏移生成网络的详细结构",{"2":{"75":1}}],["展示了可变形注意力的信息流",{"2":{"75":1}}],["展示了自注意力机制",{"2":{"68":1}}],["展示了标准卷积操作",{"2":{"68":1}}],["展示了cswin",{"2":{"63":1}}],["展示了pvt",{"2":{"63":1}}],["展示了三个分支如何捕获跨维度交互",{"2":{"16":1}}],["展示这种改进对目标检测",{"2":{"14":1}}],["捕获跨维度依赖性的重要性",{"2":{"16":1}}],["维度间依赖性的重要性",{"2":{"16":1}}],["旋转操作和残差变换",{"2":{"16":1}}],["高度",{"2":{"16":1,"83":1}}],["高效的计算性能",{"2":{"9":1}}],["跨维度的注意力权重计算",{"2":{"16":1}}],["的训练截图",{"2":{"100":1}}],["的对比图如下",{"2":{"85":1}}],["的特点",{"2":{"78":1}}],["的生成也转换为1×1卷积操作",{"2":{"78":1}}],["的计算",{"2":{"64":1}}],["的模型",{"2":{"63":1}}],["的性能对比",{"2":{"63":1}}],["的top",{"2":{"63":1}}],["的基本原理是结合了大卷积核和可变形卷积的注意力机制",{"2":{"54":1}}],["的基本原理是利用三支结构捕获输入数据的跨维度交互",{"2":{"16":1}}],["的教程",{"2":{"45":1}}],["的目录下",{"2":{"42":1,"62":1}}],["的开头模块导入部分在其中添加如下一行代码",{"2":{"37":1}}],["的结构",{"2":{"31":1}}],["的具体实现流程图",{"2":{"30":1}}],["的整体架构及其关键组成部分的结构",{"2":{"27":1}}],["的引入",{"2":{"27":1}}],["的设计理念是通过融合通道注意力和自注意力机制来提升单图像超分辨率重建的性能",{"2":{"21":1,"34":1}}],["的两个1d核",{"2":{"20":1}}],["的维度旋转输入张量",{"2":{"16":2}}],["的主要改进点包括",{"2":{"16":1}}],["的工作原理",{"2":{"13":1}}],["前馈神经网络子层通过多层感知机对注意力输出进行非线性变换和特征提取",{"2":{"15":1}}],["由此可以证明其是一种十分有效的改进机制",{"2":{"45":1}}],["由多个子层组成",{"2":{"15":1}}],["由于在现代视觉transformer设计中通道维度",{"2":{"43":1}}],["由于分解后的1d卷积核大大减少了参数的数量",{"2":{"20":1}}],["由于biformer采用了稀疏注意力机制",{"2":{"9":1}}],["由于注意力在所有空间位置上计算令牌之间的关联性",{"2":{"4":1}}],["左侧是3d",{"2":{"84":1}}],["左侧部分",{"2":{"75":1}}],["左侧",{"2":{"15":1,"84":1}}],["四",{"0":{"15":1,"25":1,"33":1,"52":1,"59":1,"67":1,"83":1,"87":1,"102":1,"103":1},"1":{"32":1,"40":1,"41":1,"50":1,"51":1,"60":1,"61":1,"62":1,"69":1,"70":1,"71":1,"72":1,"77":1,"79":1,"80":1,"86":1,"88":1,"89":1,"95":1,"96":1,"97":1,"98":1,"104":1,"105":1,"106":1,"107":1,"109":1,"110":1,"111":1,"112":1,"113":1,"115":1,"118":1,"119":1,"121":1,"125":1,"126":1,"130":1,"132":1,"134":1,"136":1}}],["接着用sigmoid函数生成注意力权重",{"2":{"30":1}}],["接着进行另一个1x1卷积",{"2":{"23":1}}],["接着通过两个全连接层",{"2":{"23":1}}],["接着",{"2":{"14":1}}],["模型的网络架构",{"2":{"84":1}}],["模型的训练过程时间比较长",{"2":{"76":1}}],["模型能够在不同的尺度上捕捉图像特征",{"2":{"13":1}}],["模仿我添加即可",{"2":{"60":1,"118":1}}],["模仿自我关注机制的感受野",{"2":{"54":1}}],["模块化设计",{"2":{"68":1}}],["模块的架构",{"2":{"64":1}}],["模块的主要改进机制包括以下几点",{"2":{"13":1}}],["模块中",{"2":{"24":1,"39":1}}],["模块相当的性能的同时",{"2":{"20":1}}],["模块",{"2":{"20":1,"53":1}}],["模块在视觉注意网络",{"2":{"20":1}}],["模块是为了利用自注意机制在不同尺度上的稀疏性",{"2":{"13":1}}],["红色框内的区域",{"2":{"13":1}}],["块的注意力图的可视化",{"2":{"13":1}}],["得到更丰富的特征表示",{"2":{"13":1}}],["头部通道分离",{"2":{"13":1}}],["降低了计算的冗余",{"2":{"13":1}}],["稀疏性利用",{"2":{"13":1}}],["稀疏注意力",{"2":{"4":1}}],["ij",{"2":{"94":2}}],["it",{"2":{"42":1}}],["item",{"2":{"19":1,"42":1}}],["i=self",{"2":{"29":5}}],["i=1",{"2":{"17":1}}],["ignore",{"2":{"19":1,"42":2}}],["im",{"2":{"49":2}}],["img",{"2":{"19":13,"42":34}}],["images",{"2":{"42":1}}],["image",{"2":{"19":2,"42":19,"63":1,"78":5}}],["impact",{"2":{"42":1}}],["implemented",{"2":{"29":2}}],["implementation",{"2":{"19":5,"42":1,"57":1,"83":1,"93":1}}],["import",{"2":{"19":6,"26":2,"29":6,"37":1,"42":6,"49":5,"78":2,"83":3,"93":3,"94":6,"101":1}}],["important",{"2":{"17":1}}],["id",{"2":{"49":6}}],["idx=r",{"2":{"29":1}}],["idx",{"2":{"29":8}}],["identity",{"2":{"19":5,"24":1,"29":7,"30":1,"42":5,"49":7,"93":1}}],["id=",{"2":{"17":1}}],["if",{"2":{"19":19,"26":1,"29":14,"42":37,"47":6,"49":13,"57":1,"78":5,"83":7,"93":8,"94":11}}],["including",{"2":{"93":1}}],["instead",{"2":{"57":1,"83":1,"93":1}}],["inference",{"2":{"42":1}}],["inf",{"2":{"42":1}}],["info",{"2":{"17":1}}],["inplace=true",{"2":{"42":2,"94":3}}],["input=einops",{"2":{"94":1}}],["input=x",{"2":{"94":1}}],["inputs",{"2":{"49":8}}],["input",{"2":{"19":1,"29":1,"42":45,"49":8,"57":2,"83":2,"93":3}}],["independent",{"2":{"29":1}}],["indexing=",{"2":{"94":2}}],["index=r",{"2":{"29":1}}],["index",{"2":{"29":4,"42":16}}],["inorporate",{"2":{"29":1}}],["interpolate",{"2":{"83":1}}],["internal",{"2":{"49":2}}],["intermediate",{"2":{"42":2}}],["into",{"2":{"42":2}}],["int",{"2":{"19":17,"29":3,"42":51,"49":6,"57":2,"83":3,"93":4,"94":1}}],["in",{"2":{"19":28,"29":10,"41":1,"42":41,"47":2,"49":17,"57":4,"72":1,"78":7,"83":6,"93":10,"94":1,"118":1}}],["initialize",{"2":{"57":1,"83":1,"93":2}}],["initializes",{"2":{"57":1,"83":1,"93":1}}],["init",{"2":{"19":27,"26":2,"29":8,"42":33,"47":6,"49":8,"57":4,"78":7,"83":6,"93":12,"94":4,"115":1}}],["inline",{"2":{"17":6}}],["issue",{"2":{"29":1}}],["isinstance",{"2":{"19":5,"42":4,"49":3,"93":3}}],["isn",{"2":{"19":1}}],["is",{"2":{"17":6,"19":2,"29":13,"42":9,"47":2,"49":4,"78":5,"93":2,"94":1,"101":1}}],["i+1",{"2":{"17":1}}],["ir",{"2":{"17":1}}],["ir⋯",{"2":{"17":2}}],["i",{"2":{"13":1,"19":26,"29":11,"42":8,"49":3,"56":1,"78":4,"80":1,"83":13,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["hk",{"2":{"94":7}}],["hg",{"2":{"94":2}}],["h=num",{"2":{"83":1}}],["h=self",{"2":{"83":2,"94":1}}],["h=h",{"2":{"29":1,"83":1}}],["hw",{"2":{"47":2,"94":2}}],["hc",{"2":{"47":2}}],["hr",{"2":{"34":1}}],["height",{"2":{"42":2,"49":3}}],["here",{"2":{"29":1}}],["head=4",{"2":{"78":1}}],["heads=",{"2":{"19":4,"42":1}}],["heads=num",{"2":{"19":6,"42":5}}],["heads=8",{"2":{"19":2,"29":1,"83":1,"94":1}}],["heads",{"2":{"19":23,"29":8,"42":38,"83":8,"94":16}}],["head",{"2":{"13":1,"19":17,"42":10,"56":4,"78":34,"80":4,"83":1,"89":4,"91":4,"94":10,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["half",{"2":{"78":3}}],["hasattr",{"2":{"49":2}}],["has",{"2":{"42":2}}],["happy",{"2":{"42":1}}],["hard",{"2":{"29":6}}],["hab利用通道注意力块",{"2":{"27":1}}],["hab",{"2":{"27":1,"42":2}}],["hattention的训练过程截图",{"0":{"100":1}}],["hattention的yaml文件一",{"0":{"91":1}}],["hattention的yaml文件",{"0":{"82":1},"1":{"91":1,"100":1}}],["hattention的核心代码",{"0":{"42":1}}],["hattention框架原理",{"0":{"27":1},"1":{"34":1}}],["hat显示了最强的像素利用和最高的psnr",{"2":{"34":1}}],["hat包括浅层特征提取",{"2":{"27":1}}],["hat的改进幅度介于0",{"2":{"27":1}}],["hat在psnr",{"2":{"27":1}}],["hat模型与其他最先进模型",{"2":{"27":1}}],["hat结合了通道注意力和自注意力",{"2":{"27":1}}],["hat利用这两种注意力机制",{"2":{"21":1,"34":1}}],["hat",{"0":{"34":1},"2":{"21":1,"27":5,"29":4,"34":2,"42":2,"72":1,"91":1}}],["hybrid",{"2":{"27":1,"42":4}}],["h是高度",{"2":{"23":1}}],["high",{"2":{"42":1}}],["hidden",{"2":{"19":30,"42":15,"57":2,"83":2,"93":2}}],["hi​进行连接后送入一个线性层进行特征聚合",{"2":{"13":1}}],["h1>",{"2":{"17":1}}],["h1>hello",{"2":{"17":1}}],["home",{"2":{"17":1}}],["h2o",{"2":{"17":1}}],["h",{"2":{"13":1,"16":1,"19":27,"23":1,"26":7,"29":34,"42":50,"78":28,"83":15,"94":45}}],["https",{"2":{"4":1,"42":2,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["多尺度特征提取",{"2":{"13":1}}],["多尺度扩张注意力",{"2":{"13":2}}],["多尺度空洞注意力",{"2":{"8":1}}],["​​​​",{"2":{"136":1,"139":1}}],["​​​​​",{"2":{"111":1,"128":1}}],["​​",{"2":{"100":1,"107":1,"112":1,"118":1,"125":1,"135":1}}],["​",{"2":{"13":2,"44":2,"48":2,"50":1,"60":1,"68":1,"78":1,"100":1,"107":1}}],["确保特征的复用并加强不同层之间的信息流动",{"2":{"12":1}}],["确定了关注的区域后",{"2":{"4":1}}],["512",{"2":{"56":7,"80":7,"89":7,"91":7,"95":7,"104":7,"113":7,"119":7,"129":7,"131":7,"132":7,"134":7}}],["50",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["53",{"2":{"26":1,"61":1}}],["56x56",{"2":{"19":3}}],["5x5",{"2":{"13":1}}],["5",{"0":{"11":1,"39":1,"90":1,"91":1,"99":1,"100":1,"101":1,"108":1,"114":1,"120":1,"122":1,"123":1,"127":1,"128":1,"129":1,"131":1,"133":1,"135":1,"138":1,"139":1},"1":{"17":1,"114":1,"120":1},"2":{"13":1,"19":5,"26":18,"29":2,"42":10,"47":1,"49":1,"56":3,"57":2,"78":2,"80":3,"83":3,"89":3,"91":3,"93":4,"94":6,"95":3,"104":3,"113":3,"119":3,"129":3,"131":3,"132":3,"134":3}}],["需要注意的是这里卷积核越大计算量就会变得越大",{"2":{"61":1}}],["需要注意的是本文的task",{"2":{"41":1,"101":1}}],["需要注意的是一个flagflops从8",{"2":{"28":1}}],["需要按照有参数的注意力机制添加",{"2":{"26":1}}],["需要在具体任务和数据集上进行适当的实验和调整",{"2":{"9":1}}],["需要进行适当的调整和优化",{"2":{"9":1}}],["使用方法看章节四",{"2":{"93":1}}],["使用1x1卷积实现",{"2":{"78":1}}],["使用大卷积核来捕捉图像的广泛上下文信息",{"2":{"54":1}}],["使用了不同的标记来代表不同的核大小",{"2":{"20":1}}],["使注意力权重更加明显",{"2":{"53":1}}],["使其能够处理三维数据集",{"2":{"84":1}}],["使其在多种视觉任务中都能有效工作",{"2":{"65":1}}],["使其在处理复杂的视觉任务时更加高效和准确",{"2":{"65":1}}],["使其在处理不同深度的数据时表现出色",{"2":{"54":1}}],["使其在处理视觉任务时更加高效和有效",{"2":{"28":1,"53":1}}],["使其成为一个强大的视觉模型",{"2":{"9":1}}],["使得模型能够更好地适应不同的数据模式",{"2":{"54":1}}],["使得biformer在相同计算预算下能够实现更高的计算性能",{"2":{"9":1}}],["使得视觉transformer能够捕捉长距离依赖关系",{"2":{"4":1}}],["总体而言",{"2":{"9":1}}],["总结",{"2":{"4":1,"16":1,"20":1,"39":1,"43":1,"53":1,"65":1}}],["以优化特征通道上的计算复杂度",{"2":{"68":1}}],["以往的线性注意力缺乏足够的聚焦能力",{"2":{"53":1}}],["以确保特征的复用并加强不同层之间的信息流动",{"2":{"39":1}}],["以充分利用其潜力",{"2":{"27":1}}],["以进一步提升超分辨率重建的性能",{"2":{"27":1}}],["以进一步发掘hat的潜力",{"2":{"27":1}}],["以改善单图像超分辨率重建",{"2":{"27":1}}],["以激活更多像素以进行高分辨率重建",{"2":{"27":1}}],["以减少计算复杂性和内存消耗",{"2":{"24":1}}],["以及自注意力聚合",{"2":{"68":1}}],["以及cotnet",{"2":{"63":1}}],["以及t2t",{"2":{"63":1}}],["以及两种类型的检测层",{"2":{"39":1}}],["以及对应的性能指标",{"2":{"34":1}}],["以及图像重建三个主要步骤",{"2":{"27":1}}],["以及一个直接的连接",{"2":{"24":1}}],["以及在van中的实际设计",{"2":{"20":1}}],["以及它如何帮助提高处理大型数据集和复杂视觉任务的效率和准确性",{"2":{"14":1}}],["以实现显著的性能提升",{"2":{"14":1}}],["以发挥其最佳性能",{"2":{"9":1}}],["以找到最佳的参数配置",{"2":{"9":1}}],["以适应查询并实现内容感知的稀疏模式",{"2":{"4":1}}],["劣势",{"2":{"9":1}}],["提出的lska设计",{"2":{"20":1}}],["提出了一种创新的大型可分离核注意力",{"2":{"20":1}}],["提出了一种区域到区域的路由方法",{"2":{"4":1}}],["提出了一个通用的视觉transformer模型",{"2":{"4":1}}],["提示内容",{"2":{"17":1}}],["提示",{"2":{"17":2,"49":1,"118":1}}],["提高运算效率",{"2":{"78":1}}],["提高模型的性能",{"2":{"48":1,"58":1}}],["提高模型的语义信息提取能力",{"2":{"39":1}}],["提高模型对特征的理解能力",{"2":{"16":1}}],["提高重建图像的质量和精度",{"2":{"34":1}}],["提高效率",{"2":{"31":1}}],["提高特征表示的多样性",{"2":{"31":1}}],["提高了transformer架构的图像识别准确率",{"2":{"63":1}}],["提高了运算效率",{"2":{"58":1,"78":1}}],["提高了大概0",{"2":{"36":1}}],["提高了模型对目标形状和尺寸的适应性",{"2":{"36":1}}],["提高了模型的聚焦能力和特征表达的多样性",{"2":{"28":1,"53":1}}],["提高了模型的表达能力和性能",{"2":{"9":1}}],["提高了自注意力机制的效率和效果",{"2":{"13":1}}],["提高计算效率",{"2":{"12":1,"43":1}}],["查询感知的自适应性",{"2":{"9":1}}],["查询感知的稀疏性",{"2":{"4":1}}],["避免了在最不相关的区域进行冗余计算",{"2":{"9":1}}],["只保留了三个特征级联",{"2":{"39":1}}],["只关注最相关的键",{"2":{"9":1}}],["只关注与查询相关的前k个窗口",{"2":{"9":1}}],["只有与查询相关的键",{"2":{"9":1}}],["只进行适用于gpu的密集矩阵乘法运算",{"2":{"9":1}}],["会导致验证的适合报类型不匹配的错误",{"2":{"78":1}}],["会对输入进行排列",{"2":{"30":1}}],["会对所有的键",{"2":{"9":1}}],["会产生高计算复杂度和大内存占用",{"2":{"4":1}}],["利用maxvit块作为编码器组件",{"2":{"84":1}}],["利用多分支结构学习丰富的特征表示",{"2":{"12":1}}],["利用稀疏性跳过最不相关区域的计算过程",{"2":{"9":1}}],["利用双层路由注意力作为基本构建模块",{"2":{"4":1}}],["上面的实验结果是用这个方法跑出来的",{"2":{"132":1}}],["上面的实验结果图三是用这个方法跑出来的",{"2":{"95":1}}],["上面的图片显示了多个视觉transformer模型的性能和计算复杂度的比较",{"2":{"63":1}}],["上面的图片是关于比较softmax注意力和线性注意力的差异",{"2":{"43":1}}],["上部分支",{"2":{"30":1}}],["上的性能对比",{"2":{"27":1}}],["上图为变形大核注意力",{"2":{"64":1}}],["上图为大家展示了rcs的结构",{"2":{"24":1}}],["上图展示了3d和2d",{"2":{"84":1}}],["上图展示了大核注意力模块不同设计的比较",{"2":{"20":1}}],["上图展示了在不同大核分解方法和核大小下的速度",{"2":{"20":1}}],["上图展示了biformer的整体架构和一个biformer块的详细信息",{"2":{"15":1}}],["上图展示了通过收集前k个相关窗口中的键",{"2":{"9":1}}],["上层路由器通过全局自注意力机制对所有图像块进行交互",{"2":{"1":1}}],["上层路由器负责捕捉全局上下文信息",{"2":{"1":1}}],["可变形卷积网络",{"2":{"85":1}}],["可变形卷积能够提供一种自适应的内核形状",{"2":{"74":1}}],["可变形卷积通过添加额外的偏移量来调整标准卷积的采样位置",{"2":{"74":1}}],["可变形卷积",{"0":{"74":1},"2":{"54":1,"74":1}}],["可变形大核注意力",{"2":{"36":1}}],["可变形注意力通过改变规则网格来实现图像自适应的稀疏性",{"2":{"4":1}}],["可变形注意力",{"2":{"4":1,"65":1}}],["可以替换中干网络中的卷积部分",{"2":{"98":1,"133":1}}],["可以修改yaml文件中输入acmix使用这个模块了",{"2":{"118":1}}],["可以修改yaml文件中输入rcsosa使用这个模块了",{"2":{"97":1}}],["可以修改yaml文件中输入msda使用这个模块了",{"2":{"60":1}}],["可以帮助模型更好地融合不同尺度的特征",{"2":{"90":1,"122":1}}],["可以看到结果文件中保存了我们的运行结果",{"2":{"76":1}}],["可以看到红框内有好多代码",{"2":{"72":1}}],["可以看到我们的结构以及打印在控制台了",{"2":{"66":1}}],["可以看到开始训练",{"2":{"66":1}}],["可以有效地构造大卷积核",{"2":{"64":1}}],["可以用搜索也可以自己手动找",{"2":{"60":1,"97":1,"118":1}}],["可以将其分为以下几点",{"2":{"54":1}}],["可以跳过本章节",{"2":{"41":1,"77":1}}],["可以包含特殊标记",{"2":{"17":1}}],["可以以内容感知的方式关注最相关的键",{"2":{"9":1}}],["可能会导致一些次要的或较远的关联信息被忽略",{"2":{"9":1}}],["可能存在信息损失",{"2":{"9":1}}],["1推荐dat可添加的位置",{"0":{"122":1}}],["19",{"2":{"89":1,"91":2,"104":1,"129":1,"134":1}}],["19th",{"2":{"17":1}}],["1e",{"2":{"83":3}}],["1准确率上相对于原始模型的提升",{"2":{"63":1}}],["1准确率",{"2":{"63":1}}],["1准确率和计算量",{"2":{"63":1}}],["1conv",{"2":{"42":5}}],["165",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["16",{"2":{"42":1,"49":1,"56":4,"80":2,"89":4,"91":4,"93":2,"95":2,"104":4,"113":2,"119":2,"129":4,"131":2,"132":2,"134":4}}],["17",{"2":{"26":2}}],["18",{"2":{"26":2,"80":2,"95":2,"113":2,"119":2,"131":2,"132":2}}],["13",{"2":{"26":2,"49":2,"91":2}}],["15",{"2":{"26":2,"56":3,"80":2,"89":1,"95":2,"104":1,"113":2,"119":2,"129":1,"131":2,"132":2,"134":1}}],["11166544",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["11166560",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["11",{"2":{"26":3,"49":2,"61":1}}],["112x112",{"2":{"19":4}}],["1x1卷积",{"2":{"68":1}}],["1x1",{"2":{"23":1,"49":5}}],["1024",{"2":{"56":6,"80":6,"89":6,"91":6,"95":6,"104":6,"113":6,"119":6,"129":6,"131":6,"132":6,"134":6}}],["100",{"2":{"42":1}}],["10",{"2":{"19":1,"91":2}}],["128",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["12",{"2":{"19":4,"56":2,"80":2,"89":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["12n−1−1",{"2":{"17":1}}],["1",{"0":{"12":1,"16":1,"32":1,"34":1,"40":2,"41":1,"43":1,"47":1,"50":1,"54":1,"58":1,"60":1,"61":1,"65":1,"68":2,"69":1,"77":1,"78":1,"79":2,"80":1,"88":1,"90":1,"91":1,"95":1,"96":1,"97":1,"101":1,"105":2,"109":2,"110":1,"112":1,"113":1,"114":1,"115":1,"118":1,"121":1,"123":1,"126":1,"129":1,"132":1,"138":1},"1":{"40":1,"50":1,"60":1,"68":1,"78":1,"79":1,"88":1,"97":1,"105":1,"112":1,"118":1},"2":{"9":2,"12":1,"13":1,"16":1,"17":17,"19":45,"23":1,"26":85,"27":1,"28":1,"29":24,"31":1,"42":127,"43":1,"47":12,"49":18,"53":1,"54":1,"56":52,"57":12,"58":1,"63":1,"64":1,"78":33,"80":48,"83":39,"84":1,"89":52,"91":48,"93":23,"94":43,"95":46,"104":52,"113":46,"119":46,"129":52,"131":48,"132":46,"134":49}}],["优势",{"2":{"9":1}}],["三个分支的结果通过平均池化聚合起来生成最终的注意力权重",{"2":{"23":1}}],["三重注意力模型能够有效地捕获输入张量中的空间和通道维度之间的交互关系",{"2":{"16":1}}],["三重注意力由三个分支组成",{"2":{"16":1}}],["三重注意力",{"2":{"16":2}}],["三重注意力机制的主要思想是在网络中引入了一种新的注意力模块",{"2":{"5":1}}],["三",{"0":{"9":1,"19":1,"26":1,"38":1,"42":1,"49":1,"73":1,"93":1,"94":1},"1":{"47":1,"57":1},"2":{"78":1}}],["并重新构建为更高效的形式",{"2":{"78":1}}],["并结合两种不同的聚合操作",{"2":{"68":1}}],["并可能因映射函数带来额外的计算开销",{"2":{"53":1}}],["并通过相似度匹配计算注意力权重",{"2":{"78":1}}],["并通过先计算ktv",{"2":{"43":1}}],["并通过线性层进行特征聚合",{"2":{"13":1}}],["并最终合成三重注意力",{"2":{"30":1}}],["并在最后的特征映射中仅聚合一次所有特征",{"2":{"31":1}}],["并在推理阶段实现快速推理",{"2":{"18":1}}],["并在推理阶段通过简化为单分支结构来减少内存消耗",{"2":{"18":1}}],["并在不同的分辨率级别上使用2d",{"2":{"84":1}}],["并在不同的头部中以不同的扩张率执行多尺度swda",{"2":{"13":1}}],["并在不同的头部中以不同的扩张率执行多尺度swda来提高模型的处理效率和检测精度",{"2":{"8":1}}],["并在不需要复杂操作和额外计算成本的情况下有效地减少自注意机制的冗余",{"2":{"13":1}}],["并提供灵活性和强大的表达能力",{"2":{"15":1}}],["并送入一个线性层进行特征聚合",{"2":{"13":1}}],["并且可以作为一个模块加入到现有的网络架构中",{"2":{"30":1}}],["并且以van",{"2":{"20":1}}],["并且根据具体任务和需求可以进行不同的配置",{"2":{"15":1}}],["并且对所有的输出hi",{"2":{"13":1}}],["并且仅进行适用于gpu的密集矩阵乘法运算",{"2":{"9":1}}],["并生成局部图像表示",{"2":{"1":1}}],["并生成全局图像表示",{"2":{"1":1}}],["s=1",{"2":{"93":1}}],["s",{"2":{"56":1,"80":1,"89":1,"91":1,"93":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["sqrt",{"2":{"49":1}}],["squeeze",{"2":{"23":1,"42":15,"78":1}}],["slice",{"2":{"42":6}}],["slices",{"2":{"42":4}}],["sr2",{"2":{"49":2}}],["sr1",{"2":{"49":2}}],["sr",{"2":{"42":4,"49":3,"83":10}}],["swin",{"2":{"42":1,"63":1,"94":1}}],["swinir",{"2":{"42":1}}],["sw",{"2":{"42":3}}],["ssim性能指标",{"2":{"34":1}}],["same",{"2":{"93":1}}],["sampled",{"2":{"94":9}}],["sample",{"2":{"29":1,"42":2,"78":1,"94":11}}],["sampling",{"2":{"29":1}}],["save",{"2":{"42":3}}],["sa",{"2":{"42":11}}],["satge=cpe",{"2":{"19":2}}],["satge=false",{"2":{"19":3}}],["satge",{"2":{"19":9}}],["some",{"2":{"42":1}}],["so",{"2":{"29":2}}],["softplus",{"2":{"83":1}}],["soft",{"2":{"29":11}}],["softmax注意力",{"2":{"43":1}}],["softmax和线性注意力机制的对比",{"0":{"43":1}}],["softmax",{"2":{"19":2,"29":4,"42":7,"78":3,"94":1}}],["sign",{"2":{"94":1}}],["sigmoid",{"2":{"42":1,"47":1,"49":1}}],["silu",{"2":{"49":1,"93":1}}],["similar",{"2":{"29":1}}],["side",{"2":{"29":5}}],["size",{"2":{"19":36,"26":9,"29":11,"42":220,"47":5,"49":7,"78":2,"83":2,"93":5,"94":15}}],["size=x",{"2":{"83":1}}],["size=5",{"2":{"83":1}}],["size=sr",{"2":{"83":1}}],["size=self",{"2":{"78":2,"94":1}}],["size=side",{"2":{"29":1}}],["size=inputs",{"2":{"49":1}}],["size=img",{"2":{"19":1,"42":5}}],["size=64",{"2":{"42":1}}],["size=window",{"2":{"42":4}}],["size=to",{"2":{"42":1}}],["size=0",{"2":{"42":2}}],["size=7",{"2":{"42":2}}],["size=",{"2":{"26":24,"42":1,"83":3,"93":3,"94":2}}],["size=2",{"2":{"19":2}}],["size=224",{"2":{"19":2,"42":3}}],["size=1",{"2":{"19":2,"42":1,"49":3,"78":5,"94":4}}],["size=patch",{"2":{"19":2,"42":5}}],["size=4",{"2":{"19":2,"42":3}}],["size=kernel",{"2":{"19":4,"47":1,"49":3,"83":1,"93":2}}],["size=3",{"2":{"19":12,"49":1,"94":1}}],["sppf",{"2":{"56":1,"80":1,"89":1,"90":1,"91":1,"95":1,"104":1,"113":1,"119":1,"122":1,"129":1,"131":1,"132":1,"134":1}}],["speed",{"2":{"42":1}}],["split",{"2":{"29":3,"42":1,"57":3,"83":3,"93":3}}],["span>强大",{"2":{"17":1}}],["span",{"2":{"17":2}}],["spatial=false",{"2":{"47":1}}],["spatial",{"2":{"3":1,"26":14,"47":4,"93":4}}],["scaling",{"2":{"29":1,"56":1,"78":2,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["scales",{"2":{"42":2,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["scale=conv",{"2":{"42":3}}],["scale=0",{"2":{"42":2}}],["scale=self",{"2":{"29":1}}],["scale=qk",{"2":{"19":6,"42":5}}],["scale=none",{"2":{"19":8,"29":2,"42":6,"83":1}}],["scale",{"2":{"8":1,"19":15,"29":8,"42":39,"47":2,"56":1,"80":1,"83":5,"89":1,"91":1,"94":2,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["small",{"2":{"19":1,"56":4,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["small的第三个多头自注意力",{"2":{"13":1}}],["std",{"2":{"49":3}}],["std=0",{"2":{"94":2}}],["std=",{"2":{"19":1,"42":4}}],["standard",{"2":{"57":1,"83":1,"93":2}}],["start",{"2":{"42":2}}],["stackrep=true",{"2":{"49":1}}],["stack",{"2":{"42":3,"94":2}}],["stages",{"2":{"19":3}}],["stage",{"2":{"19":7}}],["stochastic",{"2":{"42":8}}],["stride",{"2":{"49":2,"78":14,"93":1,"94":8}}],["stride=sr",{"2":{"83":1}}],["stride=self",{"2":{"78":1,"94":1}}],["stride=stride",{"2":{"47":1,"49":4,"78":1,"93":2}}],["stride=window",{"2":{"42":1}}],["stride=",{"2":{"26":24}}],["stride=2",{"2":{"19":8}}],["stride=1",{"2":{"19":4,"29":1,"47":2,"49":3,"78":1,"93":2,"94":6}}],["stride=patch",{"2":{"19":1}}],["stripes",{"2":{"17":1}}],["see",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["se=false",{"2":{"49":2}}],["seblock",{"2":{"49":4}}],["series",{"2":{"42":1}}],["seq",{"2":{"42":4}}],["sequential",{"2":{"19":6,"42":4,"49":4,"94":2}}],["separte",{"2":{"29":1}}],["separable",{"2":{"20":2}}],["segmentation",{"2":{"29":2}}],["semantic",{"2":{"29":2}}],["setting",{"2":{"29":2}}],["set",{"2":{"29":1,"42":5}}],["sel",{"2":{"29":12}}],["select",{"2":{"29":1}}],["self",{"2":{"13":1,"15":1,"19":177,"26":33,"29":118,"42":305,"47":31,"49":76,"57":32,"64":1,"78":96,"83":61,"93":67,"94":137}}],["se",{"2":{"23":1,"49":10}}],["sub",{"2":{"94":4}}],["supported",{"2":{"42":3}}],["supports",{"2":{"42":1}}],["super",{"2":{"19":11,"26":1,"29":4,"42":15,"47":3,"49":4,"57":2,"78":1,"83":3,"93":6,"94":2}}],["surpported",{"2":{"29":2}}],["summary",{"2":{"56":5,"80":5,"89":5,"91":5,"95":5,"104":5,"113":5,"119":5,"129":5,"131":5,"132":5,"134":5}}],["sum",{"2":{"17":1,"19":5,"42":5,"78":2,"83":1}}],["shifts=",{"2":{"42":2}}],["shift",{"2":{"42":24}}],["shifted",{"2":{"42":8}}],["shorcut",{"2":{"93":2}}],["shortcut=false",{"2":{"57":1,"83":1,"93":1}}],["shortcut=true",{"2":{"57":1,"83":1,"93":1}}],["shortcut",{"2":{"42":4,"57":4,"83":4,"93":4}}],["should",{"2":{"29":1,"83":1}}],["shot",{"2":{"12":2,"31":2,"39":2}}],["shallow",{"2":{"42":1}}],["shared",{"2":{"29":1}}],["shapes",{"2":{"42":1}}],["shape",{"2":{"19":5,"42":17,"49":1,"78":5,"83":7,"93":2}}],["shuntedtransformer",{"2":{"29":1}}],["shuffle",{"2":{"12":1,"31":1,"49":4}}],["mvitv2",{"2":{"63":1}}],["mobilenet",{"2":{"78":2}}],["momentum=0",{"2":{"47":1}}],["more",{"2":{"42":1}}],["mode",{"2":{"29":12,"49":1}}],["mode=padding",{"2":{"49":1}}],["mode=",{"2":{"29":2,"49":1,"83":1,"94":2}}],["model里添加即可",{"2":{"126":1}}],["model这个方法",{"2":{"60":1,"97":1,"118":1}}],["model=yolov8n",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["model",{"2":{"19":14,"46":1,"56":2,"78":1,"80":2,"89":2,"91":2,"93":8,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["models",{"2":{"19":3,"42":4,"56":1,"94":1}}],["modulelist",{"2":{"19":4,"42":2,"57":1,"83":1,"93":1}}],["module",{"2":{"19":11,"23":3,"26":1,"29":4,"42":26,"47":4,"49":6,"56":1,"57":3,"78":1,"80":1,"83":4,"89":1,"91":1,"93":8,"94":2,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["modules文件夹下建立一个目录名字呢就是",{"2":{"109":1}}],["modules",{"2":{"19":1,"26":1,"29":2,"37":1,"40":1,"42":1,"62":1,"79":1,"101":1,"105":1}}],["min",{"2":{"42":2}}],["m=self",{"2":{"29":3}}],["medium",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["meshgrid",{"2":{"42":3,"94":2}}],["merge",{"2":{"42":3}}],["merging",{"2":{"19":16,"42":2}}],["mean",{"2":{"29":2,"42":8,"47":1,"49":5}}],["mem",{"2":{"29":1}}],["memory",{"2":{"29":2,"42":3}}],["msa",{"2":{"27":1,"42":5}}],["msda是一种即插即用的可替换卷积的模块",{"2":{"98":1}}],["msda添加步骤",{"0":{"32":1},"1":{"40":1,"50":1,"60":1}}],["msda核心代码",{"0":{"19":1}}],["msda不仅可以捕捉局部细节",{"2":{"13":1}}],["msda的训练过程截图",{"0":{"107":1}}],["msda的yaml版本二",{"0":{"89":1}}],["msda的yaml版本一",{"0":{"80":1}}],["msda的yaml文件和训练截图",{"0":{"70":1},"1":{"80":1,"89":1}}],["msda的输出通过连接操作合并",{"2":{"13":1}}],["msda的主要思想是通过线性投影得到特征图x的相应查询",{"2":{"8":1}}],["msda将特征图的通道分离成多个头部",{"2":{"13":1}}],["msda利用了自注意力机制在不同尺度的稀疏性",{"2":{"13":1}}],["msda能够在各个头部关注不同尺度的特征",{"2":{"13":1}}],["msda能够在被关注的接受域内有效地聚合不同尺度的语义信息",{"2":{"13":1}}],["msda能够捕捉到多尺度的语义信息",{"2":{"13":1}}],["msda被公式化如下",{"2":{"13":1}}],["msda通过线性投影得到特征图x的相应查询",{"2":{"13":1}}],["msda",{"2":{"13":3}}],["msda框架原理",{"0":{"13":1}}],["m",{"2":{"19":11,"29":20,"41":1,"42":15,"49":2,"56":1,"57":7,"72":1,"80":1,"83":7,"89":1,"91":1,"93":7,"94":8,"95":1,"101":1,"104":1,"113":1,"118":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["much",{"2":{"29":1}}],["mul",{"2":{"29":10,"94":9}}],["multihead",{"2":{"29":1}}],["multiple",{"2":{"42":1}}],["multiplied",{"2":{"29":1}}],["multiply",{"2":{"29":2}}],["multidilatelocalattention",{"2":{"19":2,"80":1,"89":3}}],["multi",{"2":{"8":1,"13":1,"42":1}}],["must",{"2":{"19":1,"29":1,"42":1}}],["mlp",{"2":{"19":26,"42":33}}],["max",{"2":{"47":1,"49":1,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["maxpool2d",{"2":{"29":1}}],["maxpool",{"2":{"29":2}}],["master",{"2":{"42":2}}],["masked",{"2":{"42":2}}],["mask=attn",{"2":{"42":1}}],["mask=none",{"2":{"42":1}}],["mask=false",{"2":{"29":1}}],["mask",{"2":{"29":1,"42":32}}],["math",{"2":{"42":2,"49":2}}],["match",{"2":{"19":1}}],["map",{"2":{"83":4}}],["mapping",{"2":{"29":1}}],["map直接涨了大概有0",{"2":{"3":1,"8":1}}],["make",{"2":{"29":2,"42":1,"49":2}}],["main",{"2":{"17":1,"19":1,"42":2,"49":1,"78":1,"93":1}}],["mhsa",{"2":{"13":1}}],["算是国内计算机领域的最高期刊了",{"2":{"8":1}}],["发表于今年的中科院一区",{"2":{"8":1}}],["官方的代码中存在许多bug而且参数都未定义",{"2":{"94":1}}],["官方代码的地址",{"2":{"55":1}}],["官方代码地址点击即可跳转",{"2":{"13":1}}],["官方代码地址",{"2":{"7":1,"10":1,"20":1,"27":2,"35":1,"44":1,"48":2}}],["官方地址",{"2":{"55":1}}],["官方论文地址点击即可跳转",{"2":{"13":1}}],["官方论文地址",{"2":{"7":1,"10":1,"20":1,"27":2,"35":1,"44":1,"48":2}}],["4gfops",{"2":{"113":1}}],["4gflops",{"2":{"45":1}}],["43691504",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["43691520",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["4371",{"2":{"42":1}}],["4040",{"2":{"42":1}}],["4488",{"2":{"42":1}}],["41",{"2":{"26":1,"61":1}}],["4",{"0":{"6":1,"31":1,"32":1,"40":1,"41":1,"50":1,"51":1,"60":1,"61":1,"69":1,"70":1,"71":1,"77":1,"79":1,"80":1,"84":1,"86":1,"88":1,"89":1,"95":1,"96":1,"97":1,"98":1,"104":1,"105":1,"106":1,"107":2,"109":1,"110":1,"111":1,"112":1,"113":1,"115":1,"118":1,"119":1,"121":1,"125":1,"126":2,"130":1,"132":1,"134":1,"135":1,"136":1},"1":{"11":1,"17":1,"40":1,"50":1,"60":1,"61":1,"71":1,"79":1,"80":1,"88":1,"89":1,"95":1,"97":1,"104":1,"105":1,"109":1,"111":1,"112":1,"113":1,"115":1,"118":1,"119":1,"121":1,"125":1,"126":1,"132":1,"134":1,"136":1},"2":{"12":1,"13":1,"19":15,"23":1,"26":2,"42":14,"56":2,"63":1,"64":1,"80":2,"89":2,"91":2,"94":1,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["就涨点了0",{"2":{"80":1,"129":1}}],["就像是得到了一副三维眼镜",{"2":{"5":1}}],["就好比三个人从不同的角度来观察同一幅画",{"2":{"5":1}}],["网络就能够更全面地理解图像内容",{"2":{"5":1}}],["即能够在多个层面上分析和识别图像特征",{"2":{"84":1}}],["即插即用",{"2":{"65":1}}],["即flatten模型",{"2":{"63":1}}],["即自注意力和卷积方式",{"2":{"48":1,"58":1}}],["即不改变输入",{"2":{"30":1}}],["即在不牺牲性能的情况下提高计算效率",{"2":{"24":1}}],["即可在yolov8模型中添加biformer注意力机制",{"2":{"22":1}}],["即色彩和纹理等特征",{"2":{"5":1}}],["即一次访问几十个连续字节的块",{"2":{"4":1}}],["它包含三个头部的1x1卷积",{"2":{"68":1}}],["它比较了卷积",{"2":{"68":1}}],["它只关注图像中的一小部分关键区域",{"2":{"65":1}}],["它是由一个标准卷积层生成",{"2":{"64":1}}],["它模仿自注意力",{"2":{"64":1}}],["它解决了传统线性注意力方法的两个主要问题",{"2":{"53":1}}],["它的核心思想是",{"2":{"48":1,"58":1}}],["它需要计算查询和键之间的成对相似度",{"2":{"43":1}}],["它们的维度为",{"2":{"43":1}}],["它们分别对应不同的感受野大小",{"2":{"13":1}}],["它在所有测试中都表现得非常好",{"2":{"27":1}}],["它结合了通道注意力和自注意力机制",{"2":{"27":1}}],["它将一个大的2d核分解成水平",{"2":{"20":1}}],["它主要通过将深度卷积层的2d卷积核分解为水平和垂直1d卷积核",{"2":{"14":1}}],["它通过三个不同的视角来分析输入的数据",{"2":{"5":1}}],["它具有较高的计算复杂度并且需要大量的内存",{"2":{"4":1}}],["值",{"2":{"68":1}}],["值标记来处理",{"2":{"4":1}}],["值对参与到密集矩阵乘法运算中",{"2":{"9":1}}],["值对进行全局的计算",{"2":{"9":1}}],["值对进行关注",{"2":{"4":1}}],["值对在空间上是分散的",{"2":{"4":1}}],["值对",{"2":{"4":2,"9":4,"15":1}}],["值对数量",{"2":{"4":1}}],["相对位置偏差也通过变形点计算",{"2":{"75":1}}],["相比于edsr",{"2":{"34":1}}],["相比之下",{"2":{"20":1}}],["相结合",{"2":{"31":1}}],["相关信息",{"2":{"17":1}}],["相关区域",{"2":{"4":1}}],["相反",{"2":{"4":1}}],["因为具体的效果还要看你的数据集和实验环境所影响",{"2":{"73":1}}],["因为资源有限我发的文章都要做对比实验所以本次实验我只用了一百张图片检测的是安全帽训练了一百个epoch",{"2":{"73":1}}],["因为资源有限",{"2":{"45":1}}],["因为专栏内容有许多",{"2":{"41":1,"77":1}}],["因为不同的yolov8模型版本可能目录结构不同",{"2":{"22":1}}],["因为它包含了一个区域级别的路由步骤和一个标记级别的注意力步骤",{"2":{"4":1}}],["因为现代gpu依赖于连续内存操作",{"2":{"4":1}}],["因为现在假设键",{"2":{"4":1}}],["因此",{"2":{"4":1}}],["首先我们需要在文件的开头导入我们的acmix模块",{"2":{"112":1}}],["首先我们需要在文件的开头导入我们的rcs",{"2":{"88":1}}],["首先我们需要在文件的开头导入我们的msda模块",{"2":{"50":1}}],["首先我们找到如下的目录",{"2":{"40":1,"79":1,"105":1}}],["首先我们找到该目录",{"2":{"29":1}}],["首先使用全局平均池化和全局最大池化",{"2":{"23":1}}],["首先",{"2":{"4":1,"14":1,"20":1}}],["首先确定了前k个",{"2":{"4":1}}],["而3d",{"2":{"84":1}}],["而不是固定地处理整个图像",{"2":{"65":1}}],["而不是直接在细粒度的标记级别上进行过滤",{"2":{"4":1}}],["而dat引入了可变形注意力机制",{"2":{"65":1}}],["而且还实现了语义信息的有效提取",{"2":{"31":1}}],["而且计算代价小",{"2":{"16":1}}],["而自注意力则关注于图像内部各个位置之间的关系",{"2":{"21":1,"34":1}}],["而其它块的注意力得分较低",{"2":{"13":1}}],["而在biformer中",{"2":{"9":1}}],["而下层路由器则负责捕捉局部区域的细节",{"2":{"1":1}}],["然后还有许多融合模块",{"2":{"98":1}}],["然后根据不同的范式",{"2":{"48":1,"58":1}}],["然后在其内部导入我们的检测头如下图所示",{"2":{"115":1}}],["然后在其内部建立一个新的py文件将核心代码复制粘贴进去即可",{"2":{"109":1}}],["然后在步骤三这里定义的字典中添加你导入的注意力机制名字即可",{"2":{"46":1}}],["然后在这个目录下创建一个py文件",{"2":{"40":1,"79":1,"105":1}}],["然后将acmix添加进去即可",{"2":{"118":1}}],["然后将acmix的核心代码复制进去",{"2":{"105":1}}],["然后将rcs",{"2":{"79":1,"97":1}}],["然后将msda添加进去即可",{"2":{"60":1}}],["然后将msda的核心代码复制进去",{"2":{"40":1}}],["然后将三个分支的输出进行平均聚合",{"2":{"30":1}}],["然后同样通过sigmoid函数生成注意力权重",{"2":{"30":1}}],["然后是批量归一化和sigmoid函数",{"2":{"23":1}}],["然后使用layernorm和最终的1x1卷积",{"2":{"23":1}}],["然后使用另一个1d核进行垂直方向上的卷积",{"2":{"20":1}}],["然后进行残差变换",{"2":{"16":1}}],["然后通过一个卷积层",{"2":{"30":1}}],["然后通过1x1卷积和3x3卷积处理repvgg块的输出",{"2":{"24":1}}],["然后通过残差变换来提取特征",{"2":{"16":1}}],["然后通过z池操作和一个大小为k×k的卷积层",{"2":{"16":1}}],["然后应用于排列变换后的输入张量",{"2":{"16":1}}],["然后每个头部内部使用不同的扩张率",{"2":{"13":1}}],["然后",{"2":{"8":1,"13":1,"75":1}}],["然后共同决定哪些部分最值得注意",{"2":{"5":1}}],["然后对其进行修剪",{"2":{"4":1}}],["然后关注它们的并集",{"2":{"4":1}}],["然而",{"2":{"4":1,"9":1}}],["不给大家描述过程了",{"2":{"85":1}}],["不一定位置对并没有进行多次实验调参什么的",{"2":{"76":1}}],["不同的扩张率",{"2":{"13":1}}],["不同",{"2":{"4":1}}],["不光让大家会添加到自己的模型在写论文的时候也能够有一定的参照",{"2":{"3":1}}],["其可以添加的位置有很多",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["其余的你没有大家不用添加",{"2":{"72":1}}],["其余使用方式看章节四",{"2":{"19":1}}],["其在每个分类中都显示了相对较高的准确率或者在相似的flops计算量下具有竞争力的准确率",{"2":{"63":1}}],["其主要的核心思想是",{"2":{"45":1}}],["其发布于2022年cvpr2022上同时被评选为best",{"2":{"45":1}}],["其它使用方式看章节四",{"2":{"42":1}}],["其它的注意力机制添加的方式都大致相同",{"2":{"22":1}}],["其解决了两个在传统线性注意力方法中存在的问题",{"2":{"28":1}}],["其方法在定量和定性方面显著优于现有的最先进方法",{"2":{"27":1}}],["其中1x1卷积在两者之间共享",{"2":{"68":1}}],["其中的两外两个模块",{"2":{"60":1}}],["其中c是通道数",{"2":{"23":1}}],["其中包括rcs",{"2":{"39":1}}],["其中包括",{"2":{"23":1}}],["其中包括自注意力子层",{"2":{"15":1}}],["其中",{"2":{"20":1}}],["其中只涉及到对于硬件友好的稠密矩阵乘法",{"2":{"4":1}}],["其与本质上是局部操作的卷积",{"2":{"4":1}}],["其全称是",{"2":{"3":1,"8":1}}],["所提出的biformer在相似的模型大小下显著优于基准模型的性能",{"2":{"4":1}}],["所提出的方法通过双层路由实现了动态的",{"2":{"4":1}}],["所以大家可以在中等和小目标检测层都添加acmix模块进行尝试",{"2":{"129":1}}],["所以大家可以在中等和小目标检测层都添加msda模块进行尝试",{"2":{"80":1}}],["所以如果能够帮助到大家希望大家能给点个赞和关注支持一下",{"2":{"83":1}}],["所以我这里进行了挺多的改动的",{"2":{"83":1}}],["所以我下面推荐几个添加的位",{"2":{"90":1,"98":1,"122":1,"123":1,"133":1,"138":1}}],["所以我下面推荐了几种我自己认为可能有效果的配合方式",{"2":{"70":1,"124":1}}],["所以我下面的改进和这篇文章只用了rcs",{"2":{"49":1}}],["所以没啥效果",{"2":{"82":1}}],["所以希望大家给博主点点赞收藏以下",{"2":{"52":1}}],["所以在看第四章添加教程的时候需要按照无参数的注意力机制进行添加",{"2":{"47":1}}],["所以非常推荐大家使用",{"2":{"36":1}}],["所以效率并不高",{"2":{"4":1}}],["所以它具有transformer模型的特性",{"2":{"4":1}}],["下一步我们就需要添加该机制到模型中让我们可以使用它",{"2":{"46":1}}],["下一步是应用标记到标记的注意力",{"2":{"4":1}}],["下部分支",{"2":{"30":1}}],["下面推荐几个版本的yaml文件给大家",{"2":{"70":1,"124":1}}],["下面是我添加了focused",{"2":{"120":1}}],["下面是我添加了lskattention的训练截图",{"2":{"71":1}}],["下面是dattention的训练过程和我添加的位置截图",{"2":{"110":1}}],["下面是添加了acmix的训练截图",{"2":{"135":1}}],["下面是添加了rcs",{"2":{"125":1}}],["下面是添加了d",{"2":{"111":1,"136":1}}],["下面是添加了msda的训练截图",{"2":{"107":1}}],["下面是添加了hattention",{"2":{"100":1}}],["下面是使用教程",{"2":{"52":1}}],["下面是对这个过程的描述",{"2":{"30":1}}],["下面先来分享我测试的对比图",{"2":{"45":1}}],["下面为大家提供的图像展示的是rcs",{"2":{"39":1}}],["下面我们找到文件",{"2":{"62":1}}],["下面我来分别讲解这三种主要的改进机制",{"2":{"54":1}}],["下面我将为大家展示rcs",{"2":{"31":1}}],["下面我通过图片来辅助大家理解这一优势",{"2":{"9":1}}],["下面的添加acmix是我实验结果的版本",{"2":{"129":1}}],["下面的添加msda是我实验结果的版本",{"2":{"80":1}}],["下面的是我将flattention放在neck中的截图",{"2":{"120":1}}],["下面的是将flattention机制我添加到了c2f和bottleneck",{"2":{"120":1}}],["下面的是放在c2f中的yaml配置",{"2":{"114":1}}],["下面的是放在neck部分的截图",{"2":{"114":1}}],["下面的配置文件为我修改的rcs",{"2":{"113":1}}],["下面的配置文件为我修改的c2f",{"2":{"95":1,"132":1}}],["下面的代码是dat的网络结构代码",{"2":{"94":1}}],["下面的代码是msda的核心代码",{"2":{"19":1}}],["下面的yaml文件我会给大家推荐",{"2":{"80":1,"129":1}}],["下面的图片是三重注意力",{"2":{"30":1}}],["下面的图片是三重注意力的一个抽象表示图",{"2":{"16":1}}],["下面的图片是论文中三重注意力机制和其它注意力机制的一个对比大家有兴趣可以看看",{"2":{"23":1}}],["下面来介绍作用机制biformer是一种结合了bi",{"2":{"4":1}}],["下层路由器则使用局部自注意力机制对每个图像块与其邻近的图像块进行交互",{"2":{"1":1}}],["本例中k=3",{"2":{"4":1}}],["本文的讲解主要包含三方面",{"2":{"45":1}}],["本文给大家带来的是yolov8改进dat",{"2":{"45":1}}],["本文给大家带来的改进内容是deformable",{"2":{"36":1}}],["本文给大家带来的改进机制是acmix自注意力机制的改进版本",{"2":{"48":1}}],["本文给大家带来的改进机制是focused",{"2":{"28":1}}],["本文给大家带来的改进机制是hattention注意力机制",{"2":{"21":1}}],["本文给大家带来的改进机制是msda",{"2":{"8":1}}],["本文给大家带来的改进机制是rcs",{"2":{"3":1}}],["本文给大家带来的改进是triplet",{"2":{"5":1}}],["本文还将提供代码实现细节和使用方法",{"2":{"14":1}}],["本文介绍",{"0":{"1":1,"3":1,"5":1,"8":1,"14":1,"21":1,"28":1,"36":1,"45":1,"48":1},"1":{"58":1,"68":1,"78":1}}],["双层路由注意力",{"2":{"4":1}}],["fpn",{"2":{"57":1,"83":1,"93":1}}],["fusevggforward",{"2":{"49":1}}],["fuse",{"2":{"49":4,"93":1}}],["functools",{"2":{"19":1}}],["functional",{"2":{"29":1,"49":2,"83":1,"94":1}}],["function",{"2":{"17":2,"83":3}}],["ffn",{"2":{"42":1}}],["fla",{"2":{"83":5}}],["flattention",{"2":{"101":2}}],["flatten",{"2":{"19":2,"29":2,"42":11}}],["flops",{"2":{"63":2}}],["floor",{"2":{"42":1}}],["float16",{"2":{"83":3}}],["float32",{"2":{"49":1}}],["float",{"2":{"42":28,"78":1,"83":3,"94":4}}],["faster",{"2":{"57":1,"83":1,"93":1}}],["factor=",{"2":{"94":1}}],["factor=3",{"2":{"83":1}}],["factor=30",{"2":{"42":3}}],["factor=squeeze",{"2":{"42":4}}],["factor=16",{"2":{"42":1}}],["factor",{"2":{"29":1,"42":13,"83":6,"94":5}}],["false",{"2":{"19":3,"42":5,"83":1,"94":1}}],["fixed",{"2":{"94":5}}],["fixme",{"2":{"19":1,"29":1}}],["field",{"2":{"64":1}}],["fill",{"2":{"42":2,"78":2,"94":1}}],["filtering",{"2":{"29":1}}],["first",{"2":{"42":2}}],["fc",{"2":{"78":2}}],["fc2",{"2":{"19":2,"42":2}}],["fc1",{"2":{"19":2,"42":2}}],["feat=dim",{"2":{"42":1}}],["feat",{"2":{"42":22}}],["feature",{"2":{"29":1,"39":1,"42":6,"83":4}}],["features=in",{"2":{"49":1}}],["features=out",{"2":{"49":1}}],["features=mlp",{"2":{"19":2,"42":2}}],["features=dim",{"2":{"19":2,"42":2}}],["features=none",{"2":{"19":2,"42":2}}],["features",{"2":{"19":16,"42":18}}],["feed",{"2":{"15":1}}],["free",{"2":{"29":2}}],["from",{"2":{"19":4,"29":3,"37":1,"42":7,"49":1,"56":1,"80":1,"83":1,"89":1,"91":1,"94":2,"95":1,"101":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["fracpool",{"2":{"29":3}}],["frac",{"2":{"17":4}}],["focuslinearattention",{"2":{"101":1}}],["focusing",{"2":{"83":7}}],["focusedlinearattention",{"2":{"83":2,"101":1}}],["focusedlinearattention代码",{"0":{"83":1}}],["focused",{"0":{"35":1,"53":1,"101":1,"108":1,"114":1,"120":1},"1":{"43":1,"53":1,"63":1,"114":1,"120":1},"2":{"28":2,"53":5}}],["foo",{"2":{"17":2}}],["forward",{"2":{"15":1,"19":13,"26":1,"29":4,"42":15,"47":4,"49":4,"57":6,"78":1,"83":7,"93":11,"94":2}}],["for",{"2":{"8":1,"19":11,"29":8,"42":19,"49":3,"56":1,"57":3,"78":1,"80":1,"83":4,"89":1,"91":1,"93":5,"94":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["f",{"2":{"4":1,"19":2,"29":4,"41":1,"42":2,"49":3,"72":1,"78":4,"83":1,"94":6,"101":1,"118":2}}],["f分别是",{"2":{"4":1}}],["einsum",{"2":{"83":5,"94":2}}],["einops",{"2":{"29":1,"42":1,"83":1,"94":6}}],["encoding",{"2":{"78":1,"83":3}}],["end",{"2":{"42":2,"49":1}}],["efficient",{"2":{"63":1}}],["e=1",{"2":{"57":1,"83":1,"93":1}}],["e=0",{"2":{"49":1,"57":2,"83":2,"93":2}}],["eps",{"2":{"49":5}}],["eps=1e",{"2":{"19":2,"47":1}}],["equivalent",{"2":{"49":1}}],["each",{"2":{"42":1}}],["even",{"2":{"42":1}}],["emb",{"2":{"29":3}}],["embedding",{"2":{"19":3,"42":10}}],["embed",{"2":{"19":23,"42":34}}],["expansion",{"2":{"57":2,"83":2,"93":2}}],["expanding",{"2":{"84":1}}],["expand",{"2":{"29":2,"94":4}}],["examples",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["extend",{"2":{"57":2,"83":2,"93":2}}],["ext",{"2":{"42":10}}],["extraction",{"2":{"42":3}}],["extra",{"2":{"29":1}}],["excitation",{"2":{"23":1}}],["exist",{"2":{"19":2}}],["elif",{"2":{"19":4,"26":5,"29":9,"41":1,"42":4,"72":1,"94":4,"101":1,"118":1}}],["else",{"2":{"19":10,"29":10,"42":12,"47":3,"49":7,"57":1,"78":2,"83":3,"93":5,"94":6}}],["e",{"2":{"4":1,"29":1,"49":1,"56":1,"57":2,"80":1,"83":2,"89":1,"91":1,"93":2,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["减少网络计算负担",{"2":{"12":1}}],["减少内存消耗",{"2":{"12":1}}],["减少了重复的计算量",{"2":{"68":1}}],["减少了计算复杂性和内存占用",{"2":{"14":1}}],["减少了计算量和内存占用",{"2":{"9":1}}],["减少了考虑的键",{"2":{"4":1}}],["减少通道的空间对象注意力",{"2":{"3":1}}],["轴向条纹和扩张窗口",{"2":{"4":1}}],["d=1",{"2":{"93":2}}],["dlka",{"2":{"93":4,"132":3}}],["dat可以是一种即插即用的注意力机制",{"2":{"122":1}}],["dat可添加的位置",{"0":{"116":1},"1":{"122":1,"127":1}}],["dattentionbaseline",{"2":{"94":1}}],["dattention的代码复现",{"2":{"45":1}}],["dat即插即用的代码块",{"0":{"94":1}}],["dat与其他视觉transformer模型和cnn模型中的dcn",{"2":{"85":1}}],["dat和其他机制的对比",{"0":{"85":1}}],["dat通过引入可变形注意力机制",{"2":{"65":1}}],["dat动态地选择采样点",{"2":{"65":1}}],["dat的yaml文件和训练过程",{"0":{"110":1}}],["dat的网络结构图",{"0":{"75":1}}],["dat的网络结构思想",{"0":{"55":1},"1":{"65":1,"75":1,"85":1},"2":{"45":1}}],["dat的设计允许它适应不同的图像大小和内容",{"2":{"65":1}}],["dat的核心思想主要包括以下几个方面",{"2":{"65":1}}],["dat的主要思想和改进",{"0":{"65":1}}],["dat",{"2":{"63":1,"65":1}}],["dat论文地址",{"2":{"55":1}}],["data=kernel",{"2":{"78":1}}],["data",{"2":{"49":1,"57":1,"63":1,"78":2,"83":1,"93":2}}],["dtype=dtype",{"2":{"94":4}}],["dtype=np",{"2":{"49":1}}],["dtype=x",{"2":{"42":1}}],["dtype",{"2":{"42":1,"78":1,"83":3,"94":7}}],["ddp",{"2":{"29":1}}],["d代表扩张率​​",{"2":{"20":1}}],["dwc",{"2":{"53":1,"83":2,"94":6}}],["dwconv",{"2":{"29":5}}],["dwconv=3",{"2":{"29":1}}],["dw",{"2":{"20":3,"64":1}}],["dpr",{"2":{"19":1,"42":1}}],["docs",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["document",{"2":{"17":1}}],["don",{"2":{"42":1}}],["downsaple",{"2":{"29":1}}],["downsampling",{"2":{"29":1}}],["downsample=downsample",{"2":{"42":1}}],["downsample=downsamples",{"2":{"19":2}}],["downsample=none",{"2":{"42":3}}],["downsample=true",{"2":{"19":2}}],["downsamples=",{"2":{"19":1}}],["downsample",{"2":{"19":6,"29":26,"42":10}}],["down",{"2":{"29":7,"49":2}}],["do",{"2":{"29":2}}],["doesn",{"2":{"19":1}}],["displacement",{"2":{"94":6}}],["divided",{"2":{"83":1}}],["divisor",{"2":{"49":7}}],["divisible",{"2":{"29":1,"49":3}}],["div",{"2":{"42":1,"94":4}}],["di",{"2":{"34":1}}],["different",{"2":{"42":1}}],["differentiable",{"2":{"29":6}}],["diffrentiable",{"2":{"29":1}}],["diff",{"2":{"29":14,"42":1}}],["dims=",{"2":{"42":2}}],["dimension",{"2":{"29":1,"42":1}}],["dim=1",{"2":{"42":1,"47":1,"49":2,"78":1,"83":1}}],["dim=16",{"2":{"19":1}}],["dim=self",{"2":{"29":1}}],["dim=none",{"2":{"29":1}}],["dim=2",{"2":{"29":1,"94":1}}],["dim=72",{"2":{"19":2}}],["dim=int",{"2":{"19":2}}],["dim=embed",{"2":{"19":1,"42":3}}],["dim=dim",{"2":{"19":2,"42":6}}],["dim=96",{"2":{"19":3,"42":3}}],["dim=",{"2":{"19":2,"29":9,"42":2,"83":4}}],["dim",{"2":{"19":82,"26":51,"29":47,"42":93,"49":3,"78":18,"83":23,"93":5,"94":2}}],["dilation=d",{"2":{"93":1}}],["dilation=dilation",{"2":{"19":3,"47":1,"49":1,"93":2}}],["dilation=3",{"2":{"26":8,"93":1}}],["dilation=2",{"2":{"26":4}}],["dilation=",{"2":{"19":3}}],["dilation=1",{"2":{"19":1,"47":1,"49":1,"78":1,"93":1}}],["dilation",{"2":{"13":1,"19":24,"78":3,"93":2}}],["dilatestage",{"2":{"19":2}}],["dilateblock",{"2":{"19":2}}],["dilate",{"2":{"19":8}}],["dilateattention",{"2":{"19":2}}],["dilated",{"2":{"8":1,"64":1}}],["dilateformer",{"2":{"8":1,"19":8}}],["drop=drop",{"2":{"19":6,"42":5}}],["drop=attn",{"2":{"19":6,"42":4}}],["drop=0",{"2":{"19":16,"42":9,"83":2,"94":2}}],["dropout",{"2":{"19":5,"42":14,"83":2,"94":2}}],["drop",{"2":{"19":47,"42":60,"83":5,"94":6}}],["droppath",{"2":{"19":3,"42":3}}],["deit",{"2":{"63":1}}],["dense",{"2":{"49":4}}],["denoising",{"2":{"42":1}}],["device=device",{"2":{"83":1,"94":5}}],["device=x",{"2":{"42":1}}],["device",{"2":{"42":2,"49":1,"83":2,"94":7}}],["detect",{"2":{"56":3,"80":3,"89":3,"91":3,"95":3,"104":3,"113":3,"119":3,"129":3,"131":3,"132":3,"134":3}}],["detection",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["detecthead",{"2":{"41":1,"77":1}}],["detr的模型上看看效果",{"2":{"43":1}}],["detach",{"2":{"29":3}}],["dep",{"2":{"78":5}}],["deploy",{"2":{"49":3}}],["deploy=false",{"2":{"49":1}}],["dependent",{"2":{"29":2}}],["depth=depth",{"2":{"42":1}}],["depth=depths",{"2":{"19":2,"42":1}}],["depths",{"2":{"19":6,"42":5}}],["depths=",{"2":{"19":4,"42":1}}],["depth",{"2":{"19":4,"42":16,"56":1,"64":2,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["decay",{"2":{"19":1,"42":3}}],["deformconv2d",{"2":{"93":1}}],["deformconv",{"2":{"93":4}}],["deform",{"2":{"64":1,"93":2}}],["deformable",{"0":{"44":1,"54":1},"1":{"54":1,"64":1,"74":1,"84":1},"2":{"36":3,"45":1,"54":1,"63":1,"64":2,"65":2,"74":1,"84":1,"93":6,"134":3}}],["define",{"2":{"42":2}}],["default",{"2":{"19":3,"42":55,"66":1,"93":3}}],["def",{"2":{"19":28,"26":2,"29":8,"42":37,"47":7,"49":15,"57":5,"78":7,"83":7,"93":15,"94":6}}],["deep",{"2":{"17":3,"42":2}}],["demo",{"2":{"17":1}}],["d",{"0":{"93":2,"111":1,"130":1,"132":1,"134":1,"136":1,"137":1},"1":{"132":1,"134":1,"136":1,"138":1,"139":1},"2":{"4":1,"19":8,"20":1,"42":7,"43":2,"54":4,"64":1,"83":9,"84":10,"93":10,"94":2,"132":1,"138":2}}],["branch",{"2":{"49":16}}],["bn",{"2":{"47":4,"49":13,"93":2}}],["bn=true",{"2":{"47":1}}],["buffer",{"2":{"42":2}}],["build",{"2":{"19":2,"42":3,"49":1}}],["bmlc",{"2":{"29":3}}],["borrowed",{"2":{"94":1}}],["body",{"2":{"42":3}}],["bottleneck",{"2":{"41":1,"57":6,"77":1,"83":6,"93":6}}],["both",{"2":{"29":2,"42":1}}],["bool",{"2":{"29":3,"42":11,"83":3}}],["by",{"2":{"29":4,"49":1,"83":1}}],["b部分",{"2":{"24":1}}],["blob",{"2":{"42":2}}],["block=true",{"2":{"19":1}}],["block=cpe",{"2":{"19":4}}],["block=false",{"2":{"19":4}}],["blocks",{"2":{"19":6,"42":8}}],["block",{"2":{"19":13,"23":1,"42":6,"49":3}}],["blk",{"2":{"19":4,"42":2}}],["beta",{"2":{"49":3}}],["before",{"2":{"29":1,"42":4}}],["be",{"2":{"19":1,"29":3,"42":1,"83":1}}],["backbone",{"2":{"56":4,"80":4,"89":4,"91":4,"95":4,"98":1,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"133":1,"134":4}}],["batch",{"2":{"93":1}}],["batchsize",{"2":{"49":3}}],["batchnorm2d",{"2":{"19":7,"47":1,"49":3,"93":1}}],["base",{"2":{"19":1}}],["based",{"2":{"12":1,"39":1,"42":2}}],["basicconv",{"2":{"47":3}}],["basicsr",{"2":{"42":2}}],["basic",{"2":{"19":2}}],["badge",{"2":{"17":1}}],["bilinear",{"2":{"94":2}}],["bilevelroutingattention",{"2":{"29":2,"37":1,"56":3}}],["binarize",{"2":{"42":1}}],["biasid",{"2":{"49":2}}],["bias1x1",{"2":{"49":2}}],["bias3x3",{"2":{"49":2}}],["bias",{"2":{"19":11,"42":38,"49":3,"78":2,"83":2,"94":14}}],["bias=bias",{"2":{"29":1,"47":1}}],["bias=false",{"2":{"19":11,"42":1,"47":1,"49":1,"78":1,"93":2,"94":2}}],["bias=qkv",{"2":{"19":8,"42":7,"83":2}}],["bias=true",{"2":{"19":4,"29":1,"42":6,"49":3,"78":1,"83":1,"93":2,"94":1}}],["bi",{"2":{"4":1,"29":1}}],["biformer能够有效地利用双层路由注意力机制",{"2":{"15":1}}],["biformer能够同时捕捉全局和局部的特征信息",{"2":{"1":1}}],["biformer块是biformer的基本构建单元",{"2":{"15":1}}],["biformer块的详细信息",{"2":{"15":1}}],["biformer通过引入双层路由注意力机制",{"2":{"15":1}}],["biformer利用双层路由注意力机制",{"2":{"9":1}}],["biformer注意力机制的优势和劣势如下",{"2":{"9":1}}],["biformer的整体架构",{"2":{"15":1}}],["biformer的结构",{"0":{"15":1}}],["biformer的注意力机制具有高效的计算性能和查询感知的自适应性",{"2":{"9":1}}],["biformer的双层路由注意力机制引入了额外的参数和超参数",{"2":{"9":1}}],["biformer的双层路由注意力机制允许模型根据每个查询自适应地关注最相关的键",{"2":{"9":1}}],["biformer的优劣势",{"0":{"9":1}}],["biformer的作用机制",{"0":{"4":1}}],["biformer",{"2":{"4":1,"37":1}}],["biformer论文地址csdn",{"2":{"4":1}}],["biformer模型的核心思想是引入了双层路由注意力机制",{"2":{"1":1}}],["biformer是一种结合了bi",{"2":{"1":1}}],["b",{"2":{"4":1,"16":1,"17":21,"19":27,"29":3,"42":57,"68":1,"75":1,"78":17,"83":34,"94":51}}],["全局操作",{"2":{"4":1}}],["abs",{"2":{"94":1}}],["absolute",{"2":{"19":1,"42":6}}],["agpl",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["aggregate",{"2":{"39":1}}],["aggregation",{"2":{"12":2,"31":2,"39":1}}],["affine=true",{"2":{"47":1}}],["after",{"2":{"29":1,"42":4}}],["ape=false",{"2":{"42":1}}],["ape",{"2":{"42":5}}],["applies",{"2":{"57":1,"83":1,"93":1}}],["applied",{"2":{"42":2}}],["apply",{"2":{"19":1,"42":1,"93":1}}],["append",{"2":{"19":1,"42":5}}],["avoid",{"2":{"29":2}}],["avg",{"2":{"23":1,"30":1,"49":1,"94":1}}],["avgpool",{"2":{"19":2,"29":2}}],["avgpool2d",{"2":{"19":2,"29":1}}],["avgpool2",{"2":{"19":1}}],["avgpool3",{"2":{"19":2}}],["autopad",{"2":{"93":2}}],["auto",{"2":{"29":5,"93":1}}],["acmix是一种即插即用的可替换卷积的模块",{"2":{"133":1}}],["acmix是一种混合模型",{"2":{"58":1}}],["acmix添加步骤",{"0":{"96":1},"1":{"105":1,"112":1,"118":1}}],["acmix旨在通过共享计算资源",{"2":{"68":1}}],["acmix",{"2":{"68":1,"78":4,"118":1,"129":3,"131":1}}],["acmix可以灵活地嵌入到不同的网络结构中",{"2":{"68":1}}],["acmix模型的主要改进机制可以分为以下两点",{"2":{"58":1}}],["acmix的训练过程截图",{"0":{"135":1}}],["acmix的yaml版本二",{"0":{"131":1}}],["acmix的yaml版本一",{"0":{"129":1}}],["acmix的yaml文件和运行记录",{"0":{"124":1},"1":{"129":1,"131":1,"133":1,"135":1}}],["acmix的核心代码",{"2":{"78":1}}],["acmix的基本原理",{"0":{"58":1},"1":{"68":1,"78":1}}],["acmix的框架原理",{"2":{"48":1}}],["acmix既能利用自注意力的全局感知能力",{"2":{"48":1,"58":1}}],["acmix首先使用1x1卷积对输入特征图进行投影",{"2":{"48":1,"58":1}}],["according",{"2":{"29":1}}],["act=true",{"2":{"93":1}}],["act=frelu",{"2":{"57":1,"83":1,"93":1}}],["actual",{"2":{"29":2,"93":1}}],["activation",{"2":{"29":2,"42":1,"93":6}}],["act",{"2":{"19":12,"29":4,"42":8,"93":8}}],["arguments",{"2":{"57":1,"83":1,"93":2}}],["args",{"2":{"29":2,"41":2,"42":13,"56":1,"72":2,"80":1,"89":1,"91":1,"93":1,"95":1,"101":2,"104":1,"113":1,"118":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["arange",{"2":{"42":6,"94":2}}],["artifact",{"2":{"42":1}}],["archs",{"2":{"42":1}}],["arch",{"2":{"42":3}}],["are",{"2":{"17":2,"29":1,"42":3}}],["a部分",{"2":{"24":1}}],["advance",{"2":{"42":1}}],["addmodules",{"2":{"109":1}}],["add",{"2":{"42":7,"49":2,"57":2,"83":2,"93":2}}],["addeventlistener",{"2":{"17":1}}],["adaptivemaxpool2d",{"2":{"29":1}}],["adaptiveavgpool2d",{"2":{"29":1,"42":1}}],["adaptiveavgpool1d",{"2":{"19":1}}],["ada",{"2":{"29":3}}],["and",{"2":{"19":2,"29":10,"42":8,"49":1,"57":2,"83":2,"93":3,"94":5}}],["at",{"2":{"19":1,"42":2}}],["att^2",{"2":{"78":4}}],["att",{"2":{"78":33}}],["att=7",{"2":{"78":1}}],["attenblocks",{"2":{"42":2}}],["attention可以帮助模型更有效地融合不同层次的特征",{"2":{"123":1}}],["attention可添加的位置",{"0":{"117":1,"123":1},"1":{"123":1,"128":1}}],["attention到模型中",{"0":{"92":1},"1":{"101":1,"108":1,"114":1,"120":1}}],["attention是一种即插即用的模块",{"2":{"123":1}}],["attention是一种无参数的注意力机制如果你想要放在主干上就按照无参数的注意力机制配置就可以",{"2":{"77":1}}],["attention是对传统线性注意力方法的一种重要改进",{"2":{"28":1,"53":1}}],["attention一个是triplet",{"2":{"77":1}}],["attention机制的c2f和bottleneck",{"0":{"57":1}}],["attention机制原理",{"0":{"10":1},"1":{"16":1,"23":1,"30":1}}],["attention通过特殊的设计来增加特征的多样性和丰富性",{"2":{"53":1}}],["attention通过改进的机制增强了这种聚焦能力",{"2":{"53":1}}],["attentiongate",{"2":{"47":5}}],["attention和其它简单注意力机制的对比",{"0":{"23":1}}],["attention=",{"2":{"19":1,"29":1}}],["attention的训练截图",{"2":{"120":1}}],["attention的训练过程截图",{"0":{"120":1}}],["attention的位置",{"2":{"95":1}}],["attention的yaml文件",{"0":{"114":1}}],["attention的yaml文件二",{"0":{"104":1}}],["attention的yaml文件一",{"0":{"95":1}}],["attention的yaml文件和训练截图",{"0":{"86":1,"108":1},"1":{"95":1,"104":1,"111":1,"114":1,"120":1}}],["attention的本体代码",{"2":{"77":1}}],["attention的添加教程",{"0":{"77":1,"101":1}}],["attention的提出",{"0":{"53":1}}],["attention的核心代码",{"0":{"47":1}}],["attention的完整代码",{"0":{"38":1},"1":{"47":1,"57":1}}],["attention的机制原理",{"0":{"35":1},"1":{"43":1,"53":1,"63":1}}],["attention的实现流程",{"0":{"30":1}}],["attention的基本原理和框架",{"2":{"28":1}}],["attention的基本原理",{"0":{"16":1}}],["attention的视觉transformer模型",{"2":{"1":1,"4":1}}],["attention三重注意力机制",{"2":{"5":1}}],["attention",{"0":{"67":1},"1":{"77":1,"86":1,"95":1,"104":1,"111":1},"2":{"3":1,"4":1,"13":1,"15":1,"16":2,"19":7,"20":4,"23":2,"27":1,"28":1,"29":14,"30":1,"42":22,"45":1,"53":2,"54":1,"63":1,"64":2,"65":2,"84":2,"93":4,"123":1,"134":3}}],["attn",{"2":{"19":37,"26":10,"29":13,"42":68,"83":3,"93":6,"94":26}}],["assert",{"2":{"19":4,"29":10,"42":4,"49":1,"83":1,"94":1}}],["as",{"2":{"19":1,"26":1,"29":4,"42":3,"49":3,"78":1,"83":1,"93":1,"94":3,"101":1}}],["aside",{"2":{"17":1}}],["align",{"2":{"94":2}}],["aligned",{"2":{"17":1}}],["always",{"2":{"29":1}}],["all",{"2":{"29":1,"78":2,"93":1}}],["alert",{"2":{"17":2}}],["a",{"2":{"4":1,"16":1,"17":23,"19":2,"29":1,"42":9,"57":1,"68":1,"75":1,"78":2,"83":1,"93":1}}],["从一个1x1卷积层开始",{"2":{"23":1}}],["从图中可以看出",{"2":{"20":1,"64":1}}],["从而允许卷积核动态地适应图像的内容",{"2":{"74":1}}],["从而在保持较低计算成本的同时",{"2":{"48":1,"58":1}}],["从而在各种视觉任务中提高性能",{"2":{"23":1}}],["从而将复杂度降低到",{"2":{"43":1}}],["从而克服了densenet中密集连接的低效率问题",{"2":{"31":1}}],["从而提供更为精确的上采样结果",{"2":{"34":1}}],["从而提供更为精确的结果",{"2":{"21":1}}],["从而提高检测的效率",{"2":{"75":1}}],["从而提高对图像内容的整体理解",{"2":{"13":1}}],["从而提高了网络的计算和能源效率",{"2":{"31":1}}],["从而提高了计算性能和任务表现",{"2":{"15":1}}],["从而提高了计算效率",{"2":{"9":1}}],["从而提高了模型在视觉任务中的性能",{"2":{"1":1}}],["从而计算注意力权重",{"2":{"16":1}}],["从而能更加全面地捕捉图像中的信息",{"2":{"13":1}}],["从而实现了与原始大尺寸2d核相似的效果",{"2":{"20":1}}],["从而实现稀疏性",{"2":{"9":1}}],["从而实现这一点",{"2":{"4":1}}],["从而实现高效的计算",{"2":{"4":1}}],["从a",{"2":{"4":1}}],["在最终的输出层前加入d",{"2":{"138":1}}],["在最终的输出层前加入注意力机制可以使模型在做出最终预测之前",{"2":{"90":1,"122":1}}],["在残差网络的残差连接中加入d",{"2":{"138":1}}],["在残差网络的残差连接中加入acmix",{"2":{"133":1}}],["在残差网络的残差连接中加入triplet",{"2":{"123":1}}],["在残差网络的残差连接中加入mhsa",{"2":{"98":1}}],["在残差网络的残差连接中加入注意力机制",{"2":{"90":1,"122":1}}],["在特征金字塔网络之前",{"2":{"90":1,"122":1}}],["在场的focusedlinearattention代码是用于transformer的想要将其用于yolo上是需要进行很大改动的",{"2":{"83":1}}],["在第二阶段将两种路径得到的特征相加",{"2":{"78":1}}],["在第一阶段使用三个1x1卷积对输入特征图进行投影",{"2":{"78":1}}],["在acmix中",{"2":{"78":1}}],["在acmix模型中",{"2":{"68":1}}],["在可变形注意力机制中",{"2":{"65":1}}],["在保持或稍微增加计算量的前提下",{"2":{"63":1}}],["在开头导入我们的注意力机制",{"2":{"62":1}}],["在开始介绍作用机制之前",{"2":{"4":1}}],["在上面我们已经将代码复制粘贴到",{"2":{"62":1}}],["在其中注册我们的acmix模块",{"2":{"112":1}}],["在其中注册我们的rcs",{"2":{"88":1}}],["在其中注册我们的msda模块",{"2":{"50":1}}],["在其中创建一个文件",{"2":{"19":1}}],["在swin",{"2":{"43":1}}],["在deformable",{"2":{"64":1}}],["在deit中d=64",{"2":{"43":1}}],["在dilateformer论文中",{"2":{"13":1}}],["在yolov8中",{"2":{"36":1}}],["在重建时使用了最多的像素",{"2":{"34":1}}],["在rcs",{"2":{"31":2,"39":1}}],["在提取特征时考虑了通道之间和空间位置之间的相关性",{"2":{"27":1}}],["在深层特征提取部分",{"2":{"27":1}}],["在不同放大倍数",{"2":{"27":1}}],["在推理时减少计算复杂度",{"2":{"24":1}}],["在推理阶段",{"2":{"12":1,"24":2}}],["在van中包括标准深度卷积",{"2":{"20":1}}],["在核大小增加时会导致更高的gflops",{"2":{"20":1}}],["在这里我给大家推荐两种添加的方式",{"2":{"82":1}}],["在这里说一下这个原文是rcs",{"2":{"49":1}}],["在这张图中",{"2":{"43":1}}],["在这个添加的过程中",{"2":{"22":1}}],["在这个比较中",{"2":{"20":1}}],["在这篇文章中",{"2":{"14":1}}],["在降低计算和内存成本的同时",{"2":{"20":1}}],["在进行卷积操作时",{"2":{"20":1}}],["在每个图中",{"2":{"63":1}}],["在每个块中实现内容感知的稀疏性",{"2":{"15":1}}],["在每张图中",{"2":{"13":1}}],["在msda中",{"2":{"13":1}}],["在训练阶段",{"2":{"12":1,"24":1}}],["在传统的注意力机制中",{"2":{"9":1}}],["在查询感知的情况下",{"2":{"9":1}}],["在包括图像分类",{"2":{"4":1}}],["在biformer中",{"2":{"1":1}}],["cg",{"2":{"94":1}}],["c=self",{"2":{"94":2}}],["cswin",{"2":{"63":1}}],["csp",{"2":{"57":2,"83":2,"93":2}}],["cvt",{"2":{"63":1}}],["cv2",{"2":{"57":6,"83":6,"93":6}}],["cv1",{"2":{"57":6,"83":7,"93":7}}],["ceil",{"2":{"49":1}}],["centered",{"2":{"17":1}}],["c2",{"2":{"49":7,"57":8,"83":8,"93":12}}],["c2f",{"2":{"41":1,"56":8,"57":2,"77":1,"80":8,"83":2,"89":8,"91":8,"93":3,"95":8,"104":8,"119":4,"129":8,"131":8,"132":8,"134":8}}],["c1",{"2":{"49":8,"57":5,"83":5,"93":7}}],["cw",{"2":{"47":2}}],["cnt",{"2":{"42":3}}],["cnn",{"2":{"20":1}}],["cross",{"2":{"42":1}}],["crop",{"2":{"29":1}}],["cyclic",{"2":{"42":2}}],["chunk",{"2":{"49":1,"57":2,"83":3,"93":3}}],["checkpoint=use",{"2":{"42":2}}],["checkpoint=false",{"2":{"42":3}}],["checkpointing",{"2":{"42":3}}],["checkpoint",{"2":{"42":7}}],["ch=c",{"2":{"42":1}}],["ch",{"2":{"41":1,"42":6,"57":2,"72":1,"83":2,"93":4,"101":1,"118":2}}],["chans=embed",{"2":{"42":2}}],["chans=0",{"2":{"42":2}}],["chans=in",{"2":{"19":1}}],["chans=3",{"2":{"19":2,"42":3}}],["chans",{"2":{"19":4,"42":10}}],["channelattention",{"2":{"42":3}}],["channels=32",{"2":{"94":1}}],["channels=2",{"2":{"93":1}}],["channels=head",{"2":{"83":2}}],["channels=out",{"2":{"49":4}}],["channels=internal",{"2":{"49":2}}],["channels=input",{"2":{"49":2}}],["channels=in",{"2":{"49":4,"93":3}}],["channels",{"2":{"19":17,"42":10,"47":1,"49":34,"56":1,"57":3,"80":1,"83":3,"89":1,"91":1,"93":7,"94":17,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["channel",{"2":{"3":1,"12":1,"31":1,"42":4,"49":2}}],["cpb",{"2":{"94":4}}],["cpb=false",{"2":{"94":1}}],["cpy",{"2":{"29":1}}],["cpe",{"2":{"19":29}}],["cuda=true",{"2":{"78":1}}],["cuda",{"2":{"29":1,"78":4}}],["calculate",{"2":{"42":10}}],["call",{"2":{"29":1,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["cat",{"2":{"42":2,"47":1,"49":2,"56":4,"57":2,"78":2,"80":4,"83":2,"89":4,"91":4,"93":2,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["cascade",{"2":{"39":1}}],["case",{"2":{"29":1}}],["caused",{"2":{"29":1}}],["caution",{"2":{"17":1}}],["cannot",{"2":{"29":1,"42":1}}],["cab",{"2":{"27":1,"42":5}}],["cbam",{"2":{"23":1}}],["clamp",{"2":{"94":1}}],["classical",{"2":{"42":2}}],["classes",{"2":{"19":4,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["classes=1000",{"2":{"19":1}}],["class",{"2":{"19":11,"26":1,"29":4,"42":14,"47":4,"49":4,"57":2,"78":1,"83":3,"93":6,"94":2}}],["cls代表检测到的对象中的类别数量",{"2":{"39":1}}],["clone",{"2":{"19":1,"26":1,"93":2}}],["click",{"2":{"17":1}}],["cfg=",{"2":{"66":1}}],["cfg",{"2":{"19":7,"56":1,"66":1}}],["cdots",{"2":{"17":1}}],["c",{"2":{"16":3,"17":11,"19":18,"23":1,"29":53,"42":46,"49":8,"57":10,"68":1,"78":3,"83":39,"93":8,"94":16}}],["corners=true",{"2":{"94":2}}],["coatnet的性能对比",{"2":{"63":1}}],["coords",{"2":{"42":39}}],["cool",{"2":{"17":1}}],["codes",{"2":{"42":1}}],["code",{"2":{"17":6,"42":1}}],["concat",{"2":{"56":4,"80":4,"89":4,"91":4,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["connection=resi",{"2":{"42":1}}],["connection=",{"2":{"42":2}}],["connection",{"2":{"42":9}}],["contiguous",{"2":{"29":3,"42":9,"47":4,"49":1,"94":1}}],["contextual",{"2":{"63":1}}],["context",{"2":{"23":1}}],["consuming",{"2":{"42":1}}],["consumes",{"2":{"29":1}}],["consider",{"2":{"29":1}}],["console",{"2":{"17":3}}],["constants",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["constant",{"2":{"19":3,"42":3}}],["constraints",{"2":{"19":1}}],["const",{"2":{"17":31}}],["conv=3",{"2":{"78":1}}],["convnext",{"2":{"63":1}}],["convnets",{"2":{"42":1}}],["conv1",{"2":{"26":2,"49":3,"78":2,"93":2}}],["conv0",{"2":{"93":2}}],["conv0v",{"2":{"26":7}}],["conv0h",{"2":{"26":7}}],["conv2",{"2":{"19":2,"78":2}}],["conv2d",{"2":{"19":15,"26":25,"29":1,"42":11,"47":1,"49":4,"64":2,"78":6,"83":2,"93":5,"94":7}}],["conv3",{"2":{"19":3,"49":3,"78":2}}],["convolutions",{"2":{"57":2,"83":2,"93":2}}],["convolutional",{"2":{"23":1,"42":2,"63":1}}],["convolution",{"2":{"12":1,"64":3,"74":1,"93":3}}],["conv",{"2":{"4":1,"20":3,"23":1,"26":14,"29":2,"30":1,"41":1,"42":30,"47":4,"49":6,"56":7,"57":4,"77":1,"78":27,"80":7,"83":4,"89":7,"91":7,"93":12,"94":3,"95":7,"104":7,"113":7,"119":7,"129":7,"131":7,"132":7,"134":7}}],["color",{"2":{"17":1}}],["col",{"2":{"17":2}}],["compound",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["compatible",{"2":{"42":1}}],["compress",{"2":{"42":12,"47":4}}],["com",{"2":{"4":1,"42":2,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["二",{"0":{"4":1,"7":1,"10":1,"13":1,"20":1,"27":1,"35":1,"44":1,"55":1},"1":{"12":1,"16":1,"18":1,"23":1,"24":1,"30":1,"31":1,"34":1,"39":1,"43":1,"53":1,"54":1,"63":1,"64":1,"65":1,"74":1,"75":1,"84":1,"85":1},"2":{"48":1}}],["亲测在小目标检测和大尺度目标检测的数据集上都有大幅度的涨点效果",{"2":{"3":1,"8":1}}],["来提高模型的处理效率和检测精度",{"2":{"3":1}}],["这对于体积重建和更复杂的医学成像任务非常有用",{"2":{"84":1}}],["这对于理解图像的不同抽象层次是非常重要的",{"2":{"13":1}}],["这也是一个读者和我说的想要帮忙解决一下这个问题困扰了他很久",{"2":{"78":1}}],["这主要通过以下步骤实现",{"2":{"78":1}}],["这两种方法的添加方式有些不同",{"2":{"77":1}}],["这两步卷积操作串联执行",{"2":{"20":1}}],["这有助于提升分割的精确性和边缘定义",{"2":{"74":1}}],["这有助于提升模型对不同大小目标的检测能力",{"2":{"31":1}}],["这一改进显著提高了模型map",{"2":{"36":1}}],["这一机制通过动态调整卷积核的形状和大小来适应不同的图像特征",{"2":{"36":1}}],["这一模块用于增强跨窗口信息的交互",{"2":{"27":1}}],["这表明hat在精细化重建细节方面具有优势",{"2":{"34":1}}],["这幅图表展示了不同超分辨率网络的局部归因图",{"2":{"34":1}}],["这幅图描绘了混合注意力变换器",{"2":{"27":1}}],["这幅图展示了vit",{"2":{"13":1}}],["这篇论文的创新点主要包括",{"2":{"27":1}}],["这篇论文提出了一种新的混合注意力变换器",{"2":{"27":1}}],["这篇论文提出的lskattention的机制原理是针对传统大核注意力",{"2":{"20":1}}],["这与rcs的设计理念紧密相连",{"2":{"24":1}}],["这张图片展示了acmix提出的混合模块的结构",{"2":{"78":1}}],["这张图片展示了acmix中的主要概念",{"2":{"68":1}}],["这张图片展示了通过改进的线性注意力模块",{"2":{"63":1}}],["这张图片可能是在说明线性注意力如何在保持注意力机制核心功能的同时",{"2":{"43":1}}],["这张图片是一幅对比不同注意力模块的图示",{"2":{"23":1}}],["这张图展示了多尺度扩张注意力",{"2":{"13":1}}],["这里添加acmix可以帮助模型更有效地融合不同层次的特征",{"2":{"133":1}}],["这里添加修改后的c2f",{"2":{"123":1,"138":1}}],["这里添加msda可以帮助模型更有效地融合不同层次的特征",{"2":{"98":1}}],["这里添加注意力机制可以帮助模型更有效地融合不同层次的特征",{"2":{"90":1,"122":1}}],["这里需要注意的是我上面提供了两段代码一个是c2f",{"2":{"77":1}}],["这里需要注意的是咱们我的网络中需要改成四种的扩张率",{"2":{"13":1}}],["这里我运行的时候有一个警告我没有关",{"2":{"100":1}}],["这里我替大家都行了修改而且在使用时无需手动添加任何参数",{"2":{"94":1}}],["这里我在三个地方添加了biformer注意力机制",{"2":{"56":1}}],["这里我们定义了一个字典",{"2":{"46":1}}],["这里提到的几个关键点包括",{"2":{"43":1}}],["这里的n代表堆叠rcs模块的数量",{"2":{"39":1}}],["这里正式开始添加biformer注意力机制",{"2":{"29":1}}],["这里先介绍我用的yolov8模型版本",{"2":{"22":1}}],["这在处理大尺寸核和复杂图像数据时特别有价值",{"2":{"20":1}}],["这意味着在处理图像的关键特征",{"2":{"20":1}}],["这是三重注意力的核心直觉和设计理念",{"2":{"16":1}}],["这是三重注意力机制中的关键步骤",{"2":{"16":1}}],["这是一个非常重要的步骤",{"2":{"4":1}}],["这样的机制使得卷积层能够更加灵活地捕捉到各种形态的结构",{"2":{"74":1}}],["这样的分解大幅降低了参数数量和计算复杂度",{"2":{"20":1}}],["这样的组合使得biformer具备了适应性和表达能力",{"2":{"15":1}}],["这样在推理阶段相比普通的3×3卷积可以减少一半的计算复杂度",{"2":{"18":1}}],["这样",{"2":{"13":1,"48":1,"58":1}}],["这样可以整合各个头部学习到的信息",{"2":{"13":1}}],["这样可以并行处理",{"2":{"13":1}}],["这样一来",{"2":{"5":1}}],["这可能需要更多的实验和调试工作",{"2":{"9":1}}],["这可能会在某些情况下导致模型性能的下降",{"2":{"9":1}}],["这种分解与重构的方法减少了冗余计算",{"2":{"78":1}}],["这种动态选择机制使得模型可以更加集中地关注于那些对当前任务最重要的区域",{"2":{"65":1}}],["这种机制会处理图像中的所有像素",{"2":{"65":1}}],["这种结合使得hat能够更好地重建高频细节",{"2":{"34":1}}],["这种结合不仅保持了低成本的内存消耗",{"2":{"31":1}}],["这种结构设计用于增强模型的特征提取和利用效率",{"2":{"31":1}}],["这种结构通过不同的旋转和排列操作",{"2":{"30":1}}],["这种设计允许在训练时学习复杂特征",{"2":{"24":1}}],["这种设计允许模型在不同的尺度上理解图像",{"2":{"13":1}}],["这种模块通过将2d卷积核分解为串联的1d核",{"2":{"20":1}}],["这种方法可以显著减少计算量",{"2":{"65":1}}],["这种方法可以有效地聚合不同层次的特征",{"2":{"39":1}}],["这种方法允许网络在较大的感受野内学习特征",{"2":{"64":1}}],["这种方法对于改善视觉transformer的性能和效率具有重要意义",{"2":{"43":1}}],["这种方法在计算上是昂贵的",{"2":{"43":1}}],["这种方法通过一个高效的映射函数和秩恢复模块来提高计算效率和性能",{"2":{"28":1,"53":1}}],["这种方法特别适用于处理大尺寸的卷积核",{"2":{"20":1}}],["这种方法使模型能够构建通道与空间位置之间的相互依赖性",{"2":{"16":1}}],["这种自适应性使得模型能够更好地捕捉输入数据的语义关联",{"2":{"9":1}}],["这种做法利用了稀疏性",{"2":{"9":1}}],["这种稀疏性减少了计算和内存开销",{"2":{"9":1}}],["这种特性是有代价的",{"2":{"4":1}}],["这个文件是另一种添加的方式",{"2":{"104":1,"134":1}}],["这个文章是有个读者指定的所以实验结果都是刚刚出炉的",{"2":{"91":1}}],["这个我在大目标检测的输出添加了一个hattention注意力机制",{"2":{"91":1}}],["这个位置我推荐的原因是因为dcn放在残差里面效果挺好的大家可以尝试",{"2":{"90":1,"122":1}}],["这个offset",{"2":{"75":1}}],["这个hattention代码刚拿来不能够直接使用的",{"2":{"52":1}}],["这个架构通过堆叠的rcs模块和repvgg模块",{"2":{"39":1}}],["这个注意力机制代码巨长",{"2":{"27":1}}],["这个注意力机制挺复杂的光代码就700+行",{"2":{"21":1}}],["这个在代码中均有体现",{"2":{"27":1}}],["这个图表展示了所提出的混合注意力变换器",{"2":{"27":1}}],["这个分支保持输入的身份",{"2":{"30":1}}],["这个分支首先进行相同的z池化和卷积操作",{"2":{"30":1}}],["这个分支首先沿着宽度",{"2":{"16":1}}],["这个分支对输入张量进行z池化",{"2":{"30":1}}],["这个分支沿着高度",{"2":{"16":1}}],["这个分支直接处理输入张量",{"2":{"16":1}}],["这个方法能够构建输入通道或空间位置之间的相互依赖性",{"2":{"16":1}}],["这个模块可以替换conv",{"2":{"49":1}}],["这个模块使用全局平均池化",{"2":{"23":1}}],["这个模块包含三个分支",{"2":{"5":1}}],["这个模块的主要功能是通过减少特征图的通道数量",{"2":{"3":1}}],["这个机制可以直接使用在主干上",{"2":{"57":1}}],["这个机制",{"2":{"5":1}}],["这使得注意力机制能够根据每个查询自适应地关注最有语义相关的键",{"2":{"4":1}}],["这使得注意力机制可以集中关注输入图像的不同区域",{"2":{"4":1}}],["这些块在下采样和上采样之间交替",{"2":{"84":1}}],["这些点的偏移量是由查询通过偏移网络学习得到的",{"2":{"75":1}}],["这些创新点使得所提出的方法在超分辨率重建方面的性能显著优于现有技术",{"2":{"27":1}}],["这些模块通过不同方式计算注意力权重",{"2":{"23":1}}],["这些模式将注意力限制在特定区域",{"2":{"4":1}}],["这些权重是通过一个s形激活层生成的",{"2":{"16":1}}],["这些特征随后被连接在一起",{"2":{"13":1}}],["这些操作在围绕红色查询块的窗口内的彩色块之间进行",{"2":{"13":1}}],["这些改进使得msda在不增加额外计算成本的情况下",{"2":{"13":1}}],["这些位置路由器根据特定的规则将图像块分配给上层和下层路由器",{"2":{"1":1}}],["意即",{"2":{"3":1}}],["ops",{"2":{"93":1}}],["option",{"2":{"57":1,"83":1,"93":1}}],["optional",{"2":{"29":1,"42":28,"57":1,"83":1,"93":1}}],["ours",{"2":{"63":1}}],["out21",{"2":{"47":3}}],["out2",{"2":{"47":2}}],["out11",{"2":{"47":3}}],["out1",{"2":{"47":2}}],["outputs",{"2":{"56":1,"80":1,"89":1,"91":1,"93":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["output",{"2":{"29":2,"42":5,"57":1,"83":1,"93":1}}],["out",{"2":{"19":15,"29":11,"42":6,"47":12,"49":17,"57":1,"78":38,"83":2,"93":6,"94":8}}],["oca",{"2":{"42":9}}],["ocab进一步增强了不同窗口间特征的交互",{"2":{"27":1}}],["ocab",{"2":{"27":1,"42":3}}],["ow",{"2":{"42":4}}],["oww=self",{"2":{"42":1}}],["oww",{"2":{"42":2}}],["owh=self",{"2":{"42":1}}],["owh",{"2":{"42":2}}],["overlap",{"2":{"42":23}}],["overlapping",{"2":{"42":3}}],["overlaping",{"2":{"19":3}}],["override",{"2":{"42":5}}],["o",{"2":{"29":1,"43":2}}],["off",{"2":{"94":10}}],["off=false",{"2":{"94":1}}],["offset",{"2":{"93":2,"94":22}}],["offsets",{"2":{"64":1,"93":2}}],["of",{"2":{"19":6,"29":5,"31":1,"42":47,"56":1,"57":2,"80":1,"83":2,"89":1,"91":1,"93":3,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["original",{"2":{"42":1}}],["ori",{"2":{"42":10}}],["or",{"2":{"19":6,"29":5,"42":7}}],["obj",{"2":{"17":1}}],["object",{"2":{"3":1,"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["omega^i",{"2":{"17":1}}],["omega^r",{"2":{"17":1}}],["omega",{"2":{"17":4}}],["only",{"2":{"29":1}}],["on",{"2":{"12":1,"42":3}}],["one",{"2":{"12":2,"19":2,"29":1,"31":2,"39":2,"42":1}}],["osa添加进去即可",{"2":{"97":1}}],["osa添加步骤",{"0":{"69":1},"1":{"79":1,"88":1,"97":1}}],["osa的训练截图",{"2":{"125":1}}],["osa的训练过程截图",{"0":{"125":1}}],["osa的位置",{"2":{"113":1}}],["osa的yaml版本二",{"0":{"119":1}}],["osa的yaml版本一",{"0":{"113":1}}],["osa的yaml文件和训练截图",{"0":{"106":1},"1":{"113":1,"119":1,"125":1}}],["osa的核心代码复制进去",{"2":{"79":1}}],["osa的基本原理",{"0":{"12":1}}],["osa核心代码",{"0":{"49":1}}],["osa在模型中用于堆叠rcs模块",{"2":{"39":1}}],["osa减少了重复的特征计算和存储需求",{"2":{"31":1}}],["osa通过聚合具有不同感受野的特征来增加网络对于不同尺度的敏感性",{"2":{"31":1}}],["osa",{"0":{"31":1},"2":{"12":1,"31":2,"39":2}}],["osa模块来替换c2f",{"2":{"49":1}}],["osa模块来替换我们yolov8中的c2f模块",{"2":{"49":1}}],["osa模块中",{"2":{"31":1}}],["osa模块被进一步与rcs",{"2":{"31":1}}],["osa模块的排列和组合反映了它们如何一起工作以优化特征传递和提高检测性能",{"2":{"39":1}}],["osa模块的使用有两个主要目的",{"2":{"31":1}}],["osa模块的框架原理进行了详细的分析",{"2":{"3":1}}],["osa模块通过表示具有多个感受野的多样化特征",{"2":{"31":1}}],["osa模块通过堆叠rcs",{"2":{"12":1}}],["osa模块原理",{"0":{"7":1},"1":{"12":1,"18":1,"24":1,"31":1,"39":1}}],["osa模块到网络结构中",{"2":{"3":1}}],["osa模块",{"0":{"59":1},"1":{"69":1,"79":1,"88":1,"97":1,"106":1,"113":1,"119":1,"125":1},"2":{"3":1,"31":1,"39":1,"88":2}}],["rpe",{"2":{"78":2,"94":17}}],["rpi=rpi",{"2":{"42":1}}],["rpi",{"2":{"42":13}}],["running",{"2":{"49":10}}],["rule",{"2":{"42":1}}],["rbr",{"2":{"49":15}}],["rgb",{"2":{"42":2}}],["roll",{"2":{"42":2}}],["router",{"2":{"29":3}}],["routing=self",{"2":{"29":2}}],["routing=false",{"2":{"29":6}}],["routing",{"2":{"1":1,"4":2,"29":39}}],["rwightman",{"2":{"42":2}}],["rcan",{"2":{"42":1}}],["rcan和swinir",{"2":{"34":1}}],["rcs能够在训练阶段从输入特征中学习深层表示",{"2":{"18":1}}],["rcs利用通道分割和通道shuffle操作来降低计算复杂性",{"2":{"18":1}}],["rcs模块",{"0":{"24":1},"2":{"12":1}}],["rcsosa",{"2":{"12":1,"49":2,"113":8,"119":4}}],["rcs",{"0":{"7":1,"12":1,"18":1,"49":1,"69":1,"106":1,"113":1,"119":1,"125":1},"1":{"12":1,"18":1,"24":1,"31":1,"39":1,"79":1,"88":1,"97":1,"113":1,"119":1,"125":1},"2":{"12":3,"18":1,"24":1,"31":1,"39":3}}],["rhag",{"2":{"27":1,"42":6}}],["r",{"2":{"17":1,"29":20,"42":6,"43":1}}],["ri​",{"2":{"17":1}}],["ri",{"2":{"17":1}}],["right",{"2":{"17":4}}],["riωi",{"2":{"17":1}}],["r−i+1",{"2":{"17":2}}],["r+∑i=1r​ωi",{"2":{"17":1}}],["r+∑i=1r",{"2":{"17":1}}],["r=1",{"2":{"13":1}}],["r来执行自注意力操作",{"2":{"13":1}}],["raise",{"2":{"29":5,"42":1}}],["randn",{"2":{"49":1,"93":1}}],["random",{"2":{"29":1,"42":3}}],["rand",{"2":{"19":1,"42":1,"78":1}}],["range=1",{"2":{"42":1}}],["range",{"2":{"19":5,"42":9,"49":3,"57":1,"78":1,"83":1,"93":1,"94":8}}],["rate2",{"2":{"78":3}}],["rate1",{"2":{"78":3}}],["rate=0",{"2":{"42":3}}],["rate",{"2":{"42":19,"78":5}}],["rates",{"2":{"13":1}}],["ratio=1",{"2":{"83":1}}],["ratio=0",{"2":{"42":1}}],["ratio=overlap",{"2":{"42":3}}],["ratio=2",{"2":{"42":1}}],["ratio=compress",{"2":{"42":4}}],["ratio=3",{"2":{"42":3}}],["ratio=self",{"2":{"19":2,"42":1}}],["ratio=mlp",{"2":{"19":2,"42":3}}],["ratio=4",{"2":{"19":5,"29":1,"42":4}}],["ratio",{"2":{"19":10,"29":9,"42":38,"83":9}}],["rayleizhu",{"2":{"4":1}}],["reference",{"2":{"94":4}}],["ref",{"2":{"94":22}}],["reflectionpad2d",{"2":{"78":1}}],["requires",{"2":{"78":1,"94":1}}],["reconstruction",{"2":{"42":3}}],["recognition",{"2":{"8":1}}],["reverse",{"2":{"42":4}}],["relative",{"2":{"42":48}}],["relaxing",{"2":{"19":1}}],["relu=false",{"2":{"47":1}}],["relu=true",{"2":{"47":1}}],["relu",{"2":{"42":1,"47":5,"49":2,"83":1,"94":1}}],["region",{"2":{"29":1}}],["register",{"2":{"19":4,"42":3}}],["registry",{"2":{"19":1,"42":3}}],["ret",{"2":{"29":2}}],["returns",{"2":{"42":2,"49":1}}],["return",{"2":{"19":16,"26":1,"29":9,"42":23,"47":4,"49":14,"57":3,"78":3,"83":4,"93":9,"94":4}}],["rearrange",{"2":{"29":13,"42":2,"83":7,"94":6}}],["repeat",{"2":{"78":5}}],["repeats",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["repconv",{"2":{"49":2}}],["repconv块",{"2":{"24":1}}],["reparam",{"2":{"49":3}}],["reparameterized",{"2":{"12":1}}],["repvgg",{"2":{"49":10}}],["reset",{"2":{"78":2}}],["reso",{"2":{"63":1}}],["resolution=",{"2":{"42":1}}],["resolution=input",{"2":{"42":3}}],["resolution",{"2":{"19":3,"42":48}}],["res",{"2":{"49":1}}],["result",{"2":{"49":4}}],["results",{"2":{"42":1}}],["resi",{"2":{"42":9}}],["residual",{"2":{"42":8,"94":2}}],["reshape",{"2":{"19":9,"42":7,"49":1,"78":1,"83":4,"94":17}}],["reduction",{"2":{"42":3}}],["reduce",{"2":{"29":1}}],["reduced",{"2":{"3":1}}],["redundancy",{"2":{"29":1}}],["red",{"2":{"17":1}}],["3d",{"2":{"84":1}}],["3d版本特别擅长于交叉深度数据理解",{"2":{"84":1}}],["3db到1",{"2":{"27":1}}],["32",{"2":{"56":2,"80":2,"89":2,"91":2,"93":2,"94":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":5}}],["365",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["3157184",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["3157200",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["33",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["3conv",{"2":{"42":1}}],["35",{"2":{"26":1,"61":1}}],["3x3卷积通过1x1卷积的方式被分解",{"2":{"78":1}}],["3x3",{"2":{"13":1,"49":2}}],["3",{"0":{"2":1,"24":1,"30":1,"47":1,"57":1,"60":1,"63":1,"74":1,"85":1,"97":1,"98":1,"118":1,"121":1,"133":1},"1":{"6":1,"11":1,"17":1},"2":{"12":1,"13":2,"16":1,"17":3,"19":33,"23":1,"26":10,"27":1,"29":3,"42":36,"47":5,"49":4,"54":1,"56":16,"57":6,"63":1,"64":1,"78":2,"80":16,"83":8,"89":16,"91":15,"93":8,"94":2,"95":16,"104":16,"113":16,"119":16,"129":16,"131":16,"132":10,"134":19}}],["一篇文章很难全部介绍到",{"2":{"98":1}}],["一组参考点均匀地放置在特征图上",{"2":{"75":1}}],["一部分直接通过",{"2":{"31":1}}],["一部分输入经过repvgg块",{"2":{"24":1}}],["一个特定的查询块",{"2":{"13":1}}],["一个分支可能专注于图像的宽度",{"2":{"5":1}}],["一次性聚合多个特征级联",{"2":{"12":1}}],["一些方法引入了稀疏模式",{"2":{"4":1}}],["一",{"0":{"1":1,"3":1,"5":1,"8":1,"14":1,"21":1,"28":1,"36":1,"45":1,"48":1},"1":{"58":1,"68":1,"78":1}}],["2图示dat可添加的位置",{"0":{"127":1}}],["2图示lskattention可添加的位置",{"0":{"99":1}}],["21",{"2":{"80":1,"95":2,"113":2,"119":2,"131":1,"132":2}}],["20",{"2":{"56":2,"89":2,"104":2,"129":2,"134":2}}],["295",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["28",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["22",{"2":{"80":2,"91":2,"131":2}}],["225",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["224",{"2":{"19":2,"42":2,"78":2,"94":4}}],["224x224",{"2":{"19":1}}],["258",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["25902624",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["25902640",{"2":{"56":1,"80":1,"89":1,"91":1,"95":1,"104":1,"113":1,"119":1,"129":1,"131":1,"132":1,"134":1}}],["25",{"2":{"56":2,"80":2,"89":2,"91":2,"95":2,"104":2,"113":2,"119":2,"129":2,"131":2,"132":2,"134":2}}],["256",{"2":{"49":3,"56":4,"80":4,"89":4,"91":4,"95":4,"104":4,"113":4,"119":4,"129":4,"131":4,"132":4,"134":4}}],["255",{"2":{"42":1}}],["2d和3d适应性指的是deformable",{"2":{"84":1}}],["2d和3d适应性",{"0":{"84":1},"1":{"109":1,"115":1,"121":1,"126":1},"2":{"54":1}}],["2d",{"2":{"29":2,"42":1,"84":2,"93":1}}],["2db之间",{"2":{"27":1}}],["23",{"2":{"26":1,"61":1,"89":1,"104":1,"129":1,"134":1}}],["24",{"2":{"19":4,"26":2,"56":2,"78":3,"89":2,"104":2,"129":2,"134":2}}],["2tuple",{"2":{"19":3,"42":6,"94":1}}],["2^n",{"2":{"42":3}}],["2^",{"2":{"17":1}}],["2n−1−12^",{"2":{"17":1}}],["2n−1",{"2":{"17":2}}],["2",{"0":{"0":1,"12":1,"16":1,"18":2,"23":2,"24":1,"30":1,"31":1,"34":1,"39":1,"43":1,"50":1,"51":1,"53":2,"54":1,"57":1,"58":1,"61":1,"63":1,"64":2,"65":1,"68":1,"70":1,"71":2,"74":1,"75":2,"78":2,"80":1,"84":1,"85":1,"86":1,"88":1,"89":2,"95":1,"100":1,"104":2,"106":1,"108":1,"111":2,"112":1,"113":1,"114":1,"115":1,"119":2,"120":2,"125":2,"128":1,"130":1,"131":1,"132":1,"134":2,"136":2,"139":1},"1":{"2":1,"6":1,"11":1,"17":1,"61":1,"68":1,"71":1,"78":1,"80":1,"89":1,"95":1,"104":1,"109":1,"111":1,"113":1,"114":1,"115":1,"119":1,"120":1,"121":1,"125":1,"126":1,"132":1,"134":1,"136":1},"2":{"9":2,"12":1,"13":2,"16":1,"17":14,"19":50,"23":1,"26":14,"27":1,"29":6,"31":1,"42":77,"43":1,"47":7,"49":8,"53":1,"54":1,"56":10,"57":4,"58":1,"63":1,"64":1,"78":7,"80":10,"83":17,"84":1,"89":10,"91":11,"93":9,"94":23,"95":10,"104":10,"113":10,"119":10,"129":10,"131":10,"132":4,"134":10}}]],"serializationVersion":2}';export{t as default};
